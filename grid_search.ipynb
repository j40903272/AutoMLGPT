{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "import csv\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "tmp = {}\n",
    "for name, module in vars(preprocessing).items():\n",
    "    if name[0].isupper():\n",
    "        tmp[name] = module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_loader.select('house-prices-advanced-regression-techniques')\n",
    "name = 'house-prices-advanced-regression-techniques'\n",
    "label = dataset['label']\n",
    "data = dataset['train']\n",
    "X_train = data.drop(label, axis=1)\n",
    "y_train = data[label]\n",
    "data = dataset['test']\n",
    "if label in data:\n",
    "    data = data.drop(label, axis=1)\n",
    "X_test = data\n",
    "y_test = pd.DataFrame(data={label: []*len(X_test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Status():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.init()\n",
    "    \n",
    "    def init(self):\n",
    "        \n",
    "        self.dataset = \"\"\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        \n",
    "        self.start_explore_prompt = \"\"\n",
    "        self.how_to_explore_prompt = \"\"\n",
    "        self.explore_data_thought = \"\"\n",
    "        \n",
    "        self.explore_result = \"Nothing special.\"\n",
    "        self.hyperopt_result = \"Nothing special.\"\n",
    "        \n",
    "        self.explore = False\n",
    "        self.preprocess = False\n",
    "        self.model_select = False\n",
    "        self.hyperopt = False\n",
    "        \n",
    "        self.explore_code = \"\"\n",
    "        self.preprocess_code = \"\"\n",
    "        self.model_select_code = \"\"\n",
    "        self.hyperopt_code = \"\"\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.1, random_state=42)\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.model = LGBMRegressor()\n",
    "        self.best_param = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'kaggle/ieee-fraud-detection/train.csv'\n",
      "[Errno 20] Not a directory: 'kaggle/unzip.py/train.csv'\n",
      "[Errno 2] No such file or directory: 'kaggle/.ipynb_checkpoints/train.csv'\n"
     ]
    }
   ],
   "source": [
    "class DataLoader():\n",
    "    \n",
    "    def __init__(self, chain):\n",
    "        self.datasets = {}\n",
    "        self.chain = chain\n",
    "        \n",
    "    def select(self, name: str):\n",
    "        if name not in self.datasets:\n",
    "            pass\n",
    "        else:\n",
    "            return self.datasets[name]\n",
    "        \n",
    "    def show(self):\n",
    "        print(self.datasets.keys())\n",
    "    \n",
    "    def load(self):\n",
    "        \n",
    "        for dirn in os.listdir('kaggle'):\n",
    "            \n",
    "            if not os.path.isdir(dirn):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "            \n",
    "                train_path = os.path.join('kaggle', dirn, 'train.csv')\n",
    "                train_df = pd.read_csv(train_path)\n",
    "\n",
    "                test_path = os.path.join('kaggle', dirn, 'test.csv')\n",
    "                test_df = pd.read_csv(test_path)\n",
    "\n",
    "                label = list(set(train_df.columns) - set(test_df.columns))[0]\n",
    "\n",
    "                self.datasets[dirn] = {\n",
    "                    'train': train_df,\n",
    "                    'test': test_df,\n",
    "                    'label': label,\n",
    "                    'name': dirn\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                \n",
    "                \n",
    "data_loader = DataLoader(chain)\n",
    "data_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "Status.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = \"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\"\n",
    "os.environ['SERPAPI_API_KEY'] = '5ef522f358c7b54b6b86caf25bb85cdcace48af848db79ec66c1eb8998bf0a90'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "human_message_prompt = HumanMessagePromptTemplate(\n",
    "        prompt=PromptTemplate(\n",
    "            template=\"{text}\",\n",
    "            input_variables=[\"text\"],\n",
    "        )\n",
    "    )\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt_template)#, memory=ConversationBufferMemory())\n",
    "# print(chain.run(\"colorful socks\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  Id  \n",
       "0      9.4   0  \n",
       "1      9.8   1  \n",
       "2      9.8   2  \n",
       "3      9.8   3  \n",
       "4      9.4   4  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"WineQT.csv\")\n",
    "X = data.drop('quality', axis=1)\n",
    "y = data['quality']\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = f\"\"\"\n",
    "You are now performing a data inspection and examination before training a machine learning model.\n",
    "Your goal is to tell me that if the data should be preprocessed and give me the correspoing python code.\n",
    "The preprocess includes but is not limit to missing value imputation, scaling, normalization or encoding.\n",
    "This will be performed in two stages.\n",
    "First, you will examine the dataset.\n",
    "Second, you will decide how to preprocess the dataset.\n",
    "Now do the first stage.\n",
    "\n",
    "\n",
    "This is the dataframe:\n",
    "{X.head(30)}\n",
    "\n",
    "First, tell me your thought about how you will exmaine the dataset.\n",
    "Second, write a python function that help you examine the data. compress the output result as much as possible.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My thought about how to examine the dataset is to check for missing values, outliers, and the distribution of the variables. I will also check for any categorical variables that need to be encoded.\n",
      "\n",
      "Here is a python function that will help me examine the data:\n",
      "\n",
      "```python\n",
      "def examine_data(df):\n",
      "    # Check for missing values\n",
      "    print(\"Missing values:\\n\", df.isnull().sum())\n",
      "    \n",
      "    # Check for outliers\n",
      "    print(\"Outliers:\\n\", df.describe())\n",
      "    \n",
      "    # Check the distribution of the variables\n",
      "    print(\"Distribution:\\n\", df.hist())\n",
      "    \n",
      "    # Check for categorical variables that need to be encoded\n",
      "    print(\"Categorical variables:\\n\", df.select_dtypes(include=['object']))\n",
      "```\n",
      "\n",
      "This function will print out the number of missing values for each variable, the summary statistics for each variable, the histogram of each variable, and any categorical variables that need to be encoded.\n"
     ]
    }
   ],
   "source": [
    "result1 = chain.run(start)\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      " fixed acidity           0\n",
      "volatile acidity        0\n",
      "citric acid             0\n",
      "residual sugar          0\n",
      "chlorides               0\n",
      "free sulfur dioxide     0\n",
      "total sulfur dioxide    0\n",
      "density                 0\n",
      "pH                      0\n",
      "sulphates               0\n",
      "alcohol                 0\n",
      "Id                      0\n",
      "dtype: int64\n",
      "\n",
      "Outliers:\n",
      "        fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
      "count     914.000000        914.000000   914.000000      914.000000   \n",
      "mean        8.258096          0.531018     0.265799        2.519092   \n",
      "std         1.696354          0.179047     0.194982        1.305725   \n",
      "min         4.600000          0.120000     0.000000        0.900000   \n",
      "25%         7.100000          0.400000     0.090000        1.900000   \n",
      "50%         7.900000          0.520000     0.250000        2.200000   \n",
      "75%         9.000000          0.640000     0.420000        2.600000   \n",
      "max        15.600000          1.580000     1.000000       15.500000   \n",
      "\n",
      "        chlorides  free sulfur dioxide  total sulfur dioxide     density  \\\n",
      "count  914.000000           914.000000            914.000000  914.000000   \n",
      "mean     0.086476            15.706783             45.838621    0.996683   \n",
      "std      0.047437            10.237482             31.929403    0.001914   \n",
      "min      0.012000             1.000000              6.000000    0.990070   \n",
      "25%      0.070000             7.000000             21.000000    0.995520   \n",
      "50%      0.078000            13.000000             38.000000    0.996645   \n",
      "75%      0.090000            21.000000             62.000000    0.997800   \n",
      "max      0.611000            68.000000            278.000000    1.003200   \n",
      "\n",
      "               pH   sulphates     alcohol           Id  \n",
      "count  914.000000  914.000000  914.000000   914.000000  \n",
      "mean     3.314234    0.655646   10.439187   815.590810  \n",
      "std      0.152897    0.166146    1.074776   464.852449  \n",
      "min      2.740000    0.330000    8.400000     0.000000  \n",
      "25%      3.210000    0.550000    9.500000   412.250000  \n",
      "50%      3.310000    0.620000   10.200000   825.000000  \n",
      "75%      3.400000    0.720000   11.200000  1219.000000  \n",
      "max      4.010000    2.000000   14.000000  1597.000000  \n",
      "\n",
      "Distribution:\n",
      " [[<AxesSubplot:title={'center':'fixed acidity'}>\n",
      "  <AxesSubplot:title={'center':'volatile acidity'}>\n",
      "  <AxesSubplot:title={'center':'citric acid'}>]\n",
      " [<AxesSubplot:title={'center':'residual sugar'}>\n",
      "  <AxesSubplot:title={'center':'chlorides'}>\n",
      "  <AxesSubplot:title={'center':'free sulfur dioxide'}>]\n",
      " [<AxesSubplot:title={'center':'total sulfur dioxide'}>\n",
      "  <AxesSubplot:title={'center':'density'}>\n",
      "  <AxesSubplot:title={'center':'pH'}>]\n",
      " [<AxesSubplot:title={'center':'sulphates'}>\n",
      "  <AxesSubplot:title={'center':'alcohol'}>\n",
      "  <AxesSubplot:title={'center':'Id'}>]]\n",
      "\n",
      "Categorical variables:\n",
      " Index([], dtype='object')\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "def examine_data(df):\n",
    "    \n",
    "    output = io.StringIO()\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(\"Missing values:\\n\", df.isnull().sum(), file=output, end=\"\\n\\n\")\n",
    "    \n",
    "    # Check for outliers\n",
    "    print(\"Outliers:\\n\", df.describe(), file=output, end=\"\\n\\n\")\n",
    "    \n",
    "    # Check the distribution of each feature\n",
    "    print(\"Distribution:\\n\", df.hist(), file=output, end=\"\\n\\n\")\n",
    "    \n",
    "    # Check for categorical variables that need to be encoded\n",
    "    print(\"Categorical variables:\\n\", df.select_dtypes(include=['object']).columns, file=output)\n",
    "    \n",
    "    contents = output.getvalue()\n",
    "    output.close()\n",
    "    return contents\n",
    "    \n",
    "result2 = examine_data(X_train)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My thoughts about the results are that there are no missing values and no categorical variables that need to be encoded. However, there are some variables with outliers, such as residual sugar and total sulfur dioxide.\n",
      "\n",
      "Based on the results, I will perform scaling and normalization on the dataset to handle the outliers. Specifically, I will use the StandardScaler and MinMaxScaler from the sklearn library to scale and normalize the variables.\n",
      "\n",
      "Here is a python function that will help me perform the preprocess:\n",
      "\n",
      "```python\n",
      "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
      "\n",
      "def preprocess_data(df):\n",
      "    # Scale and normalize the variables\n",
      "    scaler = StandardScaler()\n",
      "    df_scaled = scaler.fit_transform(df)\n",
      "    \n",
      "    scaler = MinMaxScaler()\n",
      "    df_normalized = scaler.fit_transform(df_scaled)\n",
      "    \n",
      "    return df_normalized\n",
      "```\n",
      "\n",
      "This function will scale and normalize the variables in the dataset using the StandardScaler and MinMaxScaler from the sklearn library. The function will return the preprocessed dataset.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\n",
    "    start + \\\n",
    "    \"\\nYour response:\" + \\\n",
    "    result1 + \\\n",
    "    \"\\nCode Execution results:\" + \\\n",
    "    result2 + \\\n",
    "    \"\"\"\\n\\nThe dataset examine results is given above.\n",
    "    First, tell me your thought about the results.\n",
    "    Second, tell me what preprocess will you perform on the dataset. \n",
    "    Third, write a python function that help you perform the preprocess.\n",
    "    \"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # Scale and normalize the variables\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df)\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    df_normalized = scaler.fit_transform(df_scaled)\n",
    "    \n",
    "    return df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid_0's l2: 0.421429\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid_0's l2: 0.421429\n",
      "no preprocess: 0.3410522900436366, chatgpt preprocess: 0.33998530950014233\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from lightgbm import LGBMRegressor, early_stopping\n",
    "\n",
    "\n",
    "seed = 42\n",
    "\n",
    "# chat\n",
    "XX = preprocess_data(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(XX, y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "clf = LGBMRegressor(random_state=seed)\n",
    "clf.fit(X_train, y_train, eval_set=(X_valid, y_valid), callbacks=[early_stopping(stopping_rounds=10)])\n",
    "y_pred = clf.predict(X_test)\n",
    "chat_preprocess = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "# no\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "clf = LGBMRegressor(random_state=seed)\n",
    "clf.fit(X_train, y_train, eval_set=(X_valid, y_valid), callbacks=[early_stopping(stopping_rounds=10)])\n",
    "y_pred = clf.predict(X_test)\n",
    "no_preprocess = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(f\"no preprocess: {no_preprocess}, chatgpt preprocess: {chat_preprocess}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader, SeleniumURLLoader\n",
    "\n",
    "class MySeleniumURLLoader(SeleniumURLLoader):\n",
    "    def load(self) -> list:\n",
    "        docs = []\n",
    "        driver = self._get_driver()\n",
    "        for url in self.urls:\n",
    "            try:\n",
    "                from selenium.webdriver.support.ui import WebDriverWait\n",
    "                from selenium.webdriver.support import expected_conditions as EC\n",
    "                from selenium.webdriver.common.by import By\n",
    "                from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "                try:\n",
    "                    driver.get(url)\n",
    "                    elements = WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_all_elements_located(\n",
    "                            (By.CLASS_NAME, \"kc-list-items__item-container--pc\")\n",
    "                        )\n",
    "                    )\n",
    "                    for e in elements:\n",
    "                        metadata = {\"href\": e.get_attribute(\"href\")}\n",
    "                        text = e.text.split(\"\\n\")[-1]\n",
    "                        docs.append(Document(page_content=text, metadata=metadata))\n",
    "                    return docs\n",
    "                except TimeoutException:\n",
    "                    logger.error(\"Search result not found\")\n",
    "            except Exception as e:\n",
    "                if self.continue_on_failure:\n",
    "                    logging.error(f\"Error fetching or processing {url}, exception: {e}\")\n",
    "                else:\n",
    "                    raise e\n",
    "\n",
    "        driver.quit()\n",
    "        return docs\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "prompt_template = \"\"\"\n",
    "\n",
    "{text}\n",
    "\n",
    "請摘要列出3個有關購買這個商品的要點:\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "sum_chain = load_summarize_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n",
    "    \n",
    "loader = WebBaseLoader('https://scikit-learn.org/stable/modules/grid_search.html')\n",
    "docs = loader.load()\n",
    "docs[0].page_content = docs[0].page_content[:1024]\n",
    "guide = self.chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Union, List, Dict, Tuple\n",
    "from pydantic import Field\n",
    "import ast\n",
    "import astunparse\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def prompts(name: str, description: str):\n",
    "    def decorator(func):\n",
    "        func.name = name\n",
    "        func.description = description\n",
    "        return func\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def execute(self, cmd: str, glo, loc):\n",
    "    tree = ast.parse(cmd)\n",
    "    module = ast.Module(tree.body[:-1], type_ignores=[])\n",
    "    exec(astunparse.unparse(module), self.globals, self.locals)  # type: ignore\n",
    "    module_end = ast.Module(tree.body[-1:], type_ignores=[])\n",
    "    module_end_str = astunparse.unparse(module_end)  # type: ignore\n",
    "    try:\n",
    "        return eval(module_end_str, self.globals, self.locals)\n",
    "    except Exception:\n",
    "        exec(module_end_str, self.globals, self.locals)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "coding_agent = chain\n",
    "\n",
    "class HyperOptTool:\n",
    "    @prompts(\n",
    "        name=\"HYPEROPT\",\n",
    "        description=\"This is a tool for hyper parameter tuning.\"\n",
    "        \"Input should be a estimator name.\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        \n",
    "        return \"\"\"\n",
    "        Your goal is to fullfill the following steps:\n",
    "        1. Construct estimator\n",
    "        2. Define parameter grid\n",
    "        3. Construct search algorithm\n",
    "        \n",
    "        This three steps can be accomplished by use the following tools:\n",
    "        1. Construct Estimator\n",
    "        2. Parameter Grid\n",
    "        3. Construct Search\n",
    "        \n",
    "        Each of these tools will output a python code snippet.\n",
    "        Concat the three code snippets into one.\n",
    "        Input the concatenated code snippet to the Final Tool.\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "class FinalTool:\n",
    "    \n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    @prompts(\n",
    "        name=\"Final\",\n",
    "        description=\"This is a tool for the final step of hyper parameter search.\"\n",
    "        \"Input should be a code snippet including constructing estimator, constructing parameter grid and constructing search instance.\"\n",
    "        \"Output should be the final evaluation score after the hyper parameter search.\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        glo, loc, clff = {}, {}, None\n",
    "        try:\n",
    "            execute(query, glo, loc)\n",
    "            clff = loc['clff']\n",
    "            search = clff.fit(self.X, self.Y)\n",
    "            return search.best_score_\n",
    "        except Exception as e:\n",
    "            return str(e)\n",
    "    \n",
    "class ConstructEstimatorTool:\n",
    "    @prompts(\n",
    "        name=\"Construct Estimator\",\n",
    "        description=\"This is a tool for constructing an estimator instance.\"\n",
    "        \"Input should be the estimator name.\"\n",
    "        \"Output should be the python code snippet for constructing the estimator instance.\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        construct_estimator_prompt = f\"Give me two lines of python code. Construct a {query.strip()} estimator and assign the estimator clf variable.\"\n",
    "        codes = coding_agent.run(construct_estimator_prompt)\n",
    "        return codes\n",
    "    \n",
    "    \n",
    "class ConstructParamGridTool:\n",
    "    @prompts(\n",
    "        name=\"Parameter Grid\",\n",
    "        description=\"This is a tool for constructing the parameter grid for a given estimator.\"\n",
    "        \"Input should be the code snippet for constructing estimator.\"\n",
    "        \"Output should be the python code snippet for constructing the parameter grid.\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        \n",
    "        glo, loc, clf = {}, {}, None\n",
    "        try:\n",
    "            execute(query, glo, loc)\n",
    "            clf = loc['clf']\n",
    "        except Exception as e:\n",
    "            return str(e)\n",
    "        \n",
    "        param_grid_prompt = f\"\"\"\n",
    "        You are now performing a hyper parameter search on a machine learning model.\n",
    "        Your goal is to construct the paramter space to search.\n",
    "\n",
    "        For example:\n",
    "        The model is a LinearSVR.\n",
    "        The parameters are {LinearSVR().get_params()}\n",
    "        The parameter grid can be defined as:\n",
    "        ```\n",
    "        param_grid = {{'C': [1, 10, 100, 1000], 'tol': [0.001, 0.0001]}}\n",
    "        ```\n",
    "        Remeber to define the space only on important features. \n",
    "\n",
    "\n",
    "        Now.\n",
    "        The given model is a {clf.__class__.__name__}.\n",
    "        The paramters are {clf.get_params()}.\n",
    "        The detail description about the parameters is as follow:\n",
    "        {clf.__doc__}\n",
    "\n",
    "        Construct the parameter grid you will use for hyper parameter in a python dict and assign to param_grid variable:\n",
    "        \"\"\"\n",
    "\n",
    "        param_grid = chain.run(hypopt_prompt)\n",
    "        return codes\n",
    "\n",
    "    \n",
    "class ConstructSearchTool:\n",
    "    @prompts(\n",
    "        name=\"Construct Search\",\n",
    "        description=\"This is a tool for constructing the hyper parameter search instance.\"\n",
    "        \"Input should be the code snippet for constructing parameter grid.\"\n",
    "        \"Output should be the python code snippet for constructing the search instance.\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        \n",
    "        glo, loc, param_grid = {}, {}, None\n",
    "        try:\n",
    "            execute(query, glo, loc)\n",
    "            param_grid = loc['param_grid']\n",
    "        except Exception as e:\n",
    "            return str(e)\n",
    "        \n",
    "        search_algo_reason_prompt = f\"\"\"\n",
    "        You are now performing a hyper parameter search on a machine learning model.\n",
    "        Your goal is to decide the search algorithm base on model type and parameter space.\n",
    "\n",
    "\n",
    "        The given model is a {clf.__class__.__name__}.\n",
    "        The paramters grid is {param_grid}.\n",
    "        The avaiable search algorithm is as follow:\n",
    "\n",
    "        1. GridSearchCV: Exhaustive search over specified parameter values for an estimator.\n",
    "        2. HalvingGridSearchCV: Search over specified parameter values with successive halving.\n",
    "        3. ParameterGrid: Grid of parameters with a discrete number of values for each.\n",
    "        4. ParameterSampler: Generator on parameters sampled from given distributions.\n",
    "        5. RandomizedSearchCV: Randomized search on hyper parameters.\n",
    "        6. HalvingRandomSearchCV: Randomized search on hyper parameters with successive halving.\n",
    "\n",
    "        Think about which algorithm to use base on the model, parameter space and algorithm description.\n",
    "        Tell me which algorithm you have choosen and why.\n",
    "        List all the other algorithms you have choosen not to use and why.\n",
    "        \"\"\"\n",
    "        \n",
    "        reason = chain.run(search_algo_reason_prompt)\n",
    "        print(reason)\n",
    "\n",
    "        search_algo = chain.run(f\"{reason}\\n\\nAccording to above, give me the algorithm name of the choosen algorithm in a single token.\")\n",
    "        print(f\"Deciding search algo:\\n{search_algo}\\n\")\n",
    "        \n",
    "        construct_search_prompt = f\"\"\"\n",
    "        Give me two lines of python code.\n",
    "        Frist, from lib import {search_algo.strip()}.\n",
    "        Second, instatiate an instance with {search_algo.strip()}(clf, param_grid, random_state=0, n_iter=10) and assign to clff variable.\n",
    "        \"\"\"\n",
    "        codes = chain.run(construct_search_prompt)\n",
    "        return codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construct estimator with following:\n",
      "import lightgbm as lgb\n",
      "clf = lgb.LGBMRegressor()\n",
      "\n",
      "Results:\n",
      "LGBMRegressor()\n",
      "\n",
      "Construct params grid:\n",
      "{'learning_rate': [0.01, 0.05, 0.1, 0.5], 'n_estimators': [50, 100, 200, 500], 'max_depth': [3, 5, 7, 9], 'num_leaves': [15, 31, 63, 127], 'min_child_samples': [10, 20, 30, 50], 'reg_alpha': [0.0, 0.1, 0.5, 1.0], 'reg_lambda': [0.0, 0.1, 0.5, 1.0], 'colsample_bytree': [0.5, 0.7, 1.0], 'subsample': [0.5, 0.7, 1.0]}\n",
      "\n",
      "I have chosen to use RandomizedSearchCV for the hyper parameter search on the LGBMRegressor model. This is because RandomizedSearchCV is a good choice when the parameter space is large and it is not feasible to exhaustively search all possible combinations. Additionally, RandomizedSearchCV can be more efficient than GridSearchCV as it randomly samples a subset of the parameter space, which can lead to finding better hyperparameters in less time.\n",
      "\n",
      "I have chosen not to use the following algorithms:\n",
      "\n",
      "1. GridSearchCV: While GridSearchCV is a good choice when the parameter space is small, it can be computationally expensive and time-consuming when the parameter space is large, as in this case.\n",
      "\n",
      "2. HalvingGridSearchCV: HalvingGridSearchCV is a good choice when the parameter space is large and the model is computationally expensive to train. However, since LGBMRegressor is a relatively fast model to train, HalvingGridSearchCV may not provide significant benefits over RandomizedSearchCV.\n",
      "\n",
      "3. ParameterGrid: ParameterGrid is a good choice when the parameter space is small and discrete. However, since the parameter space in this case is large and continuous, ParameterGrid is not a suitable choice.\n",
      "\n",
      "4. ParameterSampler: ParameterSampler is a good choice when the parameter space is continuous and it is not feasible to exhaustively search all possible combinations. However, since RandomizedSearchCV already performs random sampling of the parameter space, ParameterSampler is not necessary.\n",
      "\n",
      "5. HalvingRandomSearchCV: HalvingRandomSearchCV is a good choice when the parameter space is large and the model is computationally expensive to train. However, since LGBMRegressor is a relatively fast model to train, HalvingRandomSearchCV may not provide significant benefits over RandomizedSearchCV.\n",
      "Deciding search algo:\n",
      "RandomizedSearchCV\n",
      "\n",
      "Constructing search algo:\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "clff = RandomizedSearchCV(clf, param_grid, random_state=0, n_iter=10)\n",
      "\n",
      "> \u001b[0;32m/tmp/ipykernel_1740627/2160592251.py\u001b[0m(29)\u001b[0;36minference\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     27 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     28 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 29 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     30 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     31 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mconstruct_search_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_algo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.locals.keys()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['X', 'Y', 'lgb', 'clf', 'param_grid', 'RandomizedSearchCV', 'clff'])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.globals.keys()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__builtins__'])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exec('global a\\na = 123')\n",
      "ipdb>  self.globals\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__builtins__': {'__name__': 'builtins', '__doc__': \"Built-in functions, exceptions, and other objects.\\n\\nNoteworthy: None is the `nil' object; Ellipsis represents `...' in slices.\", '__package__': '', '__loader__': <class '_frozen_importlib.BuiltinImporter'>, '__spec__': ModuleSpec(name='builtins', loader=<class '_frozen_importlib.BuiltinImporter'>), '__build_class__': <built-in function __build_class__>, '__import__': <built-in function __import__>, 'abs': <built-in function abs>, 'all': <built-in function all>, 'any': <built-in function any>, 'ascii': <built-in function ascii>, 'bin': <built-in function bin>, 'breakpoint': <built-in function breakpoint>, 'callable': <built-in function callable>, 'chr': <built-in function chr>, 'compile': <built-in function compile>, 'delattr': <built-in function delattr>, 'dir': <built-in function dir>, 'divmod': <built-in function divmod>, 'eval': <built-in function eval>, 'exec': <built-in function exec>, 'format': <built-in function format>, 'getattr': <built-in function getattr>, 'globals': <built-in function globals>, 'hasattr': <built-in function hasattr>, 'hash': <built-in function hash>, 'hex': <built-in function hex>, 'id': <built-in function id>, 'input': <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7fa26eb0c460>>, 'isinstance': <built-in function isinstance>, 'issubclass': <built-in function issubclass>, 'iter': <built-in function iter>, 'len': <built-in function len>, 'locals': <built-in function locals>, 'max': <built-in function max>, 'min': <built-in function min>, 'next': <built-in function next>, 'oct': <built-in function oct>, 'ord': <built-in function ord>, 'pow': <built-in function pow>, 'print': <built-in function print>, 'repr': <built-in function repr>, 'round': <built-in function round>, 'setattr': <built-in function setattr>, 'sorted': <built-in function sorted>, 'sum': <built-in function sum>, 'vars': <built-in function vars>, 'None': None, 'Ellipsis': Ellipsis, 'NotImplemented': NotImplemented, 'False': False, 'True': True, 'bool': <class 'bool'>, 'memoryview': <class 'memoryview'>, 'bytearray': <class 'bytearray'>, 'bytes': <class 'bytes'>, 'classmethod': <class 'classmethod'>, 'complex': <class 'complex'>, 'dict': <class 'dict'>, 'enumerate': <class 'enumerate'>, 'filter': <class 'filter'>, 'float': <class 'float'>, 'frozenset': <class 'frozenset'>, 'property': <class 'property'>, 'int': <class 'int'>, 'list': <class 'list'>, 'map': <class 'map'>, 'object': <class 'object'>, 'range': <class 'range'>, 'reversed': <class 'reversed'>, 'set': <class 'set'>, 'slice': <class 'slice'>, 'staticmethod': <class 'staticmethod'>, 'str': <class 'str'>, 'super': <class 'super'>, 'tuple': <class 'tuple'>, 'type': <class 'type'>, 'zip': <class 'zip'>, '__debug__': True, 'BaseException': <class 'BaseException'>, 'Exception': <class 'Exception'>, 'TypeError': <class 'TypeError'>, 'StopAsyncIteration': <class 'StopAsyncIteration'>, 'StopIteration': <class 'StopIteration'>, 'GeneratorExit': <class 'GeneratorExit'>, 'SystemExit': <class 'SystemExit'>, 'KeyboardInterrupt': <class 'KeyboardInterrupt'>, 'ImportError': <class 'ImportError'>, 'ModuleNotFoundError': <class 'ModuleNotFoundError'>, 'OSError': <class 'OSError'>, 'EnvironmentError': <class 'OSError'>, 'IOError': <class 'OSError'>, 'EOFError': <class 'EOFError'>, 'RuntimeError': <class 'RuntimeError'>, 'RecursionError': <class 'RecursionError'>, 'NotImplementedError': <class 'NotImplementedError'>, 'NameError': <class 'NameError'>, 'UnboundLocalError': <class 'UnboundLocalError'>, 'AttributeError': <class 'AttributeError'>, 'SyntaxError': <class 'SyntaxError'>, 'IndentationError': <class 'IndentationError'>, 'TabError': <class 'TabError'>, 'LookupError': <class 'LookupError'>, 'IndexError': <class 'IndexError'>, 'KeyError': <class 'KeyError'>, 'ValueError': <class 'ValueError'>, 'UnicodeError': <class 'UnicodeError'>, 'UnicodeEncodeError': <class 'UnicodeEncodeError'>, 'UnicodeDecodeError': <class 'UnicodeDecodeError'>, 'UnicodeTranslateError': <class 'UnicodeTranslateError'>, 'AssertionError': <class 'AssertionError'>, 'ArithmeticError': <class 'ArithmeticError'>, 'FloatingPointError': <class 'FloatingPointError'>, 'OverflowError': <class 'OverflowError'>, 'ZeroDivisionError': <class 'ZeroDivisionError'>, 'SystemError': <class 'SystemError'>, 'ReferenceError': <class 'ReferenceError'>, 'MemoryError': <class 'MemoryError'>, 'BufferError': <class 'BufferError'>, 'Warning': <class 'Warning'>, 'UserWarning': <class 'UserWarning'>, 'DeprecationWarning': <class 'DeprecationWarning'>, 'PendingDeprecationWarning': <class 'PendingDeprecationWarning'>, 'SyntaxWarning': <class 'SyntaxWarning'>, 'RuntimeWarning': <class 'RuntimeWarning'>, 'FutureWarning': <class 'FutureWarning'>, 'ImportWarning': <class 'ImportWarning'>, 'UnicodeWarning': <class 'UnicodeWarning'>, 'BytesWarning': <class 'BytesWarning'>, 'ResourceWarning': <class 'ResourceWarning'>, 'ConnectionError': <class 'ConnectionError'>, 'BlockingIOError': <class 'BlockingIOError'>, 'BrokenPipeError': <class 'BrokenPipeError'>, 'ChildProcessError': <class 'ChildProcessError'>, 'ConnectionAbortedError': <class 'ConnectionAbortedError'>, 'ConnectionRefusedError': <class 'ConnectionRefusedError'>, 'ConnectionResetError': <class 'ConnectionResetError'>, 'FileExistsError': <class 'FileExistsError'>, 'FileNotFoundError': <class 'FileNotFoundError'>, 'IsADirectoryError': <class 'IsADirectoryError'>, 'NotADirectoryError': <class 'NotADirectoryError'>, 'InterruptedError': <class 'InterruptedError'>, 'PermissionError': <class 'PermissionError'>, 'ProcessLookupError': <class 'ProcessLookupError'>, 'TimeoutError': <class 'TimeoutError'>, 'open': <built-in function open>, 'copyright': Copyright (c) 2001-2021 Python Software Foundation.\n",
      "All Rights Reserved.\n",
      "\n",
      "Copyright (c) 2000 BeOpen.com.\n",
      "All Rights Reserved.\n",
      "\n",
      "Copyright (c) 1995-2001 Corporation for National Research Initiatives.\n",
      "All Rights Reserved.\n",
      "\n",
      "Copyright (c) 1991-1995 Stichting Mathematisch Centrum, Amsterdam.\n",
      "All Rights Reserved., 'credits':     Thanks to CWI, CNRI, BeOpen.com, Zope Corporation and a cast of thousands\n",
      "    for supporting Python development.  See www.python.org for more information., 'license': Type license() to see the full license text, 'help': Type help() for interactive help, or help(object) for help about object., 'execfile': <function execfile at 0x7fa26f37d1f0>, 'runfile': <function runfile at 0x7fa26efe3550>, '__IPYTHON__': True, 'display': <function display at 0x7fa270359550>, '__pybind11_internals_v4_gcc_libstdcpp_cxxabi1014__': <capsule object NULL at 0x7fa19c654210>, 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7fa26eb0cfa0>>}}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.globals.keys()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__builtins__'])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.locals.keys()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['X', 'Y', 'lgb', 'clf', 'param_grid', 'RandomizedSearchCV', 'clff'])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  eval('global a\\na = 123')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** SyntaxError: invalid syntax\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  global a\n",
      "ipdb>  a = 123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self = <__main__.HyperOptTool object at 0x7fa2196a5460>\n",
      "query = 'LGBMRegressor'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.locals.keys()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['X', 'Y', 'lgb', 'clf', 'param_grid', 'RandomizedSearchCV', 'clff'])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.globals.keys()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__builtins__'])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  codes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NameError: name 'codes' is not defined\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exec(\"global a = 123\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** SyntaxError: invalid syntax\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exec(\"global a ; a = 123\")\n",
      "ipdb>  self.globals.keys()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__builtins__'])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exec(\"global a ; a = 123\", self.globals)\n",
      "ipdb>  exec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in function exec>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.globals.keys()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__builtins__', 'a'])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exec(\"global b ; b = 123 ; c = 456\", self.globals, self.locals)\n",
      "ipdb>  self.locals.keys()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['X', 'Y', 'lgb', 'clf', 'param_grid', 'RandomizedSearchCV', 'clff', 'c'])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.locals.keys()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['X', 'Y', 'lgb', 'clf', 'param_grid', 'RandomizedSearchCV', 'clff', 'c'])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.globals.keys()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__builtins__', 'a', 'b'])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pdb\n",
    "\n",
    "\n",
    "class E2EHyperOptTool:\n",
    "    globals: Optional[Dict] = dict()\n",
    "    locals: Optional[Dict] = dict()\n",
    "\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.locals = {\"X\": X, 'Y': Y}\n",
    "\n",
    "    @prompts(\n",
    "        name=\"hyperopt\",\n",
    "        description=\"This is a tool for hyper parameter tuning.\"\n",
    "        \"Input should be a estimator name.\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        clf = self.construct_estimator(query)\n",
    "        param_grid = self.construct_param_grid(clf)\n",
    "        search_algo = self.get_search_algo(clf, param_grid)\n",
    "        clff = self.construct_search_algo(search_algo)\n",
    "        self.search = clff.fit(self.X, self.Y)\n",
    "        return self.search.best_score_\n",
    "    \n",
    "    def construct_search_algo(self, search_algo):\n",
    "        # construct search instance\n",
    "        construct_search_prompt = f\"\"\"\n",
    "        Give me two lines of python code.\n",
    "        Frist, from lib import {search_algo.strip()}.\n",
    "        Second, instatiate an instance with {search_algo.strip()}(clf, param_grid, random_state=0, n_iter=10) and assign to clff variable.\n",
    "        \"\"\"\n",
    "        codes = chain.run(construct_search_prompt)\n",
    "        print(f\"Constructing search algo:\\n{codes}\\n\")\n",
    "        res = self.execute(codes)\n",
    "        if 'clff' not in self.locals:\n",
    "            raise Exception(res)\n",
    "        return self.locals['clff']\n",
    "    \n",
    "    def get_search_algo(self, clf, param_grid):\n",
    "        search_algo_reason_prompt = f\"\"\"\n",
    "        You are now performing a hyper parameter search on a machine learning model.\n",
    "        Your goal is to decide the search algorithm base on model type and parameter space.\n",
    "\n",
    "\n",
    "        The given model is a {clf.__class__.__name__}.\n",
    "        The paramters grid is {param_grid}.\n",
    "        The avaiable search algorithm is as follow:\n",
    "\n",
    "        1. GridSearchCV: Exhaustive search over specified parameter values for an estimator.\n",
    "        2. HalvingGridSearchCV: Search over specified parameter values with successive halving.\n",
    "        3. ParameterGrid: Grid of parameters with a discrete number of values for each.\n",
    "        4. ParameterSampler: Generator on parameters sampled from given distributions.\n",
    "        5. RandomizedSearchCV: Randomized search on hyper parameters.\n",
    "        6. HalvingRandomSearchCV: Randomized search on hyper parameters with successive halving.\n",
    "\n",
    "        Think about which algorithm to use base on the model, parameter space and algorithm description.\n",
    "        Tell me which algorithm you have choosen and why.\n",
    "        List all the other algorithms you have choosen not to use and why.\n",
    "        \"\"\"\n",
    "        \n",
    "        reason = chain.run(search_algo_reason_prompt)\n",
    "        print(reason)\n",
    "\n",
    "        search_algo = chain.run(f\"{reason}\\n\\nAccording to above, give me the algorithm name of the choosen algorithm in a single token.\")\n",
    "        print(f\"Deciding search algo:\\n{search_algo}\\n\")\n",
    "        return search_algo\n",
    "\n",
    "    def construct_param_grid(self, clf):\n",
    "        param_grid_prompt = f\"\"\"\n",
    "        You are now performing a hyper parameter search on a machine learning model.\n",
    "        Your goal is to construct the paramter space to search.\n",
    "\n",
    "        For example:\n",
    "        The model is a LinearSVR.\n",
    "        The parameters are {LinearSVR().get_params()}\n",
    "        The parameter grid can be defined as:\n",
    "        ```\n",
    "        param_grid = {{'C': [1, 10, 100, 1000], 'tol': [0.001, 0.0001]}}\n",
    "        ```\n",
    "        Remeber to define the space only on important features. \n",
    "\n",
    "\n",
    "        Now.\n",
    "        The given model is a {clf.__class__.__name__}.\n",
    "        The paramters are {clf.get_params()}.\n",
    "        The detail description about the parameters is as follow:\n",
    "        {clf.__doc__}\n",
    "\n",
    "        Construct the parameter grid you will use for hyper parameter in json format:\n",
    "        \"\"\"\n",
    "\n",
    "        param_grid = chain.run(hypopt_prompt)\n",
    "        param_grid = json.loads(param_grid)\n",
    "        print(f\"Construct params grid:\\n{param_grid}\\n\")\n",
    "        self.locals['param_grid'] = param_grid\n",
    "        return param_grid\n",
    "        \n",
    "    def construct_estimator(self, estimator_name: str):\n",
    "        # construct clf instance\n",
    "        construct_estimator_prompt = f\"Give me two lines of python code. Construct a {query.strip()} estimator and assign the estimator clf variable.\"\n",
    "        codes = chain.run(construct_estimator_prompt)\n",
    "        print(f\"Construct estimator with following:\\n{codes}\\n\")\n",
    "        res = self.execute(codes)\n",
    "        if 'clf' not in self.locals:\n",
    "            raise Exception(res)\n",
    "        print(f\"Results:\\n{self.locals['clf']}\\n\")\n",
    "        return self.locals['clf']\n",
    "        \n",
    "    def execute(self, cmd: str):\n",
    "        try:\n",
    "            tree = ast.parse(cmd)\n",
    "            module = ast.Module(tree.body[:-1], type_ignores=[])\n",
    "            exec(astunparse.unparse(module), self.globals, self.locals)  # type: ignore\n",
    "            module_end = ast.Module(tree.body[-1:], type_ignores=[])\n",
    "            module_end_str = astunparse.unparse(module_end)  # type: ignore\n",
    "            try:\n",
    "                return eval(module_end_str, self.globals, self.locals)\n",
    "            except Exception:\n",
    "                exec(module_end_str, self.globals, self.locals)\n",
    "                return \"\"\n",
    "        except Exception as e:\n",
    "            return str(e)\n",
    "        \n",
    "tool = HyperOptTool(X_valid, y_valid)\n",
    "tool.inference(\"LGBMRegressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pdb\n",
    "\n",
    "\n",
    "class E2EHyperOptTool:\n",
    "    globals: Optional[Dict] = dict()\n",
    "    locals: Optional[Dict] = dict()\n",
    "\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.locals = {\"X\": X, 'Y': Y}\n",
    "\n",
    "    @prompts(\n",
    "        name=\"hyperopt\",\n",
    "        description=\"This is a tool for hyper parameter tuning.\"\n",
    "        \"Input should be a estimator name.\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        clf = self.construct_estimator(query)\n",
    "        param_grid = self.construct_param_grid(clf)\n",
    "        search_algo = self.get_search_algo(clf, param_grid)\n",
    "        clff = self.construct_search_algo(search_algo)\n",
    "        self.search = clff.fit(self.X, self.Y)\n",
    "        return self.search.best_score_\n",
    "    \n",
    "    def construct_search_algo(self, search_algo):\n",
    "        # construct search instance\n",
    "        construct_search_prompt = f\"\"\"\n",
    "        Give me two lines of python code.\n",
    "        Frist, from lib import {search_algo.strip()}.\n",
    "        Second, instatiate an instance with {search_algo.strip()}(clf, param_grid, random_state=0, n_iter=10) and assign to clff variable.\n",
    "        \"\"\"\n",
    "        codes = chain.run(construct_search_prompt)\n",
    "        print(f\"Constructing search algo:\\n{codes}\\n\")\n",
    "        res = self.execute(codes)\n",
    "        if 'clff' not in self.locals:\n",
    "            raise Exception(res)\n",
    "        return self.locals['clff']\n",
    "    \n",
    "    def get_search_algo(self, clf, param_grid):\n",
    "        search_algo_reason_prompt = f\"\"\"\n",
    "        You are now performing a hyper parameter search on a machine learning model.\n",
    "        Your goal is to decide the search algorithm base on model type and parameter space.\n",
    "\n",
    "\n",
    "        The given model is a {clf.__class__.__name__}.\n",
    "        The paramters grid is {param_grid}.\n",
    "        The avaiable search algorithm is as follow:\n",
    "\n",
    "        1. GridSearchCV: Exhaustive search over specified parameter values for an estimator.\n",
    "        2. HalvingGridSearchCV: Search over specified parameter values with successive halving.\n",
    "        3. ParameterGrid: Grid of parameters with a discrete number of values for each.\n",
    "        4. ParameterSampler: Generator on parameters sampled from given distributions.\n",
    "        5. RandomizedSearchCV: Randomized search on hyper parameters.\n",
    "        6. HalvingRandomSearchCV: Randomized search on hyper parameters with successive halving.\n",
    "\n",
    "        Think about which algorithm to use base on the model, parameter space and algorithm description.\n",
    "        Tell me which algorithm you have choosen and why.\n",
    "        List all the other algorithms you have choosen not to use and why.\n",
    "        \"\"\"\n",
    "        \n",
    "        reason = chain.run(search_algo_reason_prompt)\n",
    "        print(reason)\n",
    "\n",
    "        search_algo = chain.run(f\"{reason}\\n\\nAccording to above, give me the algorithm name of the choosen algorithm in a single token.\")\n",
    "        print(f\"Deciding search algo:\\n{search_algo}\\n\")\n",
    "        return search_algo\n",
    "\n",
    "    def construct_param_grid(self, clf):\n",
    "        param_grid_prompt = f\"\"\"\n",
    "        You are now performing a hyper parameter search on a machine learning model.\n",
    "        Your goal is to construct the paramter space to search.\n",
    "\n",
    "        For example:\n",
    "        The model is a LinearSVR.\n",
    "        The parameters are {LinearSVR().get_params()}\n",
    "        The parameter grid can be defined as:\n",
    "        ```\n",
    "        param_grid = {{'C': [1, 10, 100, 1000], 'tol': [0.001, 0.0001]}}\n",
    "        ```\n",
    "        Remeber to define the space only on important features. \n",
    "\n",
    "\n",
    "        Now.\n",
    "        The given model is a {clf.__class__.__name__}.\n",
    "        The paramters are {clf.get_params()}.\n",
    "        The detail description about the parameters is as follow:\n",
    "        {clf.__doc__}\n",
    "\n",
    "        Construct the parameter grid you will use for hyper parameter in json format:\n",
    "        \"\"\"\n",
    "\n",
    "        param_grid = chain.run(hypopt_prompt)\n",
    "        param_grid = json.loads(param_grid)\n",
    "        print(f\"Construct params grid:\\n{param_grid}\\n\")\n",
    "        self.locals['param_grid'] = param_grid\n",
    "        return param_grid\n",
    "        \n",
    "    def construct_estimator(self, estimator_name: str):\n",
    "        # construct clf instance\n",
    "        construct_estimator_prompt = f\"Give me two lines of python code. Construct a {query.strip()} estimator and assign the estimator clf variable.\"\n",
    "        codes = chain.run(construct_estimator_prompt)\n",
    "        print(f\"Construct estimator with following:\\n{codes}\\n\")\n",
    "        res = self.execute(codes)\n",
    "        if 'clf' not in self.locals:\n",
    "            raise Exception(res)\n",
    "        print(f\"Results:\\n{self.locals['clf']}\\n\")\n",
    "        return self.locals['clf']\n",
    "        \n",
    "    def execute(self, cmd: str):\n",
    "        try:\n",
    "            tree = ast.parse(cmd)\n",
    "            module = ast.Module(tree.body[:-1], type_ignores=[])\n",
    "            exec(astunparse.unparse(module), self.globals, self.locals)  # type: ignore\n",
    "            module_end = ast.Module(tree.body[-1:], type_ignores=[])\n",
    "            module_end_str = astunparse.unparse(module_end)  # type: ignore\n",
    "            try:\n",
    "                return eval(module_end_str, self.globals, self.locals)\n",
    "            except Exception:\n",
    "                exec(module_end_str, self.globals, self.locals)\n",
    "                return \"\"\n",
    "        except Exception as e:\n",
    "            return str(e)\n",
    "        \n",
    "tool = HyperOptTool(X_valid, y_valid)\n",
    "tool.inference(\"LGBMRegressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examining the dataset can provide insights into which model to use. For example, if the target variable is categorical (e.g. wine quality is rated as low, medium, or high), then a classification model may be appropriate. If the target variable is continuous (e.g. alcohol content), then a regression model may be more suitable. Additionally, examining the distribution and correlation of the features can help determine which models may be effective. For example, if there are strong linear relationships between the features and the target variable, a linear regression model may be appropriate. If there are complex non-linear relationships, a neural network or decision tree model may be more effective. Finally, examining the size and complexity of the dataset can help determine which models are feasible to use. If the dataset is very large, models that can handle big data such as random forests or gradient boosting may be more appropriate.\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "def examine_dataset(X, y):\n",
      "    \"\"\"\n",
      "    Examines the dataset to determine which model may be appropriate.\n",
      "    \n",
      "    Parameters:\n",
      "    X (array-like): The feature matrix.\n",
      "    y (array-like): The target variable.\n",
      "    \n",
      "    Returns:\n",
      "    result (str): A string summarizing the findings.\n",
      "    \"\"\"\n",
      "    \n",
      "    # Check if target variable is categorical or continuous\n",
      "    if isinstance(y[0], str):\n",
      "        result = \"The target variable is categorical, so a classification model may be appropriate.\\n\"\n",
      "    else:\n",
      "        result = \"The target variable is continuous, so a regression model may be more suitable.\\n\"\n",
      "    \n",
      "    # Check for linear relationships between features and target variable\n",
      "    corr_matrix = np.corrcoef(X.T, y)\n",
      "    corr_values = corr_matrix[:-1, -1]\n",
      "    if np.all(np.abs(corr_values) > 0.5):\n",
      "        result += \"There are strong linear relationships between the features and the target variable, so a linear regression model may be appropriate.\\n\"\n",
      "    else:\n",
      "        result += \"There are no strong linear relationships between the features and the target variable, so a linear regression model may not be appropriate.\\n\"\n",
      "    \n",
      "    # Check for complex non-linear relationships between features and target variable\n",
      "    if np.any(np.abs(corr_values) > 0.8):\n",
      "        result += \"There are complex non-linear relationships between the features and the target variable, so a neural network or decision tree model may be more effective.\\n\"\n",
      "    else:\n",
      "        result += \"There are no complex non-linear relationships between the features and the target variable, so a neural network or decision tree model may not be necessary.\\n\"\n",
      "    \n",
      "    # Check if dataset is large\n",
      "    if X.shape[0] > 10000:\n",
      "        result += \"The dataset is very large, so models that can handle big data such as random forests or gradient boosting may be more appropriate.\\n\"\n",
      "    else:\n",
      "        result += \"The dataset is not very large, so models that can handle big data may not be necessary.\\n\"\n",
      "    \n",
      "    return result\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# know_how = chain.run(\"How to decide which machine learning model to use ?\")\n",
    "# thought = chain.run(know_how + f\"\"\"\\n\\n\n",
    "# This is the dataframe:\n",
    "# {X.head(30)}\n",
    "\n",
    "# Tell me how examine the dataset that can help you with deciding which model to use.\n",
    "# \"\"\")\n",
    "print(thought)\n",
    "print()\n",
    "# code = chain.run(f\"Thoughts:\\n{thought}\\n\\Write a python function to accomplish the thoughts above. Do not include model fitting. Do not include any besides the python function itself.\")\n",
    "\n",
    "prompt = f\"\"\"Thoughts:\\n{thought}\\n\n",
    "Write a Python function called `examine_dataset` to examine the dataset.\n",
    "Import any package you need beforehand.\n",
    "Do not include visualization and plotting.\n",
    "The function will be called using `result = examine_dataset(X, y)`, and all findings must be stored in a string variable `result`.\n",
    "\n",
    "\"\"\"\n",
    "exmain_dataset_thought = chain.run(prompt)\n",
    "exmain_dataset_code = extract_code(exmain_dataset_thought)\n",
    "print(exmain_dataset_code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_code(text):\n",
    "    texts = text.split(\"```\")\n",
    "    code = None\n",
    "    if len(texts) > 1:\n",
    "        code = texts[1]\n",
    "        code = code.strip(\"python\").strip(\"Python\")\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(exmain_dataset_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The target variable is continuous, so a regression model may be more suitable.\\nThere are no strong linear relationships between the features and the target variable, so a linear regression model may not be appropriate.\\nThere are no complex non-linear relationships between the features and the target variable, so a neural network or decision tree model may not be necessary.\\nThe dataset is not very large, so models that can handle big data may not be necessary.\\n'"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exmain_dataset_result = examine_dataset(X, y)\n",
    "exmain_dataset_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:\n",
      "Based on the examine result, since there are no strong linear or complex non-linear relationships between the features and the target variable, a simple and fast model such as a decision tree or random forest may be appropriate for achieving the best prediction results.\n",
      "\n",
      "```python\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "def construct_model():\n",
      "    \"\"\"\n",
      "    Constructs a random forest regression model.\n",
      "    \n",
      "    Returns:\n",
      "    clf (RandomForestRegressor): A random forest regression model.\n",
      "    \"\"\"\n",
      "    \n",
      "    clf = RandomForestRegressor()\n",
      "    \n",
      "    return clf\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "final_prompt = f\"\"\"\n",
    "Human:\\n{prompt}\n",
    "AI:\\n{exmain_dataset_thought}\n",
    "Human:\\nExecute result: {exmain_dataset_result}\n",
    "\n",
    "What model will you use to achieve the best prediction results according to the examine result?\n",
    "Write a Python function called `construct_model` for create the model instance.\n",
    "Initiate the model with no parameters.\n",
    "Ensure this model can achieve best prediction results.\n",
    "Ensure this model is fast.\n",
    "Import any package you need beforehand.\n",
    "Do not include visualization and plotting.\n",
    "The function will be called using `clf = construct_model()`, and the constructed model will be stored in a string variable `clf`.\n",
    "\"\"\"\n",
    "model_selection_thought = chain.run(final_prompt)\n",
    "print(model_selection_thought)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection_code = extract_code(model_selection_thought)\n",
    "exec(model_selection_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pdb\n",
    "\n",
    "\n",
    "class E2EModelSelectionTool:\n",
    "    globals: Optional[Dict] = dict()\n",
    "    locals: Optional[Dict] = dict()\n",
    "\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.locals = {\"X\": X, 'Y': Y}\n",
    "\n",
    "    @prompts(\n",
    "        name=\"model_selection\",\n",
    "        description=\"This is a tool for machine learning model selection.\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        understand(query)\n",
    "        \n",
    "        \n",
    "    def understand(self, query: str):\n",
    "        know_how = chain.run(\"How to perform machine learning model selection ?\")\n",
    "        inspect_code = f\"\"\"\n",
    "        This is the dataframe:\n",
    "        {self.X.head(30)}\n",
    "\n",
    "        First, tell me your thought about how examine the dataset can help you with model selection.\n",
    "        Second, write a python function that help you to accomplish your thought. Compress the output result as much as possible.\n",
    "        \"\"\"\n",
    "        codes = chain.run(know_how + '\\n\\n' + inspect_code)\n",
    "    \n",
    "    def construct_search_algo(self, search_algo):\n",
    "        # construct search instance\n",
    "        construct_search_prompt = f\"\"\"\n",
    "        Give me two lines of python code.\n",
    "        Frist, from lib import {search_algo.strip()}.\n",
    "        Second, instatiate an instance with {search_algo.strip()}(clf, param_grid, random_state=0, n_iter=10) and assign to clff variable.\n",
    "        \"\"\"\n",
    "        codes = chain.run(construct_search_prompt)\n",
    "        print(f\"Constructing search algo:\\n{codes}\\n\")\n",
    "        res = self.execute(codes)\n",
    "        if 'clff' not in self.locals:\n",
    "            raise Exception(res)\n",
    "        return self.locals['clff']\n",
    "    \n",
    "    def get_search_algo(self, clf, param_grid):\n",
    "        search_algo_reason_prompt = f\"\"\"\n",
    "        You are now performing a hyper parameter search on a machine learning model.\n",
    "        Your goal is to decide the search algorithm base on model type and parameter space.\n",
    "\n",
    "\n",
    "        The given model is a {clf.__class__.__name__}.\n",
    "        The paramters grid is {param_grid}.\n",
    "        The avaiable search algorithm is as follow:\n",
    "\n",
    "        1. GridSearchCV: Exhaustive search over specified parameter values for an estimator.\n",
    "        2. HalvingGridSearchCV: Search over specified parameter values with successive halving.\n",
    "        3. ParameterGrid: Grid of parameters with a discrete number of values for each.\n",
    "        4. ParameterSampler: Generator on parameters sampled from given distributions.\n",
    "        5. RandomizedSearchCV: Randomized search on hyper parameters.\n",
    "        6. HalvingRandomSearchCV: Randomized search on hyper parameters with successive halving.\n",
    "\n",
    "        Think about which algorithm to use base on the model, parameter space and algorithm description.\n",
    "        Tell me which algorithm you have choosen and why.\n",
    "        List all the other algorithms you have choosen not to use and why.\n",
    "        \"\"\"\n",
    "        \n",
    "        reason = chain.run(search_algo_reason_prompt)\n",
    "        print(reason)\n",
    "\n",
    "        search_algo = chain.run(f\"{reason}\\n\\nAccording to above, give me the algorithm name of the choosen algorithm in a single token.\")\n",
    "        print(f\"Deciding search algo:\\n{search_algo}\\n\")\n",
    "        return search_algo\n",
    "\n",
    "    def construct_param_grid(self, clf):\n",
    "        param_grid_prompt = f\"\"\"\n",
    "        You are now performing a hyper parameter search on a machine learning model.\n",
    "        Your goal is to construct the paramter space to search.\n",
    "\n",
    "        For example:\n",
    "        The model is a LinearSVR.\n",
    "        The parameters are {LinearSVR().get_params()}\n",
    "        The parameter grid can be defined as:\n",
    "        ```\n",
    "        param_grid = {{'C': [1, 10, 100, 1000], 'tol': [0.001, 0.0001]}}\n",
    "        ```\n",
    "        Remeber to define the space only on important features. \n",
    "\n",
    "\n",
    "        Now.\n",
    "        The given model is a {clf.__class__.__name__}.\n",
    "        The paramters are {clf.get_params()}.\n",
    "        The detail description about the parameters is as follow:\n",
    "        {clf.__doc__}\n",
    "\n",
    "        Construct the parameter grid you will use for hyper parameter in json format:\n",
    "        \"\"\"\n",
    "\n",
    "        param_grid = chain.run(hypopt_prompt)\n",
    "        param_grid = json.loads(param_grid)\n",
    "        print(f\"Construct params grid:\\n{param_grid}\\n\")\n",
    "        self.locals['param_grid'] = param_grid\n",
    "        return param_grid\n",
    "        \n",
    "    def construct_estimator(self, estimator_name: str):\n",
    "        # construct clf instance\n",
    "        construct_estimator_prompt = f\"Give me two lines of python code. Construct a {query.strip()} estimator and assign the estimator clf variable.\"\n",
    "        codes = chain.run(construct_estimator_prompt)\n",
    "        print(f\"Construct estimator with following:\\n{codes}\\n\")\n",
    "        res = self.execute(codes)\n",
    "        if 'clf' not in self.locals:\n",
    "            raise Exception(res)\n",
    "        print(f\"Results:\\n{self.locals['clf']}\\n\")\n",
    "        return self.locals['clf']\n",
    "        \n",
    "    def execute(self, cmd: str):\n",
    "        try:\n",
    "            tree = ast.parse(cmd)\n",
    "            module = ast.Module(tree.body[:-1], type_ignores=[])\n",
    "            exec(astunparse.unparse(module), self.globals, self.locals)  # type: ignore\n",
    "            module_end = ast.Module(tree.body[-1:], type_ignores=[])\n",
    "            module_end_str = astunparse.unparse(module_end)  # type: ignore\n",
    "            try:\n",
    "                return eval(module_end_str, self.globals, self.locals)\n",
    "            except Exception:\n",
    "                exec(module_end_str, self.globals, self.locals)\n",
    "                return \"\"\n",
    "        except Exception as e:\n",
    "            return str(e)\n",
    "        \n",
    "tool = HyperOptTool(X_valid, y_valid)\n",
    "tool.inference(\"LGBMRegressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    'name': 'RandomizedSearchCV',\n",
      "    'reason': 'The parameter space is quite large and randomized search can efficiently explore a wide range of hyperparameters. It also allows for more flexibility in the search process compared to GridSearchCV.',\n",
      "    'not use reason': {\n",
      "        'GridSearchCV': 'The parameter space is too large for an exhaustive search.',\n",
      "        'HalvingGridSearchCV': 'The parameter space is not too large for successive halving.',\n",
      "        'ParameterGrid': 'ParameterGrid only explores a fixed set of hyperparameters and does not allow for flexibility in the search process.',\n",
      "        'ParameterSampler': 'ParameterSampler only samples from given distributions and does not explore the entire parameter space.',\n",
      "        'HalvingRandomSearchCV': 'HalvingRandomSearchCV is not necessary as the parameter space is not too large for randomized search.'\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "search_algo_prompt = f\"\"\"\n",
    "You are now performing a hyper parameter search on a machine learning model.\n",
    "Your goal is to decide the search algorithm base on model type and parameter space.\n",
    "\n",
    "\n",
    "The given model is a {clf.__class__.__name__}.\n",
    "The paramters grid is {param_grid}.\n",
    "The avaiable search algorithm is as follow:\n",
    "\n",
    "1. GridSearchCV: Exhaustive search over specified parameter values for an estimator.\n",
    "2. HalvingGridSearchCV: Search over specified parameter values with successive halving.\n",
    "3. ParameterGrid: Grid of parameters with a discrete number of values for each.\n",
    "4. ParameterSampler: Generator on parameters sampled from given distributions.\n",
    "5. RandomizedSearchCV: Randomized search on hyper parameters.\n",
    "6. HalvingRandomSearchCV: Randomized search on hyper parameters with successive halving.\n",
    "\n",
    "Think about which algorithm to use base on the model, parameter space and algorithm description.\n",
    "Output the final results in the following json format:\n",
    "{{\n",
    "    'name': Name,\n",
    "    'reason': Reason\n",
    "    'not use reason': {{\n",
    "        Algo1: Reason,\n",
    "        Algo2: Reason,\n",
    "        ...\n",
    "    }},\n",
    "}}\n",
    "The algorithm name and the reason to use it.\n",
    "List out all the other algorithm with the reason not to use it.\n",
    "\"\"\"\n",
    "\n",
    "search_algo = chain.run(search_algo_prompt)\n",
    "print(search_algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I suggest you try the parameter values (45, 10) as this is a combination of values that has not been tested yet and should give you a good test score.\n",
      "['45', '10']\n",
      "test_score of (45, 10) is 0.4467368153352195\n",
      " I suggest you try the parameter values (60, 20) as this is the combination of the highest values of H1 and H2 within the given intervals. This should give you the best possible test score that has not been tested yet.\n",
      "['60', '20']\n",
      "test_score of (60, 20) is 0.4092228313447275\n",
      " I suggest you try the parameter values (30, 2) as this is the lowest possible combination of H1 and H2 values within the given intervals.\n",
      "['30', '2']\n",
      "test_score of (30, 2) is 0.45939565627950896\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 70\u001b[0m\n\u001b[0;32m     68\u001b[0m scores \u001b[39m=\u001b[39m []\n\u001b[0;32m     69\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> 70\u001b[0m     H1, H2 \u001b[39m=\u001b[39m get_record_H1H2(record_message, record_list)\n\u001b[0;32m     72\u001b[0m     model \u001b[39m=\u001b[39m RandomForestClassifier(\n\u001b[0;32m     73\u001b[0m         n_estimators\u001b[39m=\u001b[39mH1, min_samples_split\u001b[39m=\u001b[39mH2, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m     74\u001b[0m     model\u001b[39m.\u001b[39mfit(X_train, y_train)\n",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m, in \u001b[0;36mget_record_H1H2\u001b[1;34m(record_message, record_list, lower_H1, upper_H1, lower_H2, upper_H2)\u001b[0m\n\u001b[0;32m      3\u001b[0m llm \u001b[39m=\u001b[39m OpenAI(temperature\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m)\n\u001b[0;32m      4\u001b[0m gpt \u001b[39m=\u001b[39m ConversationChain(\n\u001b[0;32m      5\u001b[0m     llm\u001b[39m=\u001b[39mllm,\n\u001b[0;32m      6\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m      7\u001b[0m     memory\u001b[39m=\u001b[39mConversationBufferMemory()\n\u001b[0;32m      8\u001b[0m )\n\u001b[1;32m----> 9\u001b[0m r \u001b[39m=\u001b[39m gpt\u001b[39m.\u001b[39;49mpredict(\u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mA simple way to setup a grid search consists in defining a vector of lower bounds \u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m            a = (a1, a2, …, am) and a vector of upper bounds b = (b1, b2, …, bm) for each component of \u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m            ν. Grid search involves taking n equally spaced points in each interval of the form [ai, bi]\u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m            including ai and bi. This creates a total of nm possible grid points to check. Finally, once\u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m            each pair of points is calculated, the maximum of these values is chosen.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     14\u001b[0m \u001b[39m#print(r)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m k \u001b[39m=\u001b[39m gpt\u001b[39m.\u001b[39mpredict(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgiven hyperparameter H = [H1, H2] where H1 in  [30, 60], H2 in [2, 20]. what is the grid\u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m                    search space?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\chains\\llm.py:151\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m    138\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \n\u001b[0;32m    140\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[39m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\chains\\base.py:116\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m--> 116\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_end(outputs, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m    118\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\chains\\base.py:113\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    108\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[0;32m    109\u001b[0m     inputs,\n\u001b[0;32m    110\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[0;32m    111\u001b[0m )\n\u001b[0;32m    112\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs)\n\u001b[0;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\chains\\llm.py:57\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\u001b[39mself\u001b[39m, inputs: Dict[\u001b[39mstr\u001b[39m, Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply([inputs])[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\chains\\llm.py:118\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[1;34m(self, input_list)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, input_list: List[Dict[\u001b[39mstr\u001b[39m, Any]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]]:\n\u001b[0;32m    117\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Utilize the LLM generate method for speed gains.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(input_list)\n\u001b[0;32m    119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\chains\\llm.py:62\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[0;32m     61\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list)\n\u001b[1;32m---> 62\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(prompts, stop)\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\llms\\base.py:107\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[0;32m    104\u001b[0m     \u001b[39mself\u001b[39m, prompts: List[PromptValue], stop: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    105\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[0;32m    106\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[1;32m--> 107\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_strings, stop\u001b[39m=\u001b[39;49mstop)\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\llms\\base.py:140\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m--> 140\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    141\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_end(output, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m    142\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\llms\\base.py:137\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[0;32m    134\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m}, prompts, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose\n\u001b[0;32m    135\u001b[0m )\n\u001b[0;32m    136\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(prompts, stop\u001b[39m=\u001b[39;49mstop)\n\u001b[0;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\llms\\openai.py:281\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[1;34m(self, prompts, stop)\u001b[0m\n\u001b[0;32m    279\u001b[0m     choices\u001b[39m.\u001b[39mextend(response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    280\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 281\u001b[0m     response \u001b[39m=\u001b[39m completion_with_retry(\u001b[39mself\u001b[39m, prompt\u001b[39m=\u001b[39m_prompts, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m    282\u001b[0m     choices\u001b[39m.\u001b[39mextend(response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstreaming:\n\u001b[0;32m    284\u001b[0m     \u001b[39m# Can't update token usage if streaming\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\llms\\openai.py:99\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[1;34m(llm, **kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m     97\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 99\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(f, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[0;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tenacity\\__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[0;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[1;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[0;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[0;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\llms\\openai.py:97\u001b[0m, in \u001b[0;36mcompletion_with_retry.<locals>._completion_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m---> 97\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_resources\\completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_requestor.py:216\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    207\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m--> 216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[0;32m    217\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[0;32m    218\u001b[0m         url,\n\u001b[0;32m    219\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    220\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    221\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m    222\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    223\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    225\u001b[0m     )\n\u001b[0;32m    226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[0;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_requestor.py:516\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[1;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    514\u001b[0m     _thread_context\u001b[39m.\u001b[39msession \u001b[39m=\u001b[39m _make_session()\n\u001b[0;32m    515\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 516\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    517\u001b[0m         method,\n\u001b[0;32m    518\u001b[0m         abs_url,\n\u001b[0;32m    519\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    520\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[0;32m    521\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m    522\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    523\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[0;32m    524\u001b[0m         proxies\u001b[39m=\u001b[39;49m_thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mproxies,\n\u001b[0;32m    525\u001b[0m     )\n\u001b[0;32m    526\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    527\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    585\u001b[0m }\n\u001b[0;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    500\u001b[0m         )\n\u001b[0;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    704\u001b[0m     conn,\n\u001b[0;32m    705\u001b[0m     method,\n\u001b[0;32m    706\u001b[0m     url,\n\u001b[0;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    711\u001b[0m )\n\u001b[0;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[0;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[0;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1373\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1374\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1375\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[0;32m   1376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\fredp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_record_H1H2(record_message, record_list, lower_H1=30, upper_H1=60, lower_H2=2, upper_H2=20):\n",
    "    memory = ConversationBufferMemory()\n",
    "    llm = OpenAI(temperature=0.0)\n",
    "    gpt = ConversationChain(\n",
    "        llm=llm,\n",
    "        verbose=False,\n",
    "        memory=ConversationBufferMemory()\n",
    "    )\n",
    "    r = gpt.predict(input=\"A simple way to setup a grid search consists in defining a vector of lower bounds \\\n",
    "                a = (a1, a2, …, am) and a vector of upper bounds b = (b1, b2, …, bm) for each component of \\\n",
    "                ν. Grid search involves taking n equally spaced points in each interval of the form [ai, bi]\\\n",
    "                including ai and bi. This creates a total of nm possible grid points to check. Finally, once\\\n",
    "                each pair of points is calculated, the maximum of these values is chosen.\")\n",
    "    #print(r)\n",
    "    k = gpt.predict(input=\"given hyperparameter H = [H1, H2] where H1 in  [30, 60], H2 in [2, 20]. what is the grid\\\n",
    "                        search space?\")\n",
    "    #print(k)\n",
    "    g = gpt.predict(input=\"you are a parameter optimizer. I will give you several parameter names and the range of each\\\n",
    "                        parameter, I will give you all the parameters I have tested and the test score of each set\\\n",
    "                        of parameters in the format of ({H1}, {H2}) = {test score}. for example, (20, 2) = 0.48\\\n",
    "                        please give me another set of values, and I want the test score of the set of values\\\n",
    "                        you give me be as good as possible \")\n",
    "    #print(g)\n",
    "    response = gpt.predict(input=f\"given hyperparameter H = [H1, H2] where H1 in  [30, 60], H2 in [2, 20]. the tested parameters are:\\\n",
    "                        {record_message}, please give me another set of value that has not been tested yet and may have best\\\n",
    "                        test score\")\n",
    "    print(response)\n",
    "    response = response.split(\")\")[0]\n",
    "    response = response.split(\"(\")[-1]\n",
    "    integers = re.findall(r'\\d+', response)\n",
    "    print(integers)\n",
    "    H1, H2 = integers\n",
    "    H1, H2 = int(H1), int(H2)\n",
    "\n",
    "    while True:\n",
    "        if H1 > upper_H1 or H1 < lower_H1 or H2 > upper_H2 or H2 < lower_H2: \n",
    "            message = f'(H1, H2) = ({H1}, {H2}) is out of bound, please give me another set of values'\n",
    "            print(message)\n",
    "            response = gpt.predict(input=message)\n",
    "            print(response)\n",
    "            \n",
    "            response = response.split(\")\")[0]\n",
    "            response = response.split(\"(\")[-1]\n",
    "            integers = re.findall(r'\\d+', response)\n",
    "            H1, H2 = integers\n",
    "            H1, H2 = int(H1), int(H2)\n",
    "\n",
    "        elif (H1, H2) in record_list:\n",
    "            message = f'(H1, H2) = ({H1}, {H2}) is already tested, please give me another set of values. If all the suggestioned\\\n",
    "                         values have been tested, just give me a random one that has not been tested.'\n",
    "            print(message)\n",
    "            response = gpt.predict(input=message)\n",
    "            print(response)\n",
    "\n",
    "            response = response.split(\")\")[0]\n",
    "            response = response.split(\"(\")[-1]\n",
    "            integers = re.findall(r'\\d+', response)\n",
    "            H1, H2 = integers\n",
    "            H1, H2 = int(H1), int(H2)\n",
    "\n",
    "        else:\n",
    "            break\n",
    "    return H1, H2\n",
    "\n",
    "\n",
    "record_message = \"\"\n",
    "record_list = []\n",
    "scores = []\n",
    "while True:\n",
    "    H1, H2 = get_record_H1H2(record_message, record_list)\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=H1, min_samples_split=H2, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    test_score = cohen_kappa_score(y_test, y_pred_test)\n",
    "\n",
    "    print(f'test_score of ({H1}, {H2}) is {test_score}')\n",
    "    record_message += f\"({H1}, {H2}) = {test_score:.3f}\\n\"\n",
    "    record_list.append((H1, H2))\n",
    "    scores.append(test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
