{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b632fbe-16ae-4812-a408-9b375a577623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, Union, List, Dict, Tuple\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# import torch\n",
    "import asyncio\n",
    "import logging\n",
    "from functools import wraps, partial\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed_all(seed)\n",
    "    return seed\n",
    "\n",
    "\n",
    "def prompts(name: str, description: str):\n",
    "    def decorator(func):\n",
    "        func.name = name\n",
    "        func.description = description\n",
    "        return func\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def extract_code(text):\n",
    "    texts = text.split(\"```\")\n",
    "    code = None\n",
    "    if len(texts) > 1:\n",
    "        code = texts[1]\n",
    "        code = code.strip(\"python\").strip(\"Python\")\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "254de8d7-b2a4-4e4a-8ea0-ac1f804d349b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Union, List, Dict, Tuple\n",
    "from pydantic import Field\n",
    "import ast\n",
    "import astunparse\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def prompts(name: str, description: str):\n",
    "    def decorator(func):\n",
    "        func.name = name\n",
    "        func.description = description\n",
    "        return func\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def execute(cmd: str, glo, loc):\n",
    "    tree = ast.parse(cmd)\n",
    "    module = ast.Module(tree.body[:-1], type_ignores=[])\n",
    "    exec(astunparse.unparse(module), self.globals, self.locals)  # type: ignore\n",
    "    module_end = ast.Module(tree.body[-1:], type_ignores=[])\n",
    "    module_end_str = astunparse.unparse(module_end)  # type: ignore\n",
    "    try:\n",
    "        return eval(module_end_str, self.globals, self.locals)\n",
    "    except Exception:\n",
    "        exec(module_end_str, self.globals, self.locals)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d2b53ea-2062-4c56-b75f-45d0d51392d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = \"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\"\n",
    "# os.environ['OPENAI_API_KEY'] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fa12ef6-f654-4fab-9e45-877bb83df79a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import logging\n",
    "import threading\n",
    "from typing import Union\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from langchain.chains.conversation.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    # ConversationBufferWindowMemory,\n",
    "    # ConversationSummaryBufferMemory\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents.tools import Tool\n",
    "from langchain.agents.conversational.output_parser import ConvoOutputParser\n",
    "from langchain.agents.conversational.prompt import FORMAT_INSTRUCTIONS\n",
    "from langchain.agents.conversational_chat.prompt import (\n",
    "    FORMAT_INSTRUCTIONS as CHAT_FORMAT_INSTRUCTIONS,\n",
    ")\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "from langchain.agents import AgentOutputParser\n",
    "\n",
    "# from api.services.utils import prompts, seed_everything, to_async\n",
    "# from api.conversation.callback import AsyncLoggerCallbackHandler, LoggerCallbackHandler\n",
    "\n",
    "\n",
    "\n",
    "class retryActionTool:\n",
    "    \"\"\"Retry when chatgpt response parsing failed.\"\"\"\n",
    "\n",
    "    @prompts(\n",
    "        name=\"RR\",\n",
    "        description=\"Do not use it.\",\n",
    "    )\n",
    "    def inference(self, query):\n",
    "        return f\"Modify the following text in ``` such that it will follow RESPONSE FORMAT INSTRUCTIONS:```{query}```\\nDo not use retry tool again.\"\n",
    "\n",
    "\n",
    "class RetryConvoOutputParser(ConvoOutputParser):\n",
    "    ai_prefix: str = \"AI\"\n",
    "\n",
    "    def get_format_instructions(self) -> str:\n",
    "        return FORMAT_INSTRUCTIONS\n",
    "\n",
    "    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\n",
    "        if f\"{self.ai_prefix}:\" in text:\n",
    "            return AgentFinish(\n",
    "                {\"output\": text.split(f\"{self.ai_prefix}:\")[-1].strip()}, text\n",
    "            )\n",
    "        regex = r\"Action: (.*?)[\\n]*Action Input: (.*)\"\n",
    "        match = re.search(regex, text)\n",
    "        if not match:\n",
    "            return AgentAction(\"retry\", \"\", text)\n",
    "            # raise OutputParserException(f\"Could not parse LLM output: `{text}`\")\n",
    "        action = match.group(1)\n",
    "        action_input = match.group(2)\n",
    "        return AgentAction(action.strip(), action_input.strip(\" \").strip('\"'), text)\n",
    "\n",
    "\n",
    "class RobustRetryConvoOutputParser(AgentOutputParser):\n",
    "    def get_format_instructions(self) -> str:\n",
    "        return CHAT_FORMAT_INSTRUCTIONS\n",
    "\n",
    "    def parse_json(self, text: str):\n",
    "        try:\n",
    "            return [\n",
    "                i\n",
    "                for i in re.findall(r\"\\s([{\\[].*?[}\\]])$\", f\" {text}\", flags=re.DOTALL)\n",
    "                if i\n",
    "            ][0]\n",
    "        except Exception:\n",
    "            return\n",
    "\n",
    "    def parse_markdown_code(self, text: str):\n",
    "        try:\n",
    "            for i in re.findall(\n",
    "                r\"`{3}([\\w]*)\\n([\\S\\s]+?)\\n`{3}\", text, flags=re.DOTALL\n",
    "            )[0]:\n",
    "                if i:\n",
    "                    tmp = self.parse_json(i)\n",
    "                    if tmp:\n",
    "                        return tmp\n",
    "        except Exception:\n",
    "            return\n",
    "\n",
    "    def parse_origin(self, text: str):\n",
    "        try:\n",
    "            cleaned_output = text.strip()\n",
    "            if \"```json\" in cleaned_output:\n",
    "                _, cleaned_output = cleaned_output.split(\"```json\")\n",
    "            if \"```\" in cleaned_output:\n",
    "                cleaned_output, _ = cleaned_output.split(\"```\")\n",
    "            if cleaned_output.startswith(\"```json\"):\n",
    "                cleaned_output = cleaned_output[len(\"```json\"):]\n",
    "            if cleaned_output.startswith(\"```\"):\n",
    "                cleaned_output = cleaned_output[len(\"```\"):]\n",
    "            if cleaned_output.endswith(\"```\"):\n",
    "                cleaned_output = cleaned_output[: -len(\"```\")]\n",
    "            return cleaned_output.strip()\n",
    "        except Exception:\n",
    "            return\n",
    "\n",
    "    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\n",
    "        try:\n",
    "            json_str = self.parse_json(text)\n",
    "            markdown_str = self.parse_markdown_code(text)\n",
    "            origin_str = self.parse_origin(text)\n",
    "\n",
    "            if json_str:\n",
    "                cleaned_output = json_str\n",
    "            elif markdown_str:\n",
    "                cleaned_output = markdown_str\n",
    "                if cleaned_output.startswith(\"```json\"):\n",
    "                    cleaned_output = cleaned_output[len(\"```json\"):]\n",
    "                if cleaned_output.startswith(\"```\"):\n",
    "                    cleaned_output = cleaned_output[len(\"```\"):]\n",
    "                if cleaned_output.endswith(\"```\"):\n",
    "                    cleaned_output = cleaned_output[: -len(\"```\")]\n",
    "            elif origin_str:\n",
    "                cleaned_output = origin_str\n",
    "            else:\n",
    "                cleaned_output = text.strip()\n",
    "\n",
    "            response = json.loads(cleaned_output)\n",
    "            action, action_input = response[\"action\"], response[\"action_input\"]\n",
    "            if action == \"Final Answer\":\n",
    "                return AgentFinish({\"output\": action_input}, text)\n",
    "            else:\n",
    "                return AgentAction(action, action_input, text)\n",
    "        except Exception:\n",
    "            if \"action\" not in text and \"action_input\" not in text:\n",
    "                return AgentFinish({\"output\": text}, text)\n",
    "            else:\n",
    "                logger.warning(f\"Not follow format instruction\\n{cleaned_output}\")\n",
    "                return AgentAction(\"RR\", \"\", text)\n",
    "\n",
    "        \n",
    "class pythonShellTool:\n",
    "\n",
    "    @prompts(\n",
    "        name=\"python_repl_ast\",\n",
    "        description=\"This is a python shell. Use this to execute python commands. \"\n",
    "        \"Input should be a valid python command. \"\n",
    "        \"The output of the command should not be too long.\"\n",
    "        \"The dataset is a dataframe assign to a variable df\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        try:\n",
    "            tree = ast.parse(query)\n",
    "            module = ast.Module(tree.body[:-1], type_ignores=[])\n",
    "            exec(astunparse.unparse(module), self.globals, self.locals)  # type: ignore\n",
    "            module_end = ast.Module(tree.body[-1:], type_ignores=[])\n",
    "            module_end_str = astunparse.unparse(module_end)  # type: ignore\n",
    "            try:\n",
    "                return eval(module_end_str, self.globals, self.locals)[:1024]\n",
    "            except Exception:\n",
    "                exec(module_end_str, self.globals, self.locals)\n",
    "                return \"\"\n",
    "        except Exception as e:\n",
    "            return str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4830633-5499-44f3-bbc5-1741353fcb17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts.chat import (\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "human_message_prompt = HumanMessagePromptTemplate(\n",
    "        prompt=PromptTemplate(\n",
    "            template=\"{text}\",\n",
    "            input_variables=[\"text\"],\n",
    "        )\n",
    "    )\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt_template)#, memory=ConversationBufferMemory())\n",
    "\n",
    "\n",
    "\n",
    "class CodingAgent():\n",
    "    def __init__(self, chain):\n",
    "        self.chain = chain\n",
    "    @functools.lru_cache(100)\n",
    "    def run(self, text):\n",
    "        return self.chain.run(text)\n",
    "    \n",
    "    \n",
    "coding_agent = CodingAgent(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef371d4b-adea-4524-bbfe-d2635cbaa7f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from prompt import AUTOML_CHATGPT_PREFIX, AUTOML_CHATGPT_FORMAT_INSTRUCTIONS, AUTOML_CHATGPT_SUFFIX\n",
    "from langchain.agents.initialize import initialize_agent\n",
    "from langchain.agents.tools import Tool\n",
    "from sklearn.svm import LinearSVR\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from scipy.stats import boxcox\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96a6cdbe-3beb-4b45-8f19-50803da3436b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Status():\n",
    "    \n",
    "    def __init__(self, dev=True):\n",
    "        self.init()\n",
    "        self.dev = dev\n",
    "        \n",
    "    def init_data(self, dataset, X_train, X_test, y_train, y_test):\n",
    "        self.init()\n",
    "        self.dataset = dataset\n",
    "        self.X = X_train\n",
    "        self.y = y_train\n",
    "        \n",
    "        if self.dev:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.1, random_state=42)\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "    \n",
    "    def init(self):\n",
    "        \n",
    "        self.dataset = \"\"\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.y_pred = None\n",
    "        \n",
    "        self.start_explore_prompt = \"\"\n",
    "        self.how_to_explore_prompt = \"\"\n",
    "        self.explore_data_thought = \"\"\n",
    "        self.hyperopt_param_thought = \"\"\n",
    "        \n",
    "        self.explore_result = \"Nothing special.\"\n",
    "        \n",
    "        self.explore = False\n",
    "        self.preprocess = False\n",
    "        self.model_select = False\n",
    "        self.hyperopt = False\n",
    "        \n",
    "        self.explore_code = \"\"\n",
    "        self.preprocess_code = \"\"\n",
    "        self.model_select_code = \"\"\n",
    "        self.hyperopt_code = \"\"\n",
    "        \n",
    "        self.model = \"\"\n",
    "        self.best_param = {}\n",
    "\n",
    "status = Status(dev=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf955478-9990-468d-bbb0-b5573c4470eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['santander-customer-satisfaction', 'porto-seguro-safe-driver-prediction', 'santander-customer-transaction-prediction', 'house-prices-advanced-regression-techniques', 'santander-value-prediction-challenge', 'bnp-paribas-cardif-claims-management', 'ieee-fraud-detection', 'allstate-claims-severity', 'otto-group-product-classification-challenge', 'mercedes-benz-greener-manufacturing', '.ipynb_checkpoints']\n",
      "[Errno 2] No such file or directory: 'kaggle/ieee-fraud-detection/train.csv'\n",
      "None complete.\n",
      "house-prices-advanced-regression-techniques complete.\n",
      "mercedes-benz-greener-manufacturing complete.\n",
      "[Errno 2] No such file or directory: 'kaggle/.ipynb_checkpoints/train.csv'\n",
      "None complete.\n",
      "otto-group-product-classification-challenge complete.\n",
      "allstate-claims-severity complete.\n",
      "bnp-paribas-cardif-claims-management complete.\n",
      "porto-seguro-safe-driver-prediction complete.\n",
      "santander-customer-satisfaction complete.\n",
      "santander-customer-transaction-prediction complete.\n",
      "santander-value-prediction-challenge complete.\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "class DataLoader():\n",
    "    \n",
    "    def __init__(self, chain):\n",
    "        self.datasets = {}\n",
    "        self.chain = chain\n",
    "        \n",
    "    def select(self, name: str):\n",
    "        if name not in self.datasets:\n",
    "            prompt = f\"\"\"\n",
    "            These are the datasets we have:\n",
    "            {self.datasets.keys()}\n",
    "            If someone ask for {name}, what is the most possible dataset that is referred to ?\n",
    "            Give me the response in one single word:\n",
    "            \"\"\"\n",
    "            print(prompt)\n",
    "            name = self.chain.run(prompt)\n",
    "            return self.datasets[name]\n",
    "        else:\n",
    "            return self.datasets[name]\n",
    "        \n",
    "    def show(self):\n",
    "        print(self.datasets.keys())\n",
    "        \n",
    "    def load_doc(self, dirn: str):\n",
    "        try:\n",
    "            train_path = os.path.join('kaggle', dirn, 'train.csv')\n",
    "            train_df = pd.read_csv(train_path)\n",
    "\n",
    "            test_path = os.path.join('kaggle', dirn, 'test.csv')\n",
    "            test_df = pd.read_csv(test_path)\n",
    "\n",
    "            label = list(set(train_df.columns) - set(test_df.columns))[0]\n",
    "\n",
    "            self.datasets[dirn] = {\n",
    "                'train': train_df,\n",
    "                'test': test_df,\n",
    "                'label': label,\n",
    "                'name': dirn\n",
    "            }\n",
    "            return dirn\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    def load(self):\n",
    "        dir_list = []\n",
    "        for dirn in os.listdir('kaggle'):\n",
    "            if not os.path.isdir(os.path.join('kaggle', dirn)):\n",
    "                continue\n",
    "            dir_list.append(dirn)\n",
    "    \n",
    "        print(dir_list)\n",
    "        with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "            future_to_url = [executor.submit(self.load_doc, dirn) for dirn in dir_list]\n",
    "            for future in as_completed(future_to_url):\n",
    "                dirn = future.result()\n",
    "                print(f\"{dirn} complete.\")\n",
    "                \n",
    "                \n",
    "data_loader = DataLoader(chain)\n",
    "data_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f18ad817-5d64-4df1-9789-dc44c181ded2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LoadDataTool:\n",
    "    globals: Optional[Dict] = dict()\n",
    "    locals: Optional[Dict] = dict()\n",
    "\n",
    "    def __init__(self, status):\n",
    "        self.status = status\n",
    "        \n",
    "    @prompts(\n",
    "        name=\"Load\",\n",
    "        description=\"This is a tool to load dataset.\"\n",
    "        \"Input should be a dataset name.\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        \n",
    "        if self.status.dev:\n",
    "            name = \"WineQT\"\n",
    "            data = pd.read_csv(\"WineQT.csv\")\n",
    "            X = data.drop('quality', axis=1)\n",
    "            y = data['quality']\n",
    "            self.status.init_data(name, X, y, None, None)\n",
    "        else:\n",
    "            dataset = data_loader.select(query)\n",
    "            name = dataset['name']\n",
    "            label = dataset['label']\n",
    "            data = dataset['train']\n",
    "            X_train = data.drop(label, axis=1)\n",
    "            y_train = data[label]\n",
    "            data = dataset['test']\n",
    "            if label in data:\n",
    "                data = data.drop(label, axis=1)\n",
    "            X_test = data\n",
    "            y_test = pd.DataFrame(data={label: [0]*len(X_test)})[label]\n",
    "            self.status.init_data(name, X_train, X_test, y_train, y_test)\n",
    "            print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "        \n",
    "        return f\"Load dataset {name} success !\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "579f8fa8-72a1-48a5-bb92-f1df1c911907",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ExploreDataTool:\n",
    "    globals: Optional[Dict] = dict()\n",
    "    locals: Optional[Dict] = dict()\n",
    "\n",
    "    def __init__(self, status):\n",
    "        self.status = status\n",
    "        self.X = status.X\n",
    "        self.y = status.y\n",
    "        self.locals = {\"X\": self.X, 'y': self.y, 'np': np}\n",
    "        self.globals = {\"np\": np, 'pd': pd}\n",
    "        \n",
    "    @prompts(\n",
    "        name=\"Explore\",\n",
    "        description=\"This is a tool for dataset exploring.\"\n",
    "        \"Useful when the user ask for you to explore the dataset.\"\n",
    "        \"Output is the thought from exploring the dataset.\"\n",
    "        \"Input should be an empty string.\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        if not self.status.dataset:\n",
    "            return \"Pick a dataset first.\"\n",
    "        print(self.status.dataset)\n",
    "        \n",
    "        start_explore_prompt = f\"\"\"\n",
    "You are a data scientist. You are now performing data analysis before training a machine learning model.\n",
    "Your task is to tell me how the data could be preprocessed and to give me the corresponding Python code.\n",
    "The preprocessing includes, but is not limited to, missing value handling, scaling for numerical features, and categorical encoding.\n",
    "The task will be performed in two stages.\n",
    "In the first stage, you will decide which techniques are required to preprocess the dataset by investigating the dataset.\n",
    "In the second stage, you will preprocess the dataset by implementing the functions.\n",
    "\n",
    "Here is some information about the feature dataframe `X`:\n",
    "\\\"\\\"\\\"\n",
    "- First five rows:\n",
    "{self.X.head(5)}\n",
    "\n",
    "- Column types:\n",
    "{self.X.dtypes}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Here is some information about the label series `y`:\n",
    "\\\"\\\"\\\"\n",
    "- First five rows:\n",
    "{self.y.head(5)}\n",
    "\n",
    "- Column types:\n",
    "{self.y.dtypes}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "        how_to_explore_prompt = \"\"\"\n",
    "Now, let's move on to the first stage:\n",
    "1. Explain how you will explore the dataset in order to select appropriate preprocessing techniques.\n",
    "   Analyze numerical and categorical features separately.\n",
    "2. Write a Python function called `explore` for data exploration.\n",
    "   Import any required packages WITHIN the `explore` function. That is, The first line of code should be the definition of the function, followed by the necessary package imports.\n",
    "   The function will be called using `result = explore(X, y)`, and all conclusions should be stored in a string variable `result`.\n",
    "   \n",
    "   Make the code and the returned string concise. Avoid listing specific data information such as row indices or column names.\n",
    "   Here are some GOOD examples of concise conclusions (not limited) to add to the `result`:\n",
    "   - \"There are no missing values in the dataset.\"\n",
    "   - \"There are {num_cat} categorical features. Consider applying a one-hot encoding\"\n",
    "   - \"The numerical features are highly skewed (mean skewness = {mean_skew:.2f}). Consider applying a log transformation or a power transformation.\"\n",
    "   - \"There are no outliers in the numerical features.\"\n",
    "   - \"The categorical features have high cardinality (mean cardinality = {mean_card:.2f}).\"\n",
    "   Here are some BAD examples of conclusions (not limited) to avoid adding to result as they specify data information and lack conciseness:\n",
    "   - \"Numerical feature: {X[numerical_features].describe().to_string()}\"\n",
    "   - \"Categorical feature: {X[categorical_features].nunique().to_string()}\"\n",
    "   - \"There are {num_outliers_per_feature} outliers in the feature {col_name}.\" (inside a for loop)\n",
    "\n",
    "   These are the only packages in the Python environment:\n",
    "   ```\n",
    "   numpy==1.24.3\n",
    "   pandas==2.0.1\n",
    "   ```\n",
    "   No additional packages can be used.\n",
    "\"\"\"\n",
    "\n",
    "        prompt = \"\\n\\n\".join([start_explore_prompt, how_to_explore_prompt])\n",
    "        self.start_explore_prompt = start_explore_prompt\n",
    "        self.how_to_explore_prompt = how_to_explore_prompt\n",
    "        print(prompt)\n",
    "\n",
    "        explore_data_thought = coding_agent.run(prompt)\n",
    "        self.explore_data_thought = explore_data_thought\n",
    "        print(explore_data_thought)\n",
    "        \n",
    "        explore_code = extract_code(explore_data_thought)\n",
    "        explore_code += \"\\n\\n_ = explore(X, y)\"\n",
    "        self.locals['X'] = self.X\n",
    "        self.locals['y'] = self.y\n",
    "        self.explore_code = explore_code\n",
    "        print(explore_code + '\\n')\n",
    "        self.retry_execute(explore_data_thought, explore_code)\n",
    "        \n",
    "        explore_data_result = self.locals[\"explore\"](self.X, self.y)\n",
    "        self.explore_data_result = explore_data_result\n",
    "        print(explore_data_result)\n",
    "        \n",
    "        self.update_status()\n",
    "        return explore_data_result\n",
    "    \n",
    "    def update_status(self):\n",
    "        self.status.start_explore_prompt = self.start_explore_prompt\n",
    "        self.status.how_to_explore_prompt = self.how_to_explore_prompt\n",
    "        self.status.explore_code = self.explore_code\n",
    "        self.status.explore_data_result = self.explore_data_result\n",
    "        self.status.explore_data_thought = self.explore_data_thought\n",
    "        self.status.explore = True\n",
    "    \n",
    "    def retry_execute(self, prompt: str, code: str, retry: int = 3):\n",
    "        for _ in range(retry):\n",
    "            try:\n",
    "                exec(code, self.globals, self.locals)\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "                fix_code = coding_agent.run(f\"{prompt}\\n\\nExecute result:\\n{traceback.format_exc()}\\n\\nModify the code and fix the error.\")\n",
    "                code = extract_code(fix_code)\n",
    "                code += \"\\n\\n_ = explore(X, y)\"\n",
    "                print(f\"Fix{_}:\\n{code}\\n\")\n",
    "    \n",
    "\n",
    "class ProcessDataTool:\n",
    "    globals: Optional[Dict] = dict()\n",
    "    locals: Optional[Dict] = dict()\n",
    "\n",
    "    def __init__(self, status):\n",
    "        self.status = status\n",
    "        self.X = status.X\n",
    "        self.y = status.y\n",
    "        self.locals = {\"X\": self.X, 'y': self.y, 'np': np}\n",
    "        self.globals = {\n",
    "            \"np\": np,\n",
    "            'pd': pd,\n",
    "        }\n",
    "\n",
    "    @prompts(\n",
    "        name=\"Preprocess\",\n",
    "        description=\"This is a tool for preprocess the dataset.\"\n",
    "        \"Useful when you want to process the dataset.\"\n",
    "        \"Useful when you want to preprocess the dataset.\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        if not self.status.dataset:\n",
    "            return \"Pick a dataset first.\"\n",
    "        print()\n",
    "        \n",
    "        explore_data_result = self.status.explore_data_result\n",
    "        start_explore_prompt = self.status.start_explore_prompt\n",
    "        how_to_explore_prompt = self.status.how_to_explore_prompt\n",
    "        explore_data_thought = self.status.explore_data_thought\n",
    "\n",
    "        preprocess_prompt = \"\"\"\n",
    "The exploration results from the first stage are shown above.\n",
    "Now, let's move on to the second stage:\n",
    "1. Based on the conclusion, specify the preprocessing techniques to be employed for the dataset and briefly describe their implementations.\n",
    "2. Write a Python function called `preprocess` that will assist you in preprocessing the data.\n",
    "   Import any required packages WITHIN the `preprocess` function. That is, The first line of code should be the definition of the function, followed by the necessary package imports.\n",
    "   The function will be called using `proc_X_train, proc_y_train, proc_X_test, proc_y_test = preprocess(X_train, y_train, X_test, y_test)`, \n",
    "   where both the parameters and return values are eithor Pandas dataframes or series.\n",
    "   Only consider common preprocessing methods and be aware of potential data leakage issues.\n",
    "   Make the code concise. Avoid listing specific data information such as row indices or column names.\n",
    "   Make the function as simple as possible.\n",
    "   Make the function as short as possible.\n",
    "   \n",
    "\n",
    "   These are the only packages in the Python environment:\n",
    "   ```\n",
    "   numpy==1.24.3\n",
    "   pandas==2.0.1\n",
    "   scikit-learn==1.2.2\n",
    "   scipy==1.10.1\n",
    "   category_encoders==2.6.1\n",
    "   ```\n",
    "   No additional packages can be used.\n",
    "\"\"\"\n",
    "        \n",
    "\n",
    "        prompt = \"\\n\\n\".join([\n",
    "            start_explore_prompt,\n",
    "            #how_to_explore_prompt,\n",
    "            f\"First-stage Thoughts:\\n{explore_data_thought}\",\n",
    "            f\"First-stage Conclusion:\\n{explore_data_result}\",\n",
    "            preprocess_prompt]\n",
    "        )\n",
    "        print(prompt)\n",
    "        preprocess_data_thought = coding_agent.run(prompt)\n",
    "        self.preprocess_data_thought = preprocess_data_thought\n",
    "        print(preprocess_data_thought)\n",
    "        preprocess_code = extract_code(preprocess_data_thought)\n",
    "        self.preprocess_code = preprocess_code\n",
    "        \n",
    "        X_train, y_train = deepcopy(self.status.X_train), deepcopy(self.status.y_train)\n",
    "        X_test, y_test = deepcopy(self.status.X_test), deepcopy(self.status.y_test)\n",
    "        self.locals[\"X_train\"] = X_train\n",
    "        self.locals[\"y_train\"] = y_train\n",
    "        self.locals[\"X_test\"] = X_test\n",
    "        self.locals[\"y_test\"] = y_test\n",
    "        preprocess_code += \"\\n\\n_, _, _, _ = preprocess(X_train, y_train, X_test, y_test)\"\n",
    "        print(preprocess_code)\n",
    "        self.retry_execute(preprocess_data_thought, preprocess_code)\n",
    "        self.X_train, self.y_train, self.X_test, self.y_test = self.locals[\"preprocess\"](X_train, y_train, X_test, y_test)\n",
    "        #result = self.evaluate()\n",
    "        #print(result)\n",
    "        \n",
    "        \"\"\"best_result = {\n",
    "            'score': 100,\n",
    "            'code': preprocess_code\n",
    "        }\n",
    "        for _ in range(5):\n",
    "            try:\n",
    "                print(preprocess_code)\n",
    "                exec(preprocess_code, self.globals, self.locals)\n",
    "                result = self.evaluate()\n",
    "                \n",
    "                self.status.X_train = result['processed_data'][0]\n",
    "                self.status.X_test = result['processed_data'][2]\n",
    "                self.status.y_train = result['processed_data'][1]\n",
    "                self.status.y_test = result['processed_data'][3]\n",
    "                \n",
    "                if result['llm'] < best_result['score']:\n",
    "                    best_result['score'] = result['llm']\n",
    "                    best_result['code'] = preprocess_code\n",
    "                    \n",
    "                    self.status.X_train = result['processed_data'][0]\n",
    "                    self.status.X_test = result['processed_data'][2]\n",
    "                    self.status.y_train = result['processed_data'][1]\n",
    "                    self.status.y_test = result['processed_data'][3]\n",
    "                \n",
    "                fix_code = coding_agent.run(f\"{prompt}\\n\\nExecute result:\\n{result['msg']}\\n\\nYour result is LLM result. Try to improve the preprocess function.\")\n",
    "                preprocess_code = extract_code(fix_code)\n",
    "                print(f\"Revise{_}:\\n{preprocess_code}\\n\")\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "                trace = traceback.format_exc()\n",
    "                print(trace)\n",
    "                fix_code = coding_agent.run(f\"{prompt}\\n\\nExecute result:\\n{str(trace)[:512]}\\n\\nModify the code and fix the error. Try not to use the module that causes error. Simplify the preprocess function as much as posiible.\")\n",
    "                preprocess_code = extract_code(fix_code)\n",
    "                # print(f\"Fix{_}:\\n{preprocess_code}\\n\")\n",
    "                \n",
    "        print(best_result)\"\"\"\n",
    "        self.update_status()\n",
    "        return f\"{preprocess_data_thought}\\n\\nProcessing....\\n\\nProcess Completed !\"\n",
    "    \n",
    "    def update_status(self):\n",
    "        self.status.preprocess_data_thought = self.preprocess_data_thought\n",
    "        self.status.preprocess_code = self.preprocess_code\n",
    "        self.status.X_train = self.X_train\n",
    "        self.status.X_test = self.X_test\n",
    "        self.status.y_train = self.y_train\n",
    "        self.status.y_test = self.y_test\n",
    "        #self.status.X_train = self.locals['X_train']\n",
    "        #self.status.X_test = self.locals['X_test']\n",
    "        #self.status.y_train = self.locals['y_train']\n",
    "        #self.status.y_test = self.locals['y_test']\n",
    "        self.status.preprocess = True\n",
    "    \n",
    "    def retry_execute(self, prompt: str, code: str, retry: int = 3):\n",
    "        for _ in range(retry):\n",
    "            try:\n",
    "                exec(code, self.globals, self.locals)\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "                fix_code = coding_agent.run(f\"{prompt}\\n\\nExecute result:\\n{traceback.format_exc()}\\n\\nModify the code and fix the error.\")\n",
    "                code = extract_code(fix_code)\n",
    "                code += \"\\n\\n_, _, _, _ = preprocess(X_train, y_train, X_test, y_test)\"\n",
    "                print(f\"Fix{_}:\\n{code}\\n\")\n",
    "     \n",
    "    def evaluate(self):\n",
    "        \n",
    "        print('start evaluate')\n",
    "\n",
    "        def common_preprocess(X_train, y_train, X_test, y_test):\n",
    "            # TODO: Missing value dropping\n",
    "\n",
    "            # TODO: Categorical feature encoding\n",
    "\n",
    "            # Scale numerical variables using StandardScaler\n",
    "            scaler = StandardScaler()\n",
    "            X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "            X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "            return X_train, y_train, X_test, y_test\n",
    "        \n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        from lightgbm import LGBMRegressor, early_stopping\n",
    "\n",
    "        seed = 42\n",
    "\n",
    "        def evaluate(X_train, y_train, X_test, y_test, prep_func=None):\n",
    "            if prep_func:\n",
    "                try:\n",
    "                    X_train, y_train, X_test, y_test = prep_func(X_train, y_train, X_test, y_test)\n",
    "                except Exception as e:\n",
    "                    import traceback\n",
    "                    trace = traceback.format_exc()\n",
    "                    print(trace)\n",
    "                    raise Exception(trace)\n",
    "            processed_data = (X_train, y_train, X_test, y_test)\n",
    "\n",
    "            print(X_train.shape. X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=seed)\n",
    "            clf = LGBMRegressor(random_state=seed, **self.status.best_param)\n",
    "            clf.fit(X_train, y_train, eval_set=(X_valid, y_valid), callbacks=[early_stopping(stopping_rounds=10)])\n",
    "            y_pred = clf.predict(X_test)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            return mse, processed_data\n",
    "\n",
    "        from copy import deepcopy\n",
    "        X_train = deepcopy(self.status.X_train)\n",
    "        X_test = deepcopy(self.status.X_test)\n",
    "        y_train = deepcopy(self.status.y_train)\n",
    "        y_test = deepcopy(self.status.y_test)\n",
    "        self.locals[\"X_train\"] = X_train\n",
    "        self.locals[\"X_test\"] = X_test\n",
    "        self.locals[\"y_train\"] = y_train\n",
    "        self.locals[\"y_test\"] = y_test\n",
    "        \n",
    "        # LLM\n",
    "        mse_llm, processed_data = evaluate(X_train, y_train, X_test, y_test, self.locals['preprocess'])\n",
    "\n",
    "        # Common\n",
    "        # mse_com, _ = evaluate(X_train, y_train, X_test, y_test, common_preprocess)\n",
    "\n",
    "        # Raw\n",
    "        mse_raw, _ = evaluate(X_train, y_train, X_test, y_test)\n",
    "        mse_com = mse_raw\n",
    "\n",
    "        msg = f\"No preprocess: {mse_raw}, Common preprocess: {mse_com}, LLM preprocess: {mse_llm}\"\n",
    "        print(msg)\n",
    "        return {\n",
    "            'msg': msg,\n",
    "            'no': mse_raw,\n",
    "            'common': mse_com,\n",
    "            'llm': mse_llm,\n",
    "            'processed_data': processed_data\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08a52ddc-f23b-43d0-847e-34f1b767fbd6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2991522/3325005673.py\", line 82, in inference\n",
      "    examine_dataset_result = self.locals[\"find_suitable_model\"](self.X_train, self.y_train)\n",
      "  File \"<string>\", line 18, in find_suitable_model\n",
      "  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 515, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 285, in cross_validate\n",
      "    _warn_or_raise_about_fit_failures(results, error_score)\n",
      "  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 367, in _warn_or_raise_about_fit_failures\n",
      "    raise ValueError(all_fits_failed_message)\n",
      "ValueError: \n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 1342, in fit\n",
      "    super().fit(\n",
      "  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 172, in fit\n",
      "    X, y = self._validate_data(\n",
      "  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/sklearn/base.py\", line 591, in _validate_data\n",
      "    X = check_array(X, input_name=\"X\", **check_X_params)\n",
      "  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 856, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/pandas/core/generic.py\", line 2064, in __array__\n",
      "    return np.asarray(self._values, dtype=dtype)\n",
      "ValueError: could not convert string to float: 'RL'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def find_suitable_model(X, y):\n",
      "    \"\"\"\n",
      "    This function takes in a dataset X and target variable y and returns the most suitable model for the dataset.\n",
      "    \"\"\"\n",
      "    # Import necessary packages\n",
      "    from sklearn.tree import DecisionTreeRegressor\n",
      "    from sklearn.ensemble import RandomForestRegressor\n",
      "    from sklearn.neural_network import MLPRegressor\n",
      "    from sklearn.model_selection import cross_val_score\n",
      "    \n",
      "    # Initialize models\n",
      "    dt = DecisionTreeRegressor()\n",
      "    rf = RandomForestRegressor()\n",
      "    nn = MLPRegressor()\n",
      "    \n",
      "    # Evaluate models using cross-validation\n",
      "    dt_scores = cross_val_score(dt, X, y, cv=5, scoring='neg_mean_squared_error')\n",
      "    rf_scores = cross_val_score(rf, X, y, cv=5, scoring='neg_mean_squared_error')\n",
      "    nn_scores = cross_val_score(nn, X, y, cv=5, scoring='neg_mean_squared_error')\n",
      "    \n",
      "    # Calculate mean squared error for each model\n",
      "    dt_mse = -1 * dt_scores.mean()\n",
      "    rf_mse = -1 * rf_scores.mean()\n",
      "    nn_mse = -1 * nn_scores.mean()\n",
      "    \n",
      "    # Determine the best model based on mean squared error\n",
      "    if dt_mse < rf_mse and dt_mse < nn_mse:\n",
      "        result = \"The most suitable model for this dataset is Decision Tree Regressor with mean squared error of {}\".format(dt_mse)\n",
      "    elif rf_mse < dt_mse and rf_mse < nn_mse:\n",
      "        result = \"The most suitable model for this dataset is Random Forest Regressor with mean squared error of {}\".format(rf_mse)\n",
      "    else:\n",
      "        result = \"The most suitable model for this dataset is Neural Network Regressor with mean squared error of {}\".format(nn_mse)\n",
      "    \n",
      "    return result\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ModelSelectTool:\n",
    "    globals: Optional[Dict] = dict()\n",
    "    locals: Optional[Dict] = dict()\n",
    "\n",
    "    def __init__(self, status):\n",
    "        self.status = status\n",
    "        self.X_train = status.X_train\n",
    "        self.y_train = status.y_train\n",
    "        self.locals = {\"X_train\": self.X_train, 'y_train': self.y_train}\n",
    "        self.globals = {\"np\": np, 'pd': pd}\n",
    "        self.model_list = []\n",
    "        \n",
    "        from sklearn import ensemble, linear_model\n",
    "        import lightgbm\n",
    "        for module in [ensemble, linear_model, lightgbm]:\n",
    "            for name, func in vars(module).items():\n",
    "                if name[0].isupper():\n",
    "                    self.globals[name] = func\n",
    "                    self.model_list.append(name)\n",
    "        \n",
    "    @prompts(\n",
    "        name=\"Pick Model\",\n",
    "        description=\"This is a tool for deciding which model to use for a machine learning task.\"\n",
    "        \"Output is description about how to pick a model.\"\n",
    "        \"Input should be an empty string.\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        \n",
    "        if not self.status.dataset:\n",
    "            return \"Pick a dataset first.\"\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            know_how_prompt = coding_agent.run(\"How to decide which machine learning model to use ?\")\n",
    "            self.know_how_prompt = know_how_prompt\n",
    "\n",
    "            how_examine_dataset_thought = coding_agent.run(know_how_prompt + f\"\"\"\n",
    "            Here is some information about the feature dataframe `X`:\n",
    "            \\\"\\\"\\\"\n",
    "            - First five rows:\n",
    "            {self.X_train.head(5)}\n",
    "\n",
    "            - Column types:\n",
    "            {self.X_train.dtypes}\n",
    "            \\\"\\\"\\\"\n",
    "\n",
    "            Here is some information about the label series `y`:\n",
    "            \\\"\\\"\\\"\n",
    "            - First five rows:\n",
    "            {self.y_train.head(5)}\n",
    "\n",
    "            - Column types:\n",
    "            {self.y_train.dtypes}\n",
    "            \\\"\\\"\\\"\n",
    "\n",
    "            Tell me how to find the most suitable model for this dataset.\n",
    "            \"\"\")\n",
    "            self.how_examine_dataset_thought = how_examine_dataset_thought\n",
    "            # print(how_examine_dataset_thought)\n",
    "\n",
    "            examine_dataset_prompt = f\"\"\"Thoughts:\\n{how_examine_dataset_thought}\\n\n",
    "            Write a Python function called `find_suitable_model`.\n",
    "            Import any required packages WITHIN the `preprocess` function. That is, The first line of code should be the definition of the function, followed by the necessary package imports.\n",
    "            Do not include visualization and plotting.\n",
    "            The function will be called using `result = find_suitable_model(X, y)`, and all findings must be stored in a string variable `result`.\n",
    "            Write the function as simple and short as possible.\n",
    "            show the function in markdown.\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "            examine_dataset_thought = coding_agent.run(examine_dataset_prompt)\n",
    "            if '`' not in examine_dataset_thought:\n",
    "                examine_dataset_thought = f\"```\\n{examine_dataset_thought}\\n```\"\n",
    "            print(examine_dataset_thought)\n",
    "\n",
    "            examine_dataset_code = extract_code(examine_dataset_thought)\n",
    "            self.examine_dataset_thought = examine_dataset_thought\n",
    "            self.examine_dataset_code = examine_dataset_code\n",
    "            # print(examine_dataset_code)\n",
    "\n",
    "            exec(examine_dataset_code, self.globals, self.locals)\n",
    "            examine_dataset_result = self.locals[\"find_suitable_model\"](self.X_train, self.y_train)\n",
    "            self.examine_dataset_result = examine_dataset_result\n",
    "\n",
    "            final_prompt = f\"\"\"\n",
    "            Human:\\n{examine_dataset_prompt}\n",
    "            AI:\\n{examine_dataset_thought}\n",
    "            Human:\\nExecute result: {examine_dataset_result}\n",
    "\n",
    "            Write a Python function called `construct_model` to create the model instance.\n",
    "            Initiate the model with no parameters.\n",
    "            Import any package you need beforehand.\n",
    "            Do not include visualization and plotting.\n",
    "            The function will be called using `clf = construct_model()`, and the constructed model will be stored in a string variable `clf`.\n",
    "            Show results in markdown.\n",
    "            \"\"\"\n",
    "            model_selection_thought = coding_agent.run(final_prompt)\n",
    "            self.model_selection_thought = model_selection_thought\n",
    "            # print(model_selection_thought)\n",
    "            model_selection_code = extract_code(model_selection_thought)\n",
    "            self.model_selection_code = model_selection_code\n",
    "            exec(model_selection_code, self.globals, self.locals)\n",
    "            self.clf = self.locals['construct_model']()\n",
    "\n",
    "            self.update_status()\n",
    "            return model_selection_thought\n",
    "        except:\n",
    "            logging.exception(\"\")\n",
    "            return \"\"\n",
    "    \n",
    "    def update_status(self):\n",
    "        self.status.know_how_prompt = self.know_how_prompt\n",
    "        self.status.how_examine_dataset_thought = self.how_examine_dataset_thought\n",
    "        self.status.examine_dataset_thought = self.examine_dataset_thought\n",
    "        self.status.examine_dataset_result = self.examine_dataset_result\n",
    "        self.status.examine_dataset_code = self.examine_dataset_code\n",
    "        self.status.model_selection_thought = self.model_selection_thought\n",
    "        self.status.model_selection_code = self.model_selection_code\n",
    "        self.status.model = self.clf.__class__.__name__\n",
    "        self.status.model_selection = True\n",
    "\n",
    "        \n",
    "ModelSelectTool(status).inference(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e2b86cc-d01e-4e13-95a9-a2d7dd85c59c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import numpy as np\n",
      "\n",
      "def regression_or_classification(X, y):\n",
      "    \"\"\"\n",
      "    This function checks the type of the output variable in the label series y and returns whether to use a regression or classifier model.\n",
      "    \n",
      "    Parameters:\n",
      "    X (numpy.ndarray): The input features.\n",
      "    y (numpy.ndarray): The output variable.\n",
      "    \n",
      "    Returns:\n",
      "    str: A string indicating whether to use a regression or classifier model.\n",
      "    \"\"\"\n",
      "    if isinstance(y[0], np.integer):\n",
      "        return \"Use a regression model.\"\n",
      "    else:\n",
      "        return \"Use a classifier model.\"\n",
      "```\n",
      "\n",
      "To use the function, simply call it with the input features `X` and the label series `y`:\n",
      "\n",
      "```python\n",
      "result = regression_or_classification(X, y)\n",
      "print(result)\n",
      "```\n",
      "\n",
      "If the output variable is continuous, the function will return \"Use a regression model.\" If the output variable is categorical, the function will return \"Use a classifier model.\"\n",
      "AI:\n",
      "\n",
      "```python\n",
      "def construct_model():\n",
      "    \"\"\"\n",
      "    This function constructs a LightGBM regression model.\n",
      "    \n",
      "    Returns:\n",
      "    lightgbm.LGBMRegressor: A LightGBM regression model instance.\n",
      "    \"\"\"\n",
      "    import lightgbm as lgb\n",
      "    \n",
      "    clf = lgb.LGBMRegressor()\n",
      "    \n",
      "    return clf\n",
      "```\n",
      "\n",
      "To use the function, simply call it to construct the model instance:\n",
      "\n",
      "```python\n",
      "clf = construct_model()\n",
      "```\n",
      "\n",
      "This will create a LightGBM regression model instance with default parameters.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'AI:\\n\\n```python\\ndef construct_model():\\n    \"\"\"\\n    This function constructs a LightGBM regression model.\\n    \\n    Returns:\\n    lightgbm.LGBMRegressor: A LightGBM regression model instance.\\n    \"\"\"\\n    import lightgbm as lgb\\n    \\n    clf = lgb.LGBMRegressor()\\n    \\n    return clf\\n```\\n\\nTo use the function, simply call it to construct the model instance:\\n\\n```python\\nclf = construct_model()\\n```\\n\\nThis will create a LightGBM regression model instance with default parameters.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ModelSelectTool:\n",
    "    globals: Optional[Dict] = dict()\n",
    "    locals: Optional[Dict] = dict()\n",
    "\n",
    "    def __init__(self, status):\n",
    "        self.status = status\n",
    "        self.X_train = status.X_train\n",
    "        self.y_train = status.y_train\n",
    "        self.locals = {\"X_train\": self.X_train, 'y_train': self.y_train}\n",
    "        self.globals = {\"np\": np, 'pd': pd}\n",
    "        self.model_list = []\n",
    "        \n",
    "        from sklearn import ensemble, linear_model\n",
    "        import lightgbm\n",
    "        for module in [ensemble, linear_model, lightgbm]:\n",
    "            for name, func in vars(module).items():\n",
    "                if name[0].isupper():\n",
    "                    self.globals[name] = func\n",
    "                    self.model_list.append(name)\n",
    "        \n",
    "    @prompts(\n",
    "        name=\"Pick Model\",\n",
    "        description=\"This is a tool for deciding which model to use for a machine learning task.\"\n",
    "        \"Useful when the user ask for you to pick a model.\"\n",
    "        \"Output is the thought about how you deicde to pick a model.\"\n",
    "        \"Input should be an empty string.\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        \n",
    "        if not self.status.dataset:\n",
    "            return \"Pick a dataset first.\"\n",
    "        \n",
    "        know_how_prompt = coding_agent.run(\"How to decide which machine learning model to use ? regression or classifier ?\")\n",
    "        self.know_how_prompt = know_how_prompt\n",
    "        \n",
    "        how_examine_dataset_thought = coding_agent.run(know_how_prompt + f\"\"\"\n",
    "        Here is some information about the feature dataframe `X`:\n",
    "        \\\"\\\"\\\"\n",
    "        - First five rows:\n",
    "        {self.X_train[:5]}\n",
    "\n",
    "        - Column types:\n",
    "        {self.X_train.dtypes}\n",
    "        \\\"\\\"\\\"\n",
    "\n",
    "        Here is some information about the label series `y`:\n",
    "        \\\"\\\"\\\"\n",
    "        - First five rows:\n",
    "        {self.y_train[:5]}\n",
    "\n",
    "        - Column types:\n",
    "        {self.y_train.dtypes}\n",
    "        \\\"\\\"\\\"\n",
    "\n",
    "        Tell me how to check the dataset that can help you decide regression or classifier to use.\n",
    "        \"\"\")\n",
    "        self.how_examine_dataset_thought = how_examine_dataset_thought\n",
    "        # print(how_examine_dataset_thought)\n",
    "        \n",
    "        examine_dataset_prompt = f\"\"\"Thoughts:\\n{how_examine_dataset_thought}\\n\n",
    "        Write a Python function called `regression_or_classification` to check the dataset and help you decide regression or classifier to use.\n",
    "        Import any package you need beforehand.\n",
    "        Do not include visualization and plotting.\n",
    "        Make it as simple as possible.\n",
    "        The function will be called using `result = regression_or_classification(X, y)`, and all findings must be stored in a string variable `result`.\n",
    "        Show results in markdown.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        examine_dataset_thought = coding_agent.run(examine_dataset_prompt)\n",
    "        print(examine_dataset_thought)\n",
    "\n",
    "        examine_dataset_code = extract_code(examine_dataset_thought)\n",
    "        self.examine_dataset_thought = examine_dataset_thought\n",
    "        self.examine_dataset_code = examine_dataset_code\n",
    "        # print(examine_dataset_code)\n",
    "\n",
    "        exec(examine_dataset_code, self.globals, self.locals)\n",
    "        examine_dataset_result = self.locals[\"regression_or_classification\"](self.X_train, self.y_train)\n",
    "        self.examine_dataset_result = examine_dataset_result\n",
    "        \n",
    "        final_prompt = f\"\"\"\n",
    "        Human:\\n{examine_dataset_prompt}\n",
    "        AI:\\n{examine_dataset_thought}\n",
    "        Human:\\nExecute result: {examine_dataset_result}\n",
    "\n",
    "        Which model will you use ?\n",
    "        lightgbm regressor or classifier ?\n",
    "        \n",
    "        Write a Python function called `construct_model` to create the model instance.\n",
    "        Initiate the model with no parameters.\n",
    "        Do not include visualization and plotting.\n",
    "        The function will be called using `clf = construct_model()`, and the constructed model will be stored in a string variable `clf`.\n",
    "        Import any required packages WITHIN the `construct_model` function. That is, The first line of code should be the definition of the function, followed by the necessary package imports.\n",
    "        Do not imoprt any package before the function.\n",
    "        Show results in markdown.\n",
    "        \"\"\"\n",
    "        model_selection_thought = coding_agent.run(final_prompt)\n",
    "        self.model_selection_thought = model_selection_thought\n",
    "        print(model_selection_thought)\n",
    "        model_selection_code = extract_code(model_selection_thought)\n",
    "        self.model_selection_code = model_selection_code\n",
    "        exec(model_selection_code, self.globals, self.locals)\n",
    "        self.clf = self.locals['construct_model']()\n",
    "        \n",
    "        self.update_status()\n",
    "        return model_selection_thought\n",
    "    \n",
    "    def update_status(self):\n",
    "        self.status.know_how_prompt = self.know_how_prompt\n",
    "        self.status.how_examine_dataset_thought = self.how_examine_dataset_thought\n",
    "        self.status.examine_dataset_thought = self.examine_dataset_thought\n",
    "        self.status.examine_dataset_code = self.examine_dataset_code\n",
    "        self.status.model_selection_thought = self.model_selection_thought\n",
    "        self.status.model_selection_code = self.model_selection_code\n",
    "        self.status.model = self.clf.__class__.__name__\n",
    "        self.status.model_selection = True\n",
    "\n",
    "        \n",
    "ModelSelectTool(status).inference(\"\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9df1b8a-5641-4b18-a91a-12e5adec780a",
   "metadata": {
    "tags": []
   },
   "source": [
    "def gogogo():\n",
    "    print(\"====== gogogo ==========\")\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from lightgbm import LGBMRegressor, early_stopping\n",
    "    X_train = status.X_train\n",
    "    y_train = status.y_train\n",
    "    X_test = status.X_test\n",
    "    y_test = status.y_test\n",
    "    print(status.best_param)\n",
    "    seed = 42\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=seed)\n",
    "    clf = LGBMRegressor(random_state=seed, **status.best_param)\n",
    "    clf.fit(X_train, y_train, eval_set=(X_valid, y_valid), callbacks=[early_stopping(stopping_rounds=10)])\n",
    "    y_pred = clf.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(mse)\n",
    "    print(\"==============\")\n",
    "    \n",
    "gogogo()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d1d0b2d-534c-45fe-948f-1b3d5d0b8339",
   "metadata": {},
   "source": [
    "coding_agent.run.cache_clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abb5492-1b89-40cf-8fbb-ce80a398d801",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pdb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class E2EHyperOptTool:\n",
    "    globals: Optional[Dict] = dict()\n",
    "    locals: Optional[Dict] = dict()\n",
    "\n",
    "    def __init__(self, status):\n",
    "        self.status = status\n",
    "        self.X_train = status.X_train\n",
    "        self.y_train = status.y_train\n",
    "        self.X_test = status.X_test\n",
    "        self.y_test = status.y_test\n",
    "        # self.locals = {\"X\": X, 'Y': Y,}\n",
    "\n",
    "    @prompts(\n",
    "        name=\"hyperopt\",\n",
    "        description=\"This is a tool for hyper parameter tuning.\"\n",
    "        \"Input should be an empty string.\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        if not self.status.dataset:\n",
    "            return \"Pick a dataset first.\"\n",
    "        if not self.status.model:\n",
    "            msg = \"You have not select the model. Use LGBMRegressor for this task.\"\n",
    "            self.status.model = \"LGBMRegressor\"\n",
    "        \n",
    "        clf = self.construct_estimator(self.status.model)\n",
    "        param_grid = self.construct_param_grid(clf)\n",
    "        search_algo = self.get_search_algo(clf, param_grid)\n",
    "        clff = self.construct_search_algo(search_algo)\n",
    "        self.clff = clff\n",
    "        \n",
    "        # X_train = self.X_train.select_dtypes(exclude=['object'])\n",
    "        search = clff.fit(self.X_train, self.y_train)\n",
    "        self.search = search\n",
    "        \n",
    "        print(f\"Best search score: {search.best_score_}\")\n",
    "        score = self.evaluate(clf, search)\n",
    "\n",
    "        self.update_status()\n",
    "        return f\"{self.reason}\\n\\nFinal score after hyper parameter tuning is {score}\"\n",
    "    \n",
    "    def update_status(self):\n",
    "        self.status.hyperopt = True\n",
    "        self.status.hyper_result = self.search\n",
    "        self.status.hyperopt_param_thought = self.reason\n",
    "        self.status.y_pred = self.y_pred\n",
    "        \n",
    "    def evaluate(self, clf, search):\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        \n",
    "        # X_train = self.X_train.select_dtypes(exclude=['object'])\n",
    "        # X_test = self.X_test.select_dtypes(exclude=['object'])\n",
    "        X_train = self.X_train\n",
    "        X_test = self.X_test\n",
    "        \n",
    "        clff = clf.__class__(**search.best_params_)\n",
    "        clff.fit(X_train, self.y_train)\n",
    "        y_pred = clff.predict(X_test)\n",
    "        self.y_pred = y_pred\n",
    "        score = mean_squared_error(self.y_test, y_pred)\n",
    "        print(f\"Testing score: {score}\")\n",
    "        return score\n",
    "    \n",
    "    def construct_search_algo(self, search_algo):\n",
    "        # construct search instance\n",
    "        construct_search_prompt = f\"\"\"\n",
    "        Give me two lines of python code.\n",
    "        Frist, from lib import {search_algo.strip()}.\n",
    "        Second, instatiate an instance with {search_algo.strip()}(clf, param_grid, random_state=0, n_iter=50, n_jobs=24) and assign to hyper_algo variable.\n",
    "        \"\"\"\n",
    "        codes = coding_agent.run(construct_search_prompt)\n",
    "        print(f\"\\nConstructing search algo:\\n```\\n{codes}\\n```\\n\")\n",
    "        res = self.execute(codes)\n",
    "        if 'hyper_algo' not in self.locals:\n",
    "            raise Exception(res)\n",
    "        return self.locals['hyper_algo']\n",
    "\n",
    "    def get_search_algo(self, clf, param_grid):\n",
    "        search_algo_reason_prompt = f\"\"\"\n",
    "        You are now performing a hyper parameter search on a machine learning model.\n",
    "        Your goal is to decide the search algorithm base on model type and parameter space.\n",
    "\n",
    "\n",
    "        The given model is a {clf.__class__.__name__}.\n",
    "        The paramters grid is {param_grid}.\n",
    "        The avaiable search algorithm is as follow:\n",
    "\n",
    "        1. GridSearchCV: Exhaustive search over specified parameter values for an estimator.\n",
    "        2. HalvingGridSearchCV: Search over specified parameter values with successive halving.\n",
    "        3. ParameterGrid: Grid of parameters with a discrete number of values for each.\n",
    "        4. ParameterSampler: Generator on parameters sampled from given distributions.\n",
    "        5. RandomizedSearchCV: Randomized search on hyper parameters.\n",
    "        6. HalvingRandomSearchCV: Randomized search on hyper parameters with successive halving.\n",
    "\n",
    "        Think about which algorithm to use base on the model, parameter space and algorithm description.\n",
    "        Tell me which algorithm you have choosen and why.\n",
    "        List all the other algorithms you have choosen not to use and why.\n",
    "        \"\"\"\n",
    "        \n",
    "        reason = coding_agent.run(search_algo_reason_prompt)\n",
    "        self.reason = reason\n",
    "        print(reason)\n",
    "\n",
    "        search_algo = coding_agent.run(f\"{reason}\\n\\nAccording to above, give me the algorithm name of the choosen algorithm in a single token.\")\n",
    "        print(f\"\\nDeciding search algo:\\n{search_algo}\\n\")\n",
    "        return search_algo\n",
    "\n",
    "    def construct_param_grid(self, clf):\n",
    "        param_grid_prompt = f\"\"\"\n",
    "        You are now performing a hyper parameter search on a machine learning model.\n",
    "        Your goal is to construct the paramter space to search.\n",
    "\n",
    "        For example:\n",
    "        The model is a LinearSVR.\n",
    "        The parameters are {LinearSVR().get_params()}\n",
    "        The parameter grid can be defined as:\n",
    "        ```\n",
    "        param_grid = {{'C': [1, 10, 100, 1000], 'tol': [0.001, 0.0001]}}\n",
    "        ```\n",
    "        Remeber to define the space only on important features. \n",
    "\n",
    "\n",
    "        Now.\n",
    "        The given model is a {clf.__class__.__name__}.\n",
    "        The paramters are {clf.get_params()}.\n",
    "        The detail description about the parameters is as follow:\n",
    "        {clf.__doc__}\n",
    "\n",
    "        Construct the parameter grid you will use for hyper parameter in json format:\n",
    "        \"\"\"\n",
    "\n",
    "        param_grid = coding_agent.run(param_grid_prompt)\n",
    "        param_grid = json.loads(param_grid)\n",
    "        print(f\"\\nConstruct params grid:\\n{param_grid}\\n\")\n",
    "        self.locals['param_grid'] = param_grid\n",
    "        return param_grid\n",
    "        \n",
    "    def construct_estimator(self, estimator_name: str):\n",
    "        # construct clf instance\n",
    "        construct_estimator_prompt = f\"Give me two lines of python code. Construct a {estimator_name.strip()} estimator and assign the estimator clf variable.\"\n",
    "        codes = coding_agent.run(construct_estimator_prompt)\n",
    "        print(f\"\\nConstruct estimator with following:\\n```\\n{codes}\\n```\\n\")\n",
    "        res = self.execute(codes)\n",
    "        if 'clf' not in self.locals:\n",
    "            raise Exception(res)\n",
    "        return self.locals['clf']\n",
    "        \n",
    "    def execute(self, cmd: str):\n",
    "\n",
    "    \n",
    "        try:\n",
    "            tree = ast.parse(cmd)\n",
    "            module = ast.Module(tree.body[:-1], type_ignores=[])\n",
    "            exec(astunparse.unparse(module), self.globals, self.locals)  # type: ignore\n",
    "            module_end = ast.Module(tree.body[-1:], type_ignores=[])\n",
    "            module_end_str = astunparse.unparse(module_end)  # type: ignore\n",
    "            try:\n",
    "                return eval(module_end_str, self.globals, self.locals)\n",
    "            except Exception:\n",
    "                exec(module_end_str, self.globals, self.locals)\n",
    "                return \"\"\n",
    "        except Exception as e:\n",
    "            return str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e72df5c3-a5f6-4c71-b052-1db08579cbd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConversationBot(ABC):\n",
    "\n",
    "    def __init__(self):\n",
    "        seed_everything(0)\n",
    "        # self.handler = LoggerCallbackHandler()\n",
    "        # self.async_handler = AsyncLoggerCallbackHandler()\n",
    "        self.llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\", output_key=\"output\", return_messages=True\n",
    "        )\n",
    "        self.state = []\n",
    "        self.tools = self.load_tools()\n",
    "\n",
    "        # retry when llm output parsing error\n",
    "        retrytool = retryActionTool()\n",
    "        func = retrytool.inference\n",
    "        self.tools.append(\n",
    "            Tool(\n",
    "                name=func.name,\n",
    "                description=func.description,\n",
    "                # coroutine=to_async(func),\n",
    "                func=func,\n",
    "            )\n",
    "        )\n",
    "        self.output_parser = RobustRetryConvoOutputParser()\n",
    "        self.agent = self.init_agent()\n",
    "\n",
    "    def run_text(self, text: str):\n",
    "        logger.info(f\"User: {text}\")\n",
    "        res = self.agent({\"input\": text})\n",
    "        response = res[\"output\"]\n",
    "        self.state += [(text, response)]\n",
    "        logger.info(\n",
    "            f\"\\nProcessed run_text, Input text: {text}\\nCurrent state: {self.state}\\n\"\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    async def arun_text(self, text: str):\n",
    "        logger.info(f\"User: {text}\")\n",
    "        res = await self.agent.acall({\"input\": text})\n",
    "        response = res[\"output\"]\n",
    "        self.state += [(text, response)]\n",
    "        logger.info(\n",
    "            f\"\\nProcessed run_text, Input text: {text}\\nCurrent state: {self.state}\\n\"\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _clear(self):\n",
    "        self.memory.clear\n",
    "\n",
    "    def init_agent(self):\n",
    "        input_variables = [\"input\", \"agent_scratchpad\", \"chat_history\"]\n",
    "        return initialize_agent(\n",
    "            self.tools,\n",
    "            self.llm,\n",
    "            agent=\"chat-conversational-react-description\",\n",
    "            verbose=True,\n",
    "            memory=self.memory,\n",
    "            return_intermediate_steps=True,\n",
    "            max_iterations=5,\n",
    "            # max_execution_time=60,\n",
    "            agent_kwargs={\n",
    "                \"prefix\": AUTOML_CHATGPT_PREFIX,\n",
    "                \"format_instructions\": AUTOML_CHATGPT_FORMAT_INSTRUCTIONS,\n",
    "                \"suffix\": AUTOML_CHATGPT_SUFFIX,\n",
    "                \"input_variables\": input_variables,\n",
    "                \"output_parser\": self.output_parser,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def load_tools(self):\n",
    "        tools = []\n",
    "        X, X_train, X_test, y, y_train, y_test = self.load_artifacts()\n",
    "\n",
    "        for tool in [\n",
    "            # pythonShellTool(),\n",
    "            E2EHyperOptTool(status),\n",
    "            # HyperOptTool(),\n",
    "            # ConstructSearchTool(),\n",
    "            # ConstructParamGridTool(),\n",
    "            # ConstructEstimatorTool(),\n",
    "            # FinalTool(X, Y),\n",
    "            ExploreDataTool(status),\n",
    "            ProcessDataTool(status),\n",
    "            LoadDataTool(status),\n",
    "            ModelSelectTool(status),\n",
    "        ]:\n",
    "            func = tool.inference\n",
    "            tools.append(\n",
    "                Tool(\n",
    "                    name=func.name,\n",
    "                    description=func.description,\n",
    "                    # coroutine=to_async(func),\n",
    "                    func=func,\n",
    "                )\n",
    "            )\n",
    "        logger.info(f\"tools: {[i.name for i in tools]}\")\n",
    "        return tools\n",
    "\n",
    "    def load_artifacts(self):\n",
    "        data = pd.read_csv(\"WineQT.csv\")\n",
    "        X = data.drop('quality', axis=1)\n",
    "        y = data['quality']\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        # X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "        return X, X_train, X_test, y, y_train, y_test"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0155fe39-8191-441c-9f4a-6eeaa45c3ed8",
   "metadata": {
    "tags": []
   },
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "chat_agent = ConversationBot()\n",
    "X, X_train, X_test, y, y_train, y_test = chat_agent.load_artifacts()\n",
    "\n",
    "clf = LGBMRegressor()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "72fb79fc-bf38-421e-bc3d-751c145a96d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "class E2EPreprocessTool:\n",
    "    globals: Optional[Dict] = dict()\n",
    "    locals: Optional[Dict] = dict()\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.locals = {\"X\": X, 'y': y, 'np': np}\n",
    "        self.globals = {\n",
    "            \"np\": np,\n",
    "            'pd': pd,\n",
    "            # 'StandardScaler': StandardScaler,\n",
    "            # 'PowerTransformer': PowerTransformer,\n",
    "            # 'boxcox': boxcox,\n",
    "            # 'SMOTE': SMOTE,\n",
    "            # 'RandomOverSampler': RandomOverSampler\n",
    "        }\n",
    "\n",
    "    @prompts(\n",
    "        name=\"preprocess\",\n",
    "        description=\"This is a tool for dataset preprocessing.\"\n",
    "        \"Input should be a dataset name.\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        explore_prompts = self.explore_dataset(query)\n",
    "        self.preprocess_dataset(explore_prompts)\n",
    "        # self.evaluate()\n",
    "        \n",
    "    def retry_execute(self, prompt: str, code: str, retry: int = 3):\n",
    "        for _ in range(retry):\n",
    "            try:\n",
    "                exec(code, self.globals, self.locals)\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "                fix_code = coding_agent.run(f\"{prompt}\\n\\nExecute result:\\n{traceback.format_exc()}\\n\\nModify the code and fix the error.\")\n",
    "                code = extract_code(fix_code)\n",
    "                print(f\"Fix{_}:\\n{code}\\n\")\n",
    "        \n",
    "    def evaluate(self):\n",
    "\n",
    "        def common_preprocess(X_train, y_train, X_test, y_test):\n",
    "            # TODO: Missing value dropping\n",
    "\n",
    "            # TODO: Categorical feature encoding\n",
    "\n",
    "            # Scale numerical variables using StandardScaler\n",
    "            scaler = StandardScaler()\n",
    "            X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "            X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "            return X_train, y_train, X_test, y_test\n",
    "        \n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        from lightgbm import LGBMRegressor, early_stopping\n",
    "\n",
    "        seed = 42\n",
    "\n",
    "        def evaluate(X_train, y_train, X_test, y_test, prep_func=None):\n",
    "            if prep_func:\n",
    "                X_train, y_train, X_test, y_test = prep_func(X_train, y_train, X_test, y_test)\n",
    "\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=seed)\n",
    "            clf = LGBMRegressor(random_state=seed)\n",
    "            clf.fit(X_train, y_train, eval_set=(X_valid, y_valid), callbacks=[early_stopping(stopping_rounds=10)])\n",
    "            y_pred = clf.predict(X_test)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "            return mse\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.2, random_state=seed)\n",
    "\n",
    "\n",
    "        # LLM\n",
    "        mse_llm = evaluate(X_train, y_train, X_test, y_test, self.locals['preprocess'])\n",
    "\n",
    "        # Common\n",
    "        mse_com = evaluate(X_train, y_train, X_test, y_test, common_preprocess)\n",
    "\n",
    "        # Raw\n",
    "        mse_raw = evaluate(X_train, y_train, X_test, y_test)\n",
    "\n",
    "        msg = f\"No preprocess: {mse_raw}, Common preprocess: {mse_com}, LLM preprocess: {mse_llm}\"\n",
    "        print(msg)\n",
    "        return {\n",
    "            'msg': msg,\n",
    "            'no': mse_raw,\n",
    "            'common': mse_com,\n",
    "            'llm': mse_llm\n",
    "        }\n",
    "        \n",
    "    def preprocess_dataset(self, explore_prompts: str):\n",
    "        start_explore_prompt, how_to_explore_prompt, explore_data_thought, explore_data_result = explore_prompts\n",
    "        \n",
    "        preprocess_prompt = \"\"\"\n",
    "        The exploration results from the first stage are shown above.\n",
    "        Now, let's move on to the second stage:\n",
    "        1. Provide a conclusion based on the exploration results.\n",
    "        2. Based on the conclusion, specify the preprocessing techniques to be employed for the dataset and briefly describe their implementations.\n",
    "        3. Write a Python function called `preprocess` that will assist you in preprocessing the data.\n",
    "           Import any package you need beforehand.\n",
    "           The function will be called using `proc_X_train, proc_y_train, proc_X_test, proc_y_test = preprocess(X_train, y_train, X_test, y_test)`, \n",
    "           where both the parameters and return values are eithor Pandas dataframes or series.\n",
    "           Be aware of potential data leakage issues.\n",
    "\n",
    "        Here are some handy packages in the Python environment:\n",
    "        ```\n",
    "        numpy==1.24.3\n",
    "        pandas==2.0.1\n",
    "        ```\n",
    "        No additional packages can be used.\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        scikit-learn==1.2.2\n",
    "        scipy==1.10.1\n",
    "        imbalanced-learn==0.10.1\n",
    "        \"\"\"\n",
    "\n",
    "        prompt = \"\\n\\n\".join([\n",
    "            start_explore_prompt,\n",
    "            how_to_explore_prompt,\n",
    "            f\"Your Response:\\n{explore_data_thought}\",\n",
    "            f\"Code Execution Results:\\n{explore_data_result}\",\n",
    "            preprocess_prompt]\n",
    "        )\n",
    "        preprocess_data_thought = coding_agent.run(prompt)\n",
    "        print(preprocess_data_thought)\n",
    "        preprocess_code = extract_code(preprocess_data_thought)\n",
    "        self.preprocess_code = preprocess_code\n",
    "        \n",
    "        best_result = {\n",
    "            'score': 100,\n",
    "            'code': preprocess_code\n",
    "        }\n",
    "        \n",
    "        for _ in range(5):\n",
    "            try:\n",
    "                exec(preprocess_code, self.globals, self.locals)\n",
    "                result = self.evaluate()\n",
    "                \n",
    "                if result['llm'] < best_result['score']:\n",
    "                    best_result['score'] = result['llm']\n",
    "                    best_result['code'] = preprocess_code\n",
    "                \n",
    "                fix_code = coding_agent.run(f\"{prompt}\\n\\nExecute result:\\n{result['msg']}\\n\\nYour result is LLm result. Try to improve the preprocess function.\")\n",
    "                preprocess_code = extract_code(fix_code)\n",
    "                print(f\"Revise{_}:\\n{preprocess_code}\\n\")\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "                trace = traceback.format_exc()\n",
    "                print(trace)\n",
    "                fix_code = coding_agent.run(f\"{prompt}\\n\\nExecute result:\\n{str(e)}\\n\\nModify the code and fix the error. Try not to use the module that causes error. Try to simpify the function.\")\n",
    "                preprocess_code = extract_code(fix_code)\n",
    "                print(f\"Fix{_}:\\n{preprocess_code}\\n\")\n",
    "        \n",
    "        # self.retry_execute(preprocess_data_thought, preprocess_code)\n",
    "\n",
    "        \n",
    "    def explore_dataset(self, query: str):\n",
    "        start_explore_prompt = f\"\"\"\n",
    "You are a data scientist. You are now performing data analysis before training a machine learning model.\n",
    "Your task is to tell me how the data could be preprocessed and to give me the corresponding Python code.\n",
    "The preprocessing includes, but is not limited to, missing value handling, scaling, normalization, or encoding.\n",
    "The task will be performed in two stages.\n",
    "In the first stage, you will decide which techniques are required to preprocess the dataset by investigating the dataset.\n",
    "In the second stage, you will preprocess the dataset by implementing the functions.\n",
    "\n",
    "Here is some information about the feature dataframe `X`:\n",
    "\\\"\\\"\\\"\n",
    "- First five rows:\n",
    "{self.X.head(5)}\n",
    "\n",
    "- Column types:\n",
    "{self.X.dtypes}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Here is some information about the label series `y`:\n",
    "\\\"\\\"\\\"\n",
    "- First five rows:\n",
    "{self.y.head(5)}\n",
    "\n",
    "- Column types:\n",
    "{self.y.dtypes}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "        how_to_explore_prompt = \"\"\"\n",
    "Now, let's move on to the first stage:\n",
    "1. Explain how you will explore the dataset in order to select appropriate preprocessing techniques.\n",
    "2. Write a Python function called `explore` to explore the dataset.\n",
    "   Import any package you need beforehand.\n",
    "   The function will be called using `result = explore(X, y)`, and all findings must be stored in a string variable `result`.\n",
    "\n",
    "Here are some handy packages in the Python environment:\n",
    "```\n",
    "numpy==1.24.3\n",
    "pandas==2.0.1\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "        prompt = \"\\n\\n\".join([start_explore_prompt, how_to_explore_prompt])\n",
    "        self.prompt = prompt\n",
    "        print(prompt)\n",
    "\n",
    "        explore_data_thought = coding_agent.run(prompt)\n",
    "        self.explore_data_thought = explore_data_thought\n",
    "        print(explore_data_thought)\n",
    "        \n",
    "        explore_code = extract_code(explore_data_thought)\n",
    "        self.explore_code = explore_code\n",
    "        print(explore_code + '\\n')\n",
    "        self.retry_execute(explore_data_thought, explore_code)\n",
    "        \n",
    "        explore_data_result = self.locals[\"explore\"](self.X, self.y)\n",
    "        self.explore_data_result = explore_data_result\n",
    "        print(explore_data_result)\n",
    "        \n",
    "        return (start_explore_prompt, how_to_explore_prompt, explore_data_thought, explore_data_result)\n",
    "    \n",
    "    \n",
    "preprocess_tool = E2EPreprocessTool(X, y)\n",
    "preprocess_tool.inference(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3d1c67-2bf7-40c4-97ff-b377a6153426",
   "metadata": {},
   "source": [
    "# Demo start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b7628c5-ab46-4c4a-9b1c-6a8c43f6b969",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Load\",\n",
      "    \"action_input\": \"house-prices-advanced-regression-techniques\"\n",
      "}\u001b[0m(1460, 80) (1459, 80) (1460,) (1459,)\n",
      "\n",
      "Observation: \u001b[31;1m\u001b[1;3mLoad dataset house-prices-advanced-regression-techniques success !\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"Load dataset house-prices-advanced-regression-techniques success !\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Load dataset house-prices-advanced-regression-techniques success !'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_agent = ConversationBot()\n",
    "chat_agent.run_text(\"Can you load the house-prices-advanced-regression-techniques dataset ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13edb545-ac4a-4f86-b028-ff80d2efe7b0",
   "metadata": {},
   "source": [
    "# Demo 1: Can you explore the dataset ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "34b1cd82-cb8c-4c11-a14f-37e6f83f33a4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Explore\",\n",
      "    \"action_input\": \"\"\n",
      "}\u001b[0mhouse-prices-advanced-regression-techniques\n",
      "\n",
      "You are a data scientist. You are now performing data analysis before training a machine learning model.\n",
      "Your task is to tell me how the data could be preprocessed and to give me the corresponding Python code.\n",
      "The preprocessing includes, but is not limited to, missing value handling, scaling for numerical features, and categorical encoding.\n",
      "The task will be performed in two stages.\n",
      "In the first stage, you will decide which techniques are required to preprocess the dataset by investigating the dataset.\n",
      "In the second stage, you will preprocess the dataset by implementing the functions.\n",
      "\n",
      "Here is some information about the feature dataframe `X`:\n",
      "\"\"\"\n",
      "- First five rows:\n",
      "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
      "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
      "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
      "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
      "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
      "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
      "\n",
      "  LandContour Utilities  ... ScreenPorch PoolArea PoolQC Fence MiscFeature  \\\n",
      "0         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n",
      "1         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n",
      "2         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n",
      "3         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n",
      "4         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n",
      "\n",
      "  MiscVal MoSold  YrSold  SaleType  SaleCondition  \n",
      "0       0      2    2008        WD         Normal  \n",
      "1       0      5    2007        WD         Normal  \n",
      "2       0      9    2008        WD         Normal  \n",
      "3       0      2    2006        WD        Abnorml  \n",
      "4       0     12    2008        WD         Normal  \n",
      "\n",
      "[5 rows x 80 columns]\n",
      "\n",
      "- Column types:\n",
      "Id                 int64\n",
      "MSSubClass         int64\n",
      "MSZoning          object\n",
      "LotFrontage      float64\n",
      "LotArea            int64\n",
      "                  ...   \n",
      "MiscVal            int64\n",
      "MoSold             int64\n",
      "YrSold             int64\n",
      "SaleType          object\n",
      "SaleCondition     object\n",
      "Length: 80, dtype: object\n",
      "\"\"\"\n",
      "\n",
      "Here is some information about the label series `y`:\n",
      "\"\"\"\n",
      "- First five rows:\n",
      "0    208500\n",
      "1    181500\n",
      "2    223500\n",
      "3    140000\n",
      "4    250000\n",
      "Name: SalePrice, dtype: int64\n",
      "\n",
      "- Column types:\n",
      "int64\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "\n",
      "Now, let's move on to the first stage:\n",
      "1. Explain how you will explore the dataset in order to select appropriate preprocessing techniques.\n",
      "   Analyze numerical and categorical features separately.\n",
      "2. Write a Python function called `explore` for data exploration.\n",
      "   Import any required packages WITHIN the `explore` function. That is, The first line of code should be the definition of the function, followed by the necessary package imports.\n",
      "   The function will be called using `result = explore(X, y)`, and all conclusions should be stored in a string variable `result`.\n",
      "   \n",
      "   Make the code and the returned string concise. Avoid listing specific data information such as row indices or column names.\n",
      "   Here are some GOOD examples of concise conclusions (not limited) to add to the `result`:\n",
      "   - \"There are no missing values in the dataset.\"\n",
      "   - \"There are {num_cat} categorical features. Consider applying a one-hot encoding\"\n",
      "   - \"The numerical features are highly skewed (mean skewness = {mean_skew:.2f}). Consider applying a log transformation or a power transformation.\"\n",
      "   - \"There are no outliers in the numerical features.\"\n",
      "   - \"The categorical features have high cardinality (mean cardinality = {mean_card:.2f}).\"\n",
      "   Here are some BAD examples of conclusions (not limited) to avoid adding to result as they specify data information and lack conciseness:\n",
      "   - \"Numerical feature: {X[numerical_features].describe().to_string()}\"\n",
      "   - \"Categorical feature: {X[categorical_features].nunique().to_string()}\"\n",
      "   - \"There are {num_outliers_per_feature} outliers in the feature {col_name}.\" (inside a for loop)\n",
      "\n",
      "   These are the only packages in the Python environment:\n",
      "   ```\n",
      "   numpy==1.24.3\n",
      "   pandas==2.0.1\n",
      "   ```\n",
      "   No additional packages can be used.\n",
      "\n",
      "First, I will investigate the numerical features by checking their distributions, skewness, and presence of outliers. I will use histograms, boxplots, and skewness measures to visualize and quantify these characteristics. If the numerical features are highly skewed, I will consider applying a log transformation or a power transformation to make them more normally distributed. If there are outliers, I will consider removing them or using robust scaling techniques.\n",
      "\n",
      "Next, I will investigate the categorical features by checking their cardinality and the presence of missing values. If the categorical features have high cardinality, I will consider applying a one-hot encoding or a feature hashing technique to reduce the number of features. If there are missing values, I will consider imputing them using the mode or a custom imputation strategy.\n",
      "\n",
      "Finally, I will check the correlation between the features and the label to identify any strong linear relationships. If there are strong linear relationships, I will consider applying feature selection techniques to reduce the number of features and improve the model's interpretability.\n",
      "\n",
      "Here is the `explore` function:\n",
      "\n",
      "```python\n",
      "def explore(X, y):\n",
      "    import numpy as np\n",
      "    import pandas as pd\n",
      "    \n",
      "    # Check for missing values\n",
      "    if X.isnull().sum().sum() == 0:\n",
      "        result = \"There are no missing values in the dataset.\\n\"\n",
      "    else:\n",
      "        result = \"There are missing values in the dataset. Consider imputing them.\\n\"\n",
      "    \n",
      "    # Check numerical features\n",
      "    num_features = X.select_dtypes(include=np.number).columns\n",
      "    num_skewness = X[num_features].skew()\n",
      "    num_outliers = pd.DataFrame()\n",
      "    for col in num_features:\n",
      "        q1 = X[col].quantile(0.25)\n",
      "        q3 = X[col].quantile(0.75)\n",
      "        iqr = q3 - q1\n",
      "        outliers = X[(X[col] < q1 - 1.5*iqr) | (X[col] > q3 + 1.5*iqr)][col]\n",
      "        num_outliers = pd.concat([num_outliers, outliers], axis=0)\n",
      "    if num_outliers.empty:\n",
      "        result += \"There are no outliers in the numerical features.\\n\"\n",
      "    else:\n",
      "        result += f\"There are {len(num_outliers)} outliers in the numerical features. Consider removing them or using robust scaling techniques.\\n\"\n",
      "    mean_skew = num_skewness.mean()\n",
      "    if mean_skew > 1:\n",
      "        result += f\"The numerical features are highly skewed (mean skewness = {mean_skew:.2f}). Consider applying a log transformation or a power transformation.\\n\"\n",
      "    \n",
      "    # Check categorical features\n",
      "    cat_features = X.select_dtypes(include='object').columns\n",
      "    cat_cardinality = X[cat_features].nunique()\n",
      "    mean_card = cat_cardinality.mean()\n",
      "    if mean_card > 10:\n",
      "        result += f\"The categorical features have high cardinality (mean cardinality = {mean_card:.2f}). Consider applying a one-hot encoding or a feature hashing technique.\\n\"\n",
      "    if X[cat_features].isnull().sum().sum() > 0:\n",
      "        result += \"There are missing values in the categorical features. Consider imputing them.\\n\"\n",
      "    \n",
      "    # Check correlation with label\n",
      "    corr = X.corrwith(y).abs().sort_values(ascending=False)\n",
      "    if corr.iloc[1] > 0.5:\n",
      "        result += f\"The feature {corr.index[1]} has a strong linear relationship with the label. Consider applying feature selection techniques.\\n\"\n",
      "    \n",
      "    return result.strip()\n",
      "```\n",
      "\n",
      "def explore(X, y):\n",
      "    import numpy as np\n",
      "    import pandas as pd\n",
      "    \n",
      "    # Check for missing values\n",
      "    if X.isnull().sum().sum() == 0:\n",
      "        result = \"There are no missing values in the dataset.\\n\"\n",
      "    else:\n",
      "        result = \"There are missing values in the dataset. Consider imputing them.\\n\"\n",
      "    \n",
      "    # Check numerical features\n",
      "    num_features = X.select_dtypes(include=np.number).columns\n",
      "    num_skewness = X[num_features].skew()\n",
      "    num_outliers = pd.DataFrame()\n",
      "    for col in num_features:\n",
      "        q1 = X[col].quantile(0.25)\n",
      "        q3 = X[col].quantile(0.75)\n",
      "        iqr = q3 - q1\n",
      "        outliers = X[(X[col] < q1 - 1.5*iqr) | (X[col] > q3 + 1.5*iqr)][col]\n",
      "        num_outliers = pd.concat([num_outliers, outliers], axis=0)\n",
      "    if num_outliers.empty:\n",
      "        result += \"There are no outliers in the numerical features.\\n\"\n",
      "    else:\n",
      "        result += f\"There are {len(num_outliers)} outliers in the numerical features. Consider removing them or using robust scaling techniques.\\n\"\n",
      "    mean_skew = num_skewness.mean()\n",
      "    if mean_skew > 1:\n",
      "        result += f\"The numerical features are highly skewed (mean skewness = {mean_skew:.2f}). Consider applying a log transformation or a power transformation.\\n\"\n",
      "    \n",
      "    # Check categorical features\n",
      "    cat_features = X.select_dtypes(include='object').columns\n",
      "    cat_cardinality = X[cat_features].nunique()\n",
      "    mean_card = cat_cardinality.mean()\n",
      "    if mean_card > 10:\n",
      "        result += f\"The categorical features have high cardinality (mean cardinality = {mean_card:.2f}). Consider applying a one-hot encoding or a feature hashing technique.\\n\"\n",
      "    if X[cat_features].isnull().sum().sum() > 0:\n",
      "        result += \"There are missing values in the categorical features. Consider imputing them.\\n\"\n",
      "    \n",
      "    # Check correlation with label\n",
      "    corr = X.corrwith(y).abs().sort_values(ascending=False)\n",
      "    if corr.iloc[1] > 0.5:\n",
      "        result += f\"The feature {corr.index[1]} has a strong linear relationship with the label. Consider applying feature selection techniques.\\n\"\n",
      "    \n",
      "    return result.strip()\n",
      "\n",
      "\n",
      "_ = explore(X, y)\n",
      "\n",
      "There are missing values in the dataset. Consider imputing them.\n",
      "There are 1596 outliers in the numerical features. Consider removing them or using robust scaling techniques.\n",
      "The numerical features are highly skewed (mean skewness = 3.00). Consider applying a log transformation or a power transformation.\n",
      "There are missing values in the categorical features. Consider imputing them.\n",
      "The feature GrLivArea has a strong linear relationship with the label. Consider applying feature selection techniques.\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mThere are missing values in the dataset. Consider imputing them.\n",
      "There are 1596 outliers in the numerical features. Consider removing them or using robust scaling techniques.\n",
      "The numerical features are highly skewed (mean skewness = 3.00). Consider applying a log transformation or a power transformation.\n",
      "There are missing values in the categorical features. Consider imputing them.\n",
      "The feature GrLivArea has a strong linear relationship with the label. Consider applying feature selection techniques.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"Based on the dataset exploration, it is recommended to impute missing values in both numerical and categorical features, remove or use robust scaling techniques for the 1596 outliers in the numerical features, apply a log transformation or a power transformation to the highly skewed numerical features, and consider applying feature selection techniques on the GrLivArea feature which has a strong linear relationship with the label.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the dataset exploration, it is recommended to impute missing values in both numerical and categorical features, remove or use robust scaling techniques for the 1596 outliers in the numerical features, apply a log transformation or a power transformation to the highly skewed numerical features, and consider applying feature selection techniques on the GrLivArea feature which has a strong linear relationship with the label.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_agent = ConversationBot()\n",
    "chat_agent.run_text(\"Can you explore the dataset ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b70ed09-bf75-4653-8b1f-0993a9f0ab78",
   "metadata": {},
   "source": [
    "# Demo 2: Can you preprocess the dataset ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "26a0e3a0-bd54-42c7-81b6-a6b8effb4309",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Preprocess\",\n",
      "    \"action_input\": \"\"\n",
      "}\u001b[0m\n",
      "\n",
      "You are a data scientist. You are now performing data analysis before training a machine learning model.\n",
      "Your task is to tell me how the data could be preprocessed and to give me the corresponding Python code.\n",
      "The preprocessing includes, but is not limited to, missing value handling, scaling for numerical features, and categorical encoding.\n",
      "The task will be performed in two stages.\n",
      "In the first stage, you will decide which techniques are required to preprocess the dataset by investigating the dataset.\n",
      "In the second stage, you will preprocess the dataset by implementing the functions.\n",
      "\n",
      "Here is some information about the feature dataframe `X`:\n",
      "\"\"\"\n",
      "- First five rows:\n",
      "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
      "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
      "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
      "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
      "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
      "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
      "\n",
      "  LandContour Utilities  ... ScreenPorch PoolArea PoolQC Fence MiscFeature  \\\n",
      "0         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n",
      "1         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n",
      "2         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n",
      "3         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n",
      "4         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n",
      "\n",
      "  MiscVal MoSold  YrSold  SaleType  SaleCondition  \n",
      "0       0      2    2008        WD         Normal  \n",
      "1       0      5    2007        WD         Normal  \n",
      "2       0      9    2008        WD         Normal  \n",
      "3       0      2    2006        WD        Abnorml  \n",
      "4       0     12    2008        WD         Normal  \n",
      "\n",
      "[5 rows x 80 columns]\n",
      "\n",
      "- Column types:\n",
      "Id                 int64\n",
      "MSSubClass         int64\n",
      "MSZoning          object\n",
      "LotFrontage      float64\n",
      "LotArea            int64\n",
      "                  ...   \n",
      "MiscVal            int64\n",
      "MoSold             int64\n",
      "YrSold             int64\n",
      "SaleType          object\n",
      "SaleCondition     object\n",
      "Length: 80, dtype: object\n",
      "\"\"\"\n",
      "\n",
      "Here is some information about the label series `y`:\n",
      "\"\"\"\n",
      "- First five rows:\n",
      "0    208500\n",
      "1    181500\n",
      "2    223500\n",
      "3    140000\n",
      "4    250000\n",
      "Name: SalePrice, dtype: int64\n",
      "\n",
      "- Column types:\n",
      "int64\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "First-stage Thoughts:\n",
      "First, I will investigate the numerical features by checking their distributions, skewness, and presence of outliers. I will use histograms, boxplots, and skewness measures to visualize and quantify these characteristics. If the numerical features are highly skewed, I will consider applying a log transformation or a power transformation to make them more normally distributed. If there are outliers, I will consider removing them or using robust scaling techniques.\n",
      "\n",
      "Next, I will investigate the categorical features by checking their cardinality and the presence of missing values. If the categorical features have high cardinality, I will consider applying a one-hot encoding or a feature hashing technique to reduce the number of features. If there are missing values, I will consider imputing them using the mode or a custom imputation strategy.\n",
      "\n",
      "Finally, I will check the correlation between the features and the label to identify any strong linear relationships. If there are strong linear relationships, I will consider applying feature selection techniques to reduce the number of features and improve the model's interpretability.\n",
      "\n",
      "Here is the `explore` function:\n",
      "\n",
      "```python\n",
      "def explore(X, y):\n",
      "    import numpy as np\n",
      "    import pandas as pd\n",
      "    \n",
      "    # Check for missing values\n",
      "    if X.isnull().sum().sum() == 0:\n",
      "        result = \"There are no missing values in the dataset.\\n\"\n",
      "    else:\n",
      "        result = \"There are missing values in the dataset. Consider imputing them.\\n\"\n",
      "    \n",
      "    # Check numerical features\n",
      "    num_features = X.select_dtypes(include=np.number).columns\n",
      "    num_skewness = X[num_features].skew()\n",
      "    num_outliers = pd.DataFrame()\n",
      "    for col in num_features:\n",
      "        q1 = X[col].quantile(0.25)\n",
      "        q3 = X[col].quantile(0.75)\n",
      "        iqr = q3 - q1\n",
      "        outliers = X[(X[col] < q1 - 1.5*iqr) | (X[col] > q3 + 1.5*iqr)][col]\n",
      "        num_outliers = pd.concat([num_outliers, outliers], axis=0)\n",
      "    if num_outliers.empty:\n",
      "        result += \"There are no outliers in the numerical features.\\n\"\n",
      "    else:\n",
      "        result += f\"There are {len(num_outliers)} outliers in the numerical features. Consider removing them or using robust scaling techniques.\\n\"\n",
      "    mean_skew = num_skewness.mean()\n",
      "    if mean_skew > 1:\n",
      "        result += f\"The numerical features are highly skewed (mean skewness = {mean_skew:.2f}). Consider applying a log transformation or a power transformation.\\n\"\n",
      "    \n",
      "    # Check categorical features\n",
      "    cat_features = X.select_dtypes(include='object').columns\n",
      "    cat_cardinality = X[cat_features].nunique()\n",
      "    mean_card = cat_cardinality.mean()\n",
      "    if mean_card > 10:\n",
      "        result += f\"The categorical features have high cardinality (mean cardinality = {mean_card:.2f}). Consider applying a one-hot encoding or a feature hashing technique.\\n\"\n",
      "    if X[cat_features].isnull().sum().sum() > 0:\n",
      "        result += \"There are missing values in the categorical features. Consider imputing them.\\n\"\n",
      "    \n",
      "    # Check correlation with label\n",
      "    corr = X.corrwith(y).abs().sort_values(ascending=False)\n",
      "    if corr.iloc[1] > 0.5:\n",
      "        result += f\"The feature {corr.index[1]} has a strong linear relationship with the label. Consider applying feature selection techniques.\\n\"\n",
      "    \n",
      "    return result.strip()\n",
      "```\n",
      "\n",
      "First-stage Conclusion:\n",
      "There are missing values in the dataset. Consider imputing them.\n",
      "There are 1596 outliers in the numerical features. Consider removing them or using robust scaling techniques.\n",
      "The numerical features are highly skewed (mean skewness = 3.00). Consider applying a log transformation or a power transformation.\n",
      "There are missing values in the categorical features. Consider imputing them.\n",
      "The feature GrLivArea has a strong linear relationship with the label. Consider applying feature selection techniques.\n",
      "\n",
      "\n",
      "The exploration results from the first stage are shown above.\n",
      "Now, let's move on to the second stage:\n",
      "1. Based on the conclusion, specify the preprocessing techniques to be employed for the dataset and briefly describe their implementations.\n",
      "2. Write a Python function called `preprocess` that will assist you in preprocessing the data.\n",
      "   Import any required packages WITHIN the `preprocess` function. That is, The first line of code should be the definition of the function, followed by the necessary package imports.\n",
      "   The function will be called using `proc_X_train, proc_y_train, proc_X_test, proc_y_test = preprocess(X_train, y_train, X_test, y_test)`, \n",
      "   where both the parameters and return values are eithor Pandas dataframes or series.\n",
      "   Only consider common preprocessing methods and be aware of potential data leakage issues.\n",
      "   Make the code concise. Avoid listing specific data information such as row indices or column names.\n",
      "   Make the code simple.\n",
      "\n",
      "   These are the only packages in the Python environment:\n",
      "   ```\n",
      "   numpy==1.24.3\n",
      "   pandas==2.0.1\n",
      "   scikit-learn==1.2.2\n",
      "   scipy==1.10.1\n",
      "   category_encoders==2.6.1\n",
      "   ```\n",
      "   No additional packages can be used.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8ccec54e2c4d2163d83d82ee48f7d36a in your message.).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised APIError: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Tue, 23 May 2023 08:01:07 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7cbbbb9aedb84a84-TPE', 'alt-svc': 'h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Preprocessing Techniques:\n",
      "- Missing Value Handling: Impute missing values in numerical features with the median and in categorical features with the mode.\n",
      "- Scaling for Numerical Features: Apply a power transformation to the numerical features using the Yeo-Johnson method to make them more normally distributed and scale them using the RobustScaler to reduce the effect of outliers.\n",
      "- Categorical Encoding: Apply a one-hot encoding to the categorical features using the OneHotEncoder from scikit-learn.\n",
      "\n",
      "2. `preprocess` Function:\n",
      "```python\n",
      "def preprocess(X_train, y_train, X_test, y_test):\n",
      "    from sklearn.impute import SimpleImputer\n",
      "    from sklearn.preprocessing import PowerTransformer, RobustScaler, OneHotEncoder\n",
      "    from category_encoders import OneHotEncoder as CatOneHotEncoder\n",
      "    \n",
      "    # Impute missing values\n",
      "    num_imputer = SimpleImputer(strategy='median')\n",
      "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
      "    X_train_num = num_imputer.fit_transform(X_train.select_dtypes(include=np.number))\n",
      "    X_train_cat = cat_imputer.fit_transform(X_train.select_dtypes(include='object'))\n",
      "    X_test_num = num_imputer.transform(X_test.select_dtypes(include=np.number))\n",
      "    X_test_cat = cat_imputer.transform(X_test.select_dtypes(include='object'))\n",
      "    \n",
      "    # Scale numerical features\n",
      "    pt = PowerTransformer(method='yeo-johnson')\n",
      "    scaler = RobustScaler()\n",
      "    X_train_num = pt.fit_transform(X_train_num)\n",
      "    X_train_num = scaler.fit_transform(X_train_num)\n",
      "    X_test_num = pt.transform(X_test_num)\n",
      "    X_test_num = scaler.transform(X_test_num)\n",
      "    \n",
      "    # One-hot encode categorical features\n",
      "    ohe = OneHotEncoder(handle_unknown='ignore')\n",
      "    X_train_cat = ohe.fit_transform(X_train_cat)\n",
      "    X_test_cat = ohe.transform(X_test_cat)\n",
      "    \n",
      "    # Combine numerical and categorical features\n",
      "    X_train = np.hstack((X_train_num, X_train_cat.toarray()))\n",
      "    X_test = np.hstack((X_test_num, X_test_cat.toarray()))\n",
      "    \n",
      "    # Encode label\n",
      "    y_train = pt.fit_transform(y_train.values.reshape(-1, 1))\n",
      "    y_test = pt.transform(y_test.values.reshape(-1, 1))\n",
      "    \n",
      "    return X_train, y_train.ravel(), X_test, y_test.ravel()\n",
      "```\n",
      "\n",
      "The `preprocess` function first imputes missing values in numerical features with the median and in categorical features with the mode using the SimpleImputer from scikit-learn. Then, it applies a power transformation to the numerical features using the Yeo-Johnson method and scales them using the RobustScaler from scikit-learn to reduce the effect of outliers. Next, it applies a one-hot encoding to the categorical features using the OneHotEncoder from scikit-learn. The function then combines the numerical and categorical features using np.hstack. Finally, it encodes the label using the same power transformation as the numerical features. The function returns the preprocessed data as numpy arrays and the encoded label as numpy arrays.\n",
      "\n",
      "def preprocess(X_train, y_train, X_test, y_test):\n",
      "    from sklearn.impute import SimpleImputer\n",
      "    from sklearn.preprocessing import PowerTransformer, RobustScaler, OneHotEncoder\n",
      "    from category_encoders import OneHotEncoder as CatOneHotEncoder\n",
      "    \n",
      "    # Impute missing values\n",
      "    num_imputer = SimpleImputer(strategy='median')\n",
      "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
      "    X_train_num = num_imputer.fit_transform(X_train.select_dtypes(include=np.number))\n",
      "    X_train_cat = cat_imputer.fit_transform(X_train.select_dtypes(include='object'))\n",
      "    X_test_num = num_imputer.transform(X_test.select_dtypes(include=np.number))\n",
      "    X_test_cat = cat_imputer.transform(X_test.select_dtypes(include='object'))\n",
      "    \n",
      "    # Scale numerical features\n",
      "    pt = PowerTransformer(method='yeo-johnson')\n",
      "    scaler = RobustScaler()\n",
      "    X_train_num = pt.fit_transform(X_train_num)\n",
      "    X_train_num = scaler.fit_transform(X_train_num)\n",
      "    X_test_num = pt.transform(X_test_num)\n",
      "    X_test_num = scaler.transform(X_test_num)\n",
      "    \n",
      "    # One-hot encode categorical features\n",
      "    ohe = OneHotEncoder(handle_unknown='ignore')\n",
      "    X_train_cat = ohe.fit_transform(X_train_cat)\n",
      "    X_test_cat = ohe.transform(X_test_cat)\n",
      "    \n",
      "    # Combine numerical and categorical features\n",
      "    X_train = np.hstack((X_train_num, X_train_cat.toarray()))\n",
      "    X_test = np.hstack((X_test_num, X_test_cat.toarray()))\n",
      "    \n",
      "    # Encode label\n",
      "    y_train = pt.fit_transform(y_train.values.reshape(-1, 1))\n",
      "    y_test = pt.transform(y_test.values.reshape(-1, 1))\n",
      "    \n",
      "    return X_train, y_train.ravel(), X_test, y_test.ravel()\n",
      "\n",
      "\n",
      "_, _, _, _ = preprocess(X_train, y_train, X_test, y_test)\n",
      "\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m1. Preprocessing Techniques:\n",
      "- Missing Value Handling: Impute missing values in numerical features with the median and in categorical features with the mode.\n",
      "- Scaling for Numerical Features: Apply a power transformation to the numerical features using the Yeo-Johnson method to make them more normally distributed and scale them using the RobustScaler to reduce the effect of outliers.\n",
      "- Categorical Encoding: Apply a one-hot encoding to the categorical features using the OneHotEncoder from scikit-learn.\n",
      "\n",
      "2. `preprocess` Function:\n",
      "```python\n",
      "def preprocess(X_train, y_train, X_test, y_test):\n",
      "    from sklearn.impute import SimpleImputer\n",
      "    from sklearn.preprocessing import PowerTransformer, RobustScaler, OneHotEncoder\n",
      "    from category_encoders import OneHotEncoder as CatOneHotEncoder\n",
      "    \n",
      "    # Impute missing values\n",
      "    num_imputer = SimpleImputer(strategy='median')\n",
      "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
      "    X_train_num = num_imputer.fit_transform(X_train.select_dtypes(include=np.number))\n",
      "    X_train_cat = cat_imputer.fit_transform(X_train.select_dtypes(include='object'))\n",
      "    X_test_num = num_imputer.transform(X_test.select_dtypes(include=np.number))\n",
      "    X_test_cat = cat_imputer.transform(X_test.select_dtypes(include='object'))\n",
      "    \n",
      "    # Scale numerical features\n",
      "    pt = PowerTransformer(method='yeo-johnson')\n",
      "    scaler = RobustScaler()\n",
      "    X_train_num = pt.fit_transform(X_train_num)\n",
      "    X_train_num = scaler.fit_transform(X_train_num)\n",
      "    X_test_num = pt.transform(X_test_num)\n",
      "    X_test_num = scaler.transform(X_test_num)\n",
      "    \n",
      "    # One-hot encode categorical features\n",
      "    ohe = OneHotEncoder(handle_unknown='ignore')\n",
      "    X_train_cat = ohe.fit_transform(X_train_cat)\n",
      "    X_test_cat = ohe.transform(X_test_cat)\n",
      "    \n",
      "    # Combine numerical and categorical features\n",
      "    X_train = np.hstack((X_train_num, X_train_cat.toarray()))\n",
      "    X_test = np.hstack((X_test_num, X_test_cat.toarray()))\n",
      "    \n",
      "    # Encode label\n",
      "    y_train = pt.fit_transform(y_train.values.reshape(-1, 1))\n",
      "    y_test = pt.transform(y_test.values.reshape(-1, 1))\n",
      "    \n",
      "    return X_train, y_train.ravel(), X_test, y_test.ravel()\n",
      "```\n",
      "\n",
      "The `preprocess` function first imputes missing values in numerical features with the median and in categorical features with the mode using the SimpleImputer from scikit-learn. Then, it applies a power transformation to the numerical features using the Yeo-Johnson method and scales them using the RobustScaler from scikit-learn to reduce the effect of outliers. Next, it applies a one-hot encoding to the categorical features using the OneHotEncoder from scikit-learn. The function then combines the numerical and categorical features using np.hstack. Finally, it encodes the label using the same power transformation as the numerical features. The function returns the preprocessed data as numpy arrays and the encoded label as numpy arrays.\n",
      "\n",
      "Processing....\n",
      "\n",
      "Process Completed !\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The `preprocess` function imputes missing values, applies a power transformation to the numerical features using the Yeo-Johnson method, scales them using the RobustScaler to reduce the effect of outliers, applies a one-hot encoding to the categorical features, combines the numerical and categorical features, and encodes the label using the same power transformation as the numerical features.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The `preprocess` function imputes missing values, applies a power transformation to the numerical features using the Yeo-Johnson method, scales them using the RobustScaler to reduce the effect of outliers, applies a one-hot encoding to the categorical features, combines the numerical and categorical features, and encodes the label using the same power transformation as the numerical features.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_agent = ConversationBot()\n",
    "chat_agent.run_text(\"Can you process the dataset ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01619d9-e173-4ba1-a7ac-c1fce627a9f1",
   "metadata": {},
   "source": [
    "# Demo 3: Can you select the model ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "623040e3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Pick Model\",\n",
      "    \"action_input\": \"\"\n",
      "}\u001b[0m```python\n",
      "import numpy as np\n",
      "\n",
      "def regression_or_classification(X, y):\n",
      "    \"\"\"\n",
      "    This function checks the type of the output variable in the label series `y` and helps to decide whether to use a regression or classifier model.\n",
      "    \n",
      "    Parameters:\n",
      "    X (pandas.DataFrame): The feature dataframe.\n",
      "    y (pandas.Series): The label series.\n",
      "    \n",
      "    Returns:\n",
      "    result (str): A string variable containing the decision on whether to use a regression or classifier model.\n",
      "    \"\"\"\n",
      "    \n",
      "    if isinstance(y[0], (int, np.integer, float, np.floating)):\n",
      "        result = \"The output variable is continuous, so a regression model may be more appropriate.\"\n",
      "    elif isinstance(y[0], str):\n",
      "        result = \"The output variable is categorical, so a classifier model may be more appropriate.\"\n",
      "    else:\n",
      "        result = \"The output variable is neither continuous nor categorical. Please check the data and try again.\"\n",
      "    \n",
      "    return result\n",
      "```\n",
      "\n",
      "To use this function, simply call it with the feature dataframe `X` and label series `y` as arguments:\n",
      "\n",
      "```python\n",
      "result = regression_or_classification(X, y)\n",
      "print(result)\n",
      "```\n",
      "\n",
      "The output will be a string variable containing the decision on whether to use a regression or classifier model based on the type of the output variable in the label series `y`.\n",
      "AI:\n",
      "```python\n",
      "def construct_model():\n",
      "    \"\"\"\n",
      "    This function creates an instance of the LightGBM regressor model.\n",
      "    \n",
      "    Returns:\n",
      "    clf (lightgbm.LGBMRegressor): An instance of the LightGBM regressor model.\n",
      "    \"\"\"\n",
      "    \n",
      "    import lightgbm as lgb\n",
      "    \n",
      "    clf = lgb.LGBMRegressor()\n",
      "    \n",
      "    return clf\n",
      "```\n",
      "\n",
      "To use this function, simply call it to create an instance of the LightGBM regressor model:\n",
      "\n",
      "```python\n",
      "clf = construct_model()\n",
      "```\n",
      "\n",
      "The `clf` variable will contain an instance of the LightGBM regressor model that can be used for training and prediction.\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mAI:\n",
      "```python\n",
      "def construct_model():\n",
      "    \"\"\"\n",
      "    This function creates an instance of the LightGBM regressor model.\n",
      "    \n",
      "    Returns:\n",
      "    clf (lightgbm.LGBMRegressor): An instance of the LightGBM regressor model.\n",
      "    \"\"\"\n",
      "    \n",
      "    import lightgbm as lgb\n",
      "    \n",
      "    clf = lgb.LGBMRegressor()\n",
      "    \n",
      "    return clf\n",
      "```\n",
      "\n",
      "To use this function, simply call it to create an instance of the LightGBM regressor model:\n",
      "\n",
      "```python\n",
      "clf = construct_model()\n",
      "```\n",
      "\n",
      "The `clf` variable will contain an instance of the LightGBM regressor model that can be used for training and prediction.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"Based on the available information, I would suggest using the LightGBM regressor model for your machine learning task. You can create an instance of this model by calling the `construct_model()` function provided above. Good luck with your project!\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the available information, I would suggest using the LightGBM regressor model for your machine learning task. You can create an instance of this model by calling the `construct_model()` function provided above. Good luck with your project!'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_agent = ConversationBot()\n",
    "chat_agent.run_text(\"Can you pick a suitable model ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88050eb-4494-4175-82fa-b0c11d9cd1b4",
   "metadata": {},
   "source": [
    "# Demo 4: Can you run hyper parameter tuning ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3fccba47-9d07-4ec5-8731-3c7691a861dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "clf = LGBMClassifier()\n",
    "param_grid = {'learning_rate': [0.01, 0.05, 0.1, 0.5], 'n_estimators': [50, 100, 200, 500], 'max_depth': [3, 5, 7, 9], 'num_leaves': [15, 31, 63, 127], 'min_child_samples': [10, 20, 30, 50], 'reg_alpha': [0.0, 0.1, 0.5, 1.0], 'reg_lambda': [0.0, 0.1, 0.5, 1.0], 'colsample_bytree': [0.5, 0.7, 1.0], 'subsample': [0.5, 0.7, 1.0]}\n",
    "hyper_algo = RandomizedSearchCV(clf, param_grid, random_state=0, n_iter=10, n_jobs=24)\n",
    "search = hyper_algo.fit(ss.X_train.drop(\"ID_code\", axis=1), ss.y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6463a56a-65ef-46db-9cb4-c8012e5f9dd8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"hyperopt\",\n",
      "    \"action_input\": \"\"\n",
      "}\u001b[0m\n",
      "Construct estimator with following:\n",
      "```\n",
      "import lightgbm as lgb\n",
      "\n",
      "clf = lgb.LGBMRegressor()\n",
      "```\n",
      "\n",
      "\n",
      "Construct params grid:\n",
      "{'learning_rate': [0.01, 0.05, 0.1, 0.5], 'n_estimators': [50, 100, 200, 500], 'max_depth': [3, 5, 7, 9], 'num_leaves': [15, 31, 63, 127], 'min_child_samples': [10, 20, 30, 50], 'reg_alpha': [0.0, 0.1, 0.5, 1.0], 'reg_lambda': [0.0, 0.1, 0.5, 1.0], 'colsample_bytree': [0.6, 0.8, 1.0], 'subsample': [0.6, 0.8, 1.0]}\n",
      "\n",
      "I have chosen to use RandomizedSearchCV for the hyper parameter search on the LGBMRegressor model. This is because RandomizedSearchCV is a good choice when the parameter space is large and it is not feasible to try all possible combinations. It also allows for a more efficient search by randomly sampling a subset of the parameter space.\n",
      "\n",
      "I have not chosen to use GridSearchCV because it would require trying all possible combinations of the parameters, which would be computationally expensive and time-consuming. \n",
      "\n",
      "I have not chosen to use HalvingGridSearchCV or HalvingRandomSearchCV because these algorithms are designed to be more efficient by eliminating poorly performing parameter combinations early on. However, since the LGBMRegressor model is relatively fast to train, it is not necessary to use these algorithms to save time. \n",
      "\n",
      "I have not chosen to use ParameterGrid or ParameterSampler because they are not as efficient as RandomizedSearchCV for large parameter spaces. They also do not allow for random sampling, which can be useful for exploring the parameter space more thoroughly.\n",
      "\n",
      "Deciding search algo:\n",
      "RandomizedSearchCV\n",
      "\n",
      "\n",
      "Constructing search algo:\n",
      "```\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "hyper_algo = RandomizedSearchCV(clf, param_grid, random_state=0, n_iter=50, n_jobs=24)\n",
      "```\n",
      "\n",
      "Best search score: 0.8926749301302405\n",
      "> \u001b[0;32m/tmp/ipykernel_1757992/4292860630.py\u001b[0m(67)\u001b[0;36mevaluate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     65 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     66 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 67 \u001b[0;31m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Testing score: {score}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     68 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     69 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing score: 2451.8422150346905\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mI have chosen to use RandomizedSearchCV for the hyper parameter search on the LGBMRegressor model. This is because RandomizedSearchCV is a good choice when the parameter space is large and it is not feasible to try all possible combinations. It also allows for a more efficient search by randomly sampling a subset of the parameter space.\n",
      "\n",
      "I have not chosen to use GridSearchCV because it would require trying all possible combinations of the parameters, which would be computationally expensive and time-consuming. \n",
      "\n",
      "I have not chosen to use HalvingGridSearchCV or HalvingRandomSearchCV because these algorithms are designed to be more efficient by eliminating poorly performing parameter combinations early on. However, since the LGBMRegressor model is relatively fast to train, it is not necessary to use these algorithms to save time. \n",
      "\n",
      "I have not chosen to use ParameterGrid or ParameterSampler because they are not as efficient as RandomizedSearchCV for large parameter spaces. They also do not allow for random sampling, which can be useful for exploring the parameter space more thoroughly.\n",
      "\n",
      "Final score after hyper parameter tuning is 2451.8422150346905\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The final score after hyper parameter tuning is 2451.8422150346905.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The final score after hyper parameter tuning is 2451.8422150346905.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_agent = ConversationBot()\n",
    "chat_agent.run_text(\"Can you run hyper parameter tuning ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f994f41e-e24d-40f8-ab3d-c06866458d26",
   "metadata": {},
   "source": [
    "# Demo 5: Can you run the ML process end to end ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "072992b4-2f89-45f3-91e1-9efcf917f565",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otto-group-product-classification-challenge\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Load\",\n",
      "    \"action_input\": \"otto-group-product-classification-challenge\"\n",
      "}\n",
      "```\u001b[0m(61878, 94) (144368, 94) (61878,) (144368,)\n",
      "\n",
      "Observation: \u001b[31;1m\u001b[1;3mLoad dataset otto-group-product-classification-challenge success !\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The otto-group-product-classification-challenge dataset has been successfully loaded.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Explore\",\n",
      "    \"action_input\": \"\"\n",
      "}\u001b[0motto-group-product-classification-challenge\n",
      "\n",
      "You are a data scientist. You are now performing data analysis before training a machine learning model.\n",
      "Your task is to tell me how the data could be preprocessed and to give me the corresponding Python code.\n",
      "The preprocessing includes, but is not limited to, missing value handling, scaling for numerical features, and categorical encoding.\n",
      "The task will be performed in two stages.\n",
      "In the first stage, you will decide which techniques are required to preprocess the dataset by investigating the dataset.\n",
      "In the second stage, you will preprocess the dataset by implementing the functions.\n",
      "\n",
      "Here is some information about the feature dataframe `X`:\n",
      "\"\"\"\n",
      "- First five rows:\n",
      "   id  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  feat_9  \\\n",
      "0   1       1       0       0       0       0       0       0       0       0   \n",
      "1   2       0       0       0       0       0       0       0       1       0   \n",
      "2   3       0       0       0       0       0       0       0       1       0   \n",
      "3   4       1       0       0       1       6       1       5       0       0   \n",
      "4   5       0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   ...  feat_84  feat_85  feat_86  feat_87  feat_88  feat_89  feat_90  \\\n",
      "0  ...        0        1        0        0        0        0        0   \n",
      "1  ...        0        0        0        0        0        0        0   \n",
      "2  ...        0        0        0        0        0        0        0   \n",
      "3  ...       22        0        1        2        0        0        0   \n",
      "4  ...        0        1        0        0        0        0        1   \n",
      "\n",
      "   feat_91  feat_92  feat_93  \n",
      "0        0        0        0  \n",
      "1        0        0        0  \n",
      "2        0        0        0  \n",
      "3        0        0        0  \n",
      "4        0        0        0  \n",
      "\n",
      "[5 rows x 94 columns]\n",
      "\n",
      "- Column types:\n",
      "id         int64\n",
      "feat_1     int64\n",
      "feat_2     int64\n",
      "feat_3     int64\n",
      "feat_4     int64\n",
      "           ...  \n",
      "feat_89    int64\n",
      "feat_90    int64\n",
      "feat_91    int64\n",
      "feat_92    int64\n",
      "feat_93    int64\n",
      "Length: 94, dtype: object\n",
      "\"\"\"\n",
      "\n",
      "Here is some information about the label series `y`:\n",
      "\"\"\"\n",
      "- First five rows:\n",
      "0    Class_1\n",
      "1    Class_1\n",
      "2    Class_1\n",
      "3    Class_1\n",
      "4    Class_1\n",
      "Name: target, dtype: object\n",
      "\n",
      "- Column types:\n",
      "object\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "\n",
      "Now, let's move on to the first stage:\n",
      "1. Explain how you will explore the dataset in order to select appropriate preprocessing techniques.\n",
      "   Analyze numerical and categorical features separately.\n",
      "2. Write a Python function called `explore` for data exploration.\n",
      "   Import any required packages WITHIN the `explore` function. That is, The first line of code should be the definition of the function, followed by the necessary package imports.\n",
      "   The function will be called using `result = explore(X, y)`, and all conclusions should be stored in a string variable `result`.\n",
      "   \n",
      "   Make the code and the returned string concise. Avoid listing specific data information such as row indices or column names.\n",
      "   Here are some GOOD examples of concise conclusions (not limited) to add to the `result`:\n",
      "   - \"There are no missing values in the dataset.\"\n",
      "   - \"There are {num_cat} categorical features. Consider applying a one-hot encoding\"\n",
      "   - \"The numerical features are highly skewed (mean skewness = {mean_skew:.2f}). Consider applying a log transformation or a power transformation.\"\n",
      "   - \"There are no outliers in the numerical features.\"\n",
      "   - \"The categorical features have high cardinality (mean cardinality = {mean_card:.2f}).\"\n",
      "   Here are some BAD examples of conclusions (not limited) to avoid adding to result as they specify data information and lack conciseness:\n",
      "   - \"Numerical feature: {X[numerical_features].describe().to_string()}\"\n",
      "   - \"Categorical feature: {X[categorical_features].nunique().to_string()}\"\n",
      "   - \"There are {num_outliers_per_feature} outliers in the feature {col_name}.\" (inside a for loop)\n",
      "\n",
      "   These are the only packages in the Python environment:\n",
      "   ```\n",
      "   numpy==1.24.3\n",
      "   pandas==2.0.1\n",
      "   ```\n",
      "   No additional packages can be used.\n",
      "\n",
      "1. To explore the dataset, I will first check for missing values in both numerical and categorical features. Then, I will analyze the distribution of numerical features to check for skewness and outliers. For categorical features, I will check the cardinality and consider applying one-hot encoding.\n",
      "\n",
      "2. Here's the implementation of the `explore` function:\n",
      "\n",
      "```python\n",
      "def explore(X, y):\n",
      "    import numpy as np\n",
      "    import pandas as pd\n",
      "    \n",
      "    # Check for missing values\n",
      "    if X.isnull().sum().sum() == 0:\n",
      "        result = \"There are no missing values in the dataset.\\n\"\n",
      "    else:\n",
      "        result = \"There are missing values in the dataset.\\n\"\n",
      "    \n",
      "    # Analyze numerical features\n",
      "    numerical_features = X.select_dtypes(include=np.number).columns\n",
      "    numerical_summary = X[numerical_features].describe()\n",
      "    mean_skew = (X[numerical_features].skew()).mean()\n",
      "    if mean_skew > 1:\n",
      "        result += f\"The numerical features are highly skewed (mean skewness = {mean_skew:.2f}). Consider applying a log transformation or a power transformation.\\n\"\n",
      "    else:\n",
      "        result += \"The numerical features are not highly skewed.\\n\"\n",
      "    if (X[numerical_features] < numerical_summary.loc['25%'] - 1.5*(numerical_summary.loc['75%']-numerical_summary.loc['25%'])).any().any() or (X[numerical_features] > numerical_summary.loc['75%'] + 1.5*(numerical_summary.loc['75%']-numerical_summary.loc['25%'])).any().any():\n",
      "        result += \"There are outliers in the numerical features.\\n\"\n",
      "    else:\n",
      "        result += \"There are no outliers in the numerical features.\\n\"\n",
      "    \n",
      "    # Analyze categorical features\n",
      "    categorical_features = X.select_dtypes(include='object').columns\n",
      "    mean_card = X[categorical_features].nunique().mean()\n",
      "    if mean_card > 10:\n",
      "        result += \"The categorical features have high cardinality.\\n\"\n",
      "    else:\n",
      "        result += f\"There are {len(categorical_features)} categorical features. Consider applying a one-hot encoding.\\n\"\n",
      "    \n",
      "    return result\n",
      "```\n",
      "\n",
      "Note that the function checks for missing values using the `isnull()` method of pandas dataframes, which returns a boolean dataframe with the same shape as the input dataframe, where `True` values indicate missing values. The `sum()` method is then called twice to count the number of missing values in the entire dataframe.\n",
      "\n",
      "The function then selects numerical features using the `select_dtypes()` method of pandas dataframes, which returns a dataframe with only the columns of the specified data type. Similarly, categorical features are selected using the same method with the `object` data type.\n",
      "\n",
      "The skewness of numerical features is calculated using the `skew()` method of pandas dataframes, which returns the skewness of each column. The mean skewness is then calculated using the `mean()` method of numpy arrays.\n",
      "\n",
      "Outliers are detected using the interquartile range (IQR) method, where any value outside the range `[Q1 - 1.5*IQR, Q3 + 1.5*IQR]` is considered an outlier. The `describe()` method of pandas dataframes is used to calculate the first and third quartiles (`Q1` and `Q3`) and the IQR, and the `any()` method is used twice to check if any value in the dataframe is outside the outlier range.\n",
      "\n",
      "Finally, the cardinality of categorical features is calculated using the `nunique()` method of pandas dataframes, which returns the number of unique values in each column. The mean cardinality is then calculated using the `mean()` method of numpy arrays. If the mean cardinality is greater than 10, the function suggests that the categorical features have high cardinality and may not be suitable for one-hot encoding. Otherwise, the function suggests applying one-hot encoding to the categorical features.\n",
      "\n",
      "def explore(X, y):\n",
      "    import numpy as np\n",
      "    import pandas as pd\n",
      "    \n",
      "    # Check for missing values\n",
      "    if X.isnull().sum().sum() == 0:\n",
      "        result = \"There are no missing values in the dataset.\\n\"\n",
      "    else:\n",
      "        result = \"There are missing values in the dataset.\\n\"\n",
      "    \n",
      "    # Analyze numerical features\n",
      "    numerical_features = X.select_dtypes(include=np.number).columns\n",
      "    numerical_summary = X[numerical_features].describe()\n",
      "    mean_skew = (X[numerical_features].skew()).mean()\n",
      "    if mean_skew > 1:\n",
      "        result += f\"The numerical features are highly skewed (mean skewness = {mean_skew:.2f}). Consider applying a log transformation or a power transformation.\\n\"\n",
      "    else:\n",
      "        result += \"The numerical features are not highly skewed.\\n\"\n",
      "    if (X[numerical_features] < numerical_summary.loc['25%'] - 1.5*(numerical_summary.loc['75%']-numerical_summary.loc['25%'])).any().any() or (X[numerical_features] > numerical_summary.loc['75%'] + 1.5*(numerical_summary.loc['75%']-numerical_summary.loc['25%'])).any().any():\n",
      "        result += \"There are outliers in the numerical features.\\n\"\n",
      "    else:\n",
      "        result += \"There are no outliers in the numerical features.\\n\"\n",
      "    \n",
      "    # Analyze categorical features\n",
      "    categorical_features = X.select_dtypes(include='object').columns\n",
      "    mean_card = X[categorical_features].nunique().mean()\n",
      "    if mean_card > 10:\n",
      "        result += \"The categorical features have high cardinality.\\n\"\n",
      "    else:\n",
      "        result += f\"There are {len(categorical_features)} categorical features. Consider applying a one-hot encoding.\\n\"\n",
      "    \n",
      "    return result\n",
      "\n",
      "\n",
      "_ = explore(X, y)\n",
      "\n",
      "There are no missing values in the dataset.\n",
      "The numerical features are highly skewed (mean skewness = 10.79). Consider applying a log transformation or a power transformation.\n",
      "There are outliers in the numerical features.\n",
      "There are 0 categorical features. Consider applying a one-hot encoding.\n",
      "\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mThere are no missing values in the dataset.\n",
      "The numerical features are highly skewed (mean skewness = 10.79). Consider applying a log transformation or a power transformation.\n",
      "There are outliers in the numerical features.\n",
      "There are 0 categorical features. Consider applying a one-hot encoding.\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The dataset has no missing values, but the numerical features are highly skewed and have outliers. There are no categorical features, so consider applying a one-hot encoding. \"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Preprocess\",\n",
      "    \"action_input\": \"\"\n",
      "}\u001b[0m\n",
      "\n",
      "You are a data scientist. You are now performing data analysis before training a machine learning model.\n",
      "Your task is to tell me how the data could be preprocessed and to give me the corresponding Python code.\n",
      "The preprocessing includes, but is not limited to, missing value handling, scaling for numerical features, and categorical encoding.\n",
      "The task will be performed in two stages.\n",
      "In the first stage, you will decide which techniques are required to preprocess the dataset by investigating the dataset.\n",
      "In the second stage, you will preprocess the dataset by implementing the functions.\n",
      "\n",
      "Here is some information about the feature dataframe `X`:\n",
      "\"\"\"\n",
      "- First five rows:\n",
      "   id  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  feat_9  \\\n",
      "0   1       1       0       0       0       0       0       0       0       0   \n",
      "1   2       0       0       0       0       0       0       0       1       0   \n",
      "2   3       0       0       0       0       0       0       0       1       0   \n",
      "3   4       1       0       0       1       6       1       5       0       0   \n",
      "4   5       0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   ...  feat_84  feat_85  feat_86  feat_87  feat_88  feat_89  feat_90  \\\n",
      "0  ...        0        1        0        0        0        0        0   \n",
      "1  ...        0        0        0        0        0        0        0   \n",
      "2  ...        0        0        0        0        0        0        0   \n",
      "3  ...       22        0        1        2        0        0        0   \n",
      "4  ...        0        1        0        0        0        0        1   \n",
      "\n",
      "   feat_91  feat_92  feat_93  \n",
      "0        0        0        0  \n",
      "1        0        0        0  \n",
      "2        0        0        0  \n",
      "3        0        0        0  \n",
      "4        0        0        0  \n",
      "\n",
      "[5 rows x 94 columns]\n",
      "\n",
      "- Column types:\n",
      "id         int64\n",
      "feat_1     int64\n",
      "feat_2     int64\n",
      "feat_3     int64\n",
      "feat_4     int64\n",
      "           ...  \n",
      "feat_89    int64\n",
      "feat_90    int64\n",
      "feat_91    int64\n",
      "feat_92    int64\n",
      "feat_93    int64\n",
      "Length: 94, dtype: object\n",
      "\"\"\"\n",
      "\n",
      "Here is some information about the label series `y`:\n",
      "\"\"\"\n",
      "- First five rows:\n",
      "0    Class_1\n",
      "1    Class_1\n",
      "2    Class_1\n",
      "3    Class_1\n",
      "4    Class_1\n",
      "Name: target, dtype: object\n",
      "\n",
      "- Column types:\n",
      "object\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "First-stage Thoughts:\n",
      "1. To explore the dataset, I will first check for missing values in both numerical and categorical features. Then, I will analyze the distribution of numerical features to check for skewness and outliers. For categorical features, I will check the cardinality and consider applying one-hot encoding.\n",
      "\n",
      "2. Here's the implementation of the `explore` function:\n",
      "\n",
      "```python\n",
      "def explore(X, y):\n",
      "    import numpy as np\n",
      "    import pandas as pd\n",
      "    \n",
      "    # Check for missing values\n",
      "    if X.isnull().sum().sum() == 0:\n",
      "        result = \"There are no missing values in the dataset.\\n\"\n",
      "    else:\n",
      "        result = \"There are missing values in the dataset.\\n\"\n",
      "    \n",
      "    # Analyze numerical features\n",
      "    numerical_features = X.select_dtypes(include=np.number).columns\n",
      "    numerical_summary = X[numerical_features].describe()\n",
      "    mean_skew = (X[numerical_features].skew()).mean()\n",
      "    if mean_skew > 1:\n",
      "        result += f\"The numerical features are highly skewed (mean skewness = {mean_skew:.2f}). Consider applying a log transformation or a power transformation.\\n\"\n",
      "    else:\n",
      "        result += \"The numerical features are not highly skewed.\\n\"\n",
      "    if (X[numerical_features] < numerical_summary.loc['25%'] - 1.5*(numerical_summary.loc['75%']-numerical_summary.loc['25%'])).any().any() or (X[numerical_features] > numerical_summary.loc['75%'] + 1.5*(numerical_summary.loc['75%']-numerical_summary.loc['25%'])).any().any():\n",
      "        result += \"There are outliers in the numerical features.\\n\"\n",
      "    else:\n",
      "        result += \"There are no outliers in the numerical features.\\n\"\n",
      "    \n",
      "    # Analyze categorical features\n",
      "    categorical_features = X.select_dtypes(include='object').columns\n",
      "    mean_card = X[categorical_features].nunique().mean()\n",
      "    if mean_card > 10:\n",
      "        result += \"The categorical features have high cardinality.\\n\"\n",
      "    else:\n",
      "        result += f\"There are {len(categorical_features)} categorical features. Consider applying a one-hot encoding.\\n\"\n",
      "    \n",
      "    return result\n",
      "```\n",
      "\n",
      "Note that the function checks for missing values using the `isnull()` method of pandas dataframes, which returns a boolean dataframe with the same shape as the input dataframe, where `True` values indicate missing values. The `sum()` method is then called twice to count the number of missing values in the entire dataframe.\n",
      "\n",
      "The function then selects numerical features using the `select_dtypes()` method of pandas dataframes, which returns a dataframe with only the columns of the specified data type. Similarly, categorical features are selected using the same method with the `object` data type.\n",
      "\n",
      "The skewness of numerical features is calculated using the `skew()` method of pandas dataframes, which returns the skewness of each column. The mean skewness is then calculated using the `mean()` method of numpy arrays.\n",
      "\n",
      "Outliers are detected using the interquartile range (IQR) method, where any value outside the range `[Q1 - 1.5*IQR, Q3 + 1.5*IQR]` is considered an outlier. The `describe()` method of pandas dataframes is used to calculate the first and third quartiles (`Q1` and `Q3`) and the IQR, and the `any()` method is used twice to check if any value in the dataframe is outside the outlier range.\n",
      "\n",
      "Finally, the cardinality of categorical features is calculated using the `nunique()` method of pandas dataframes, which returns the number of unique values in each column. The mean cardinality is then calculated using the `mean()` method of numpy arrays. If the mean cardinality is greater than 10, the function suggests that the categorical features have high cardinality and may not be suitable for one-hot encoding. Otherwise, the function suggests applying one-hot encoding to the categorical features.\n",
      "\n",
      "First-stage Conclusion:\n",
      "There are no missing values in the dataset.\n",
      "The numerical features are highly skewed (mean skewness = 10.79). Consider applying a log transformation or a power transformation.\n",
      "There are outliers in the numerical features.\n",
      "There are 0 categorical features. Consider applying a one-hot encoding.\n",
      "\n",
      "\n",
      "\n",
      "The exploration results from the first stage are shown above.\n",
      "Now, let's move on to the second stage:\n",
      "1. Based on the conclusion, specify the preprocessing techniques to be employed for the dataset and briefly describe their implementations.\n",
      "2. Write a Python function called `preprocess` that will assist you in preprocessing the data.\n",
      "   Import any required packages WITHIN the `preprocess` function. That is, The first line of code should be the definition of the function, followed by the necessary package imports.\n",
      "   The function will be called using `proc_X_train, proc_y_train, proc_X_test, proc_y_test = preprocess(X_train, y_train, X_test, y_test)`, \n",
      "   where both the parameters and return values are eithor Pandas dataframes or series.\n",
      "   Only consider common preprocessing methods and be aware of potential data leakage issues.\n",
      "   Make the code concise. Avoid listing specific data information such as row indices or column names.\n",
      "   Make the function as simple as possible.\n",
      "   Make the function as short as possible.\n",
      "   \n",
      "\n",
      "   These are the only packages in the Python environment:\n",
      "   ```\n",
      "   numpy==1.24.3\n",
      "   pandas==2.0.1\n",
      "   scikit-learn==1.2.2\n",
      "   scipy==1.10.1\n",
      "   category_encoders==2.6.1\n",
      "   ```\n",
      "   No additional packages can be used.\n",
      "\n",
      "Second-stage Thoughts:\n",
      "1. To preprocess the dataset, I will first apply a log transformation to the numerical features to reduce skewness. Then, I will apply a standard scaler to the numerical features to scale them to a similar range. For categorical features, I will apply one-hot encoding using the `OneHotEncoder` class from scikit-learn. I will also drop the `id` column from the dataset as it is not useful for training the machine learning model.\n",
      "\n",
      "2. Here's the implementation of the `preprocess` function:\n",
      "\n",
      "```python\n",
      "def preprocess(X_train, y_train, X_test, y_test):\n",
      "    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
      "    from sklearn.compose import ColumnTransformer\n",
      "    \n",
      "    # Drop the id column\n",
      "    X_train = X_train.drop('id', axis=1)\n",
      "    X_test = X_test.drop('id', axis=1)\n",
      "    \n",
      "    # Apply log transformation to numerical features\n",
      "    numerical_features = X_train.select_dtypes(include='number').columns\n",
      "    X_train[numerical_features] = np.log1p(X_train[numerical_features])\n",
      "    X_test[numerical_features] = np.log1p(X_test[numerical_features])\n",
      "    \n",
      "    # Apply standard scaling to numerical features\n",
      "    scaler = StandardScaler()\n",
      "    X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
      "    X_test[numerical_features] = scaler.transform(X_test[numerical_features])\n",
      "    \n",
      "    # Apply one-hot encoding to categorical features\n",
      "    categorical_features = X_train.select_dtypes(include='object').columns\n",
      "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
      "    transformer = ColumnTransformer(transformers=[('cat', encoder, categorical_features)], remainder='passthrough')\n",
      "    X_train = transformer.fit_transform(X_train)\n",
      "    X_test = transformer.transform(X_test)\n",
      "    \n",
      "    return X_train, y_train, X_test, y_test\n",
      "```\n",
      "\n",
      "Note that the function imports the necessary packages within the function definition, as required. The `StandardScaler` and `OneHotEncoder` classes are imported from scikit-learn, and the `ColumnTransformer` class is imported from the `sklearn.compose` module.\n",
      "\n",
      "The function first drops the `id` column from both the training and test datasets using the `drop()` method of pandas dataframes.\n",
      "\n",
      "The log transformation is applied to numerical features using the `log1p()` function of numpy arrays, which calculates the natural logarithm of the input array plus 1. This is done to avoid taking the logarithm of zero or negative values.\n",
      "\n",
      "The standard scaling is applied to numerical features using the `fit_transform()` method of the `StandardScaler` class, which fits the scaler to the training data and transforms both the training and test data using the same scaler.\n",
      "\n",
      "The one-hot encoding is applied to categorical features using the `fit_transform()` method of the `ColumnTransformer` class, which applies the `OneHotEncoder` to the categorical features and leaves the numerical features unchanged. The `handle_unknown='ignore'` parameter is used to ignore unknown categories in the test data, which may not be present in the training data.\n",
      "\n",
      "Finally, the preprocessed datasets are returned as a tuple.\n",
      "\n",
      "Second-stage Conclusion:\n",
      "The `preprocess` function applies a log transformation and standard scaling to the numerical features, and one-hot encoding to the categorical features. The `id` column is dropped from the dataset. The function is concise and avoids potential data leakage issues.\n",
      "\n",
      "def preprocess(X_train, y_train, X_test, y_test):\n",
      "    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
      "    from sklearn.compose import ColumnTransformer\n",
      "    \n",
      "    # Drop the id column\n",
      "    X_train = X_train.drop('id', axis=1)\n",
      "    X_test = X_test.drop('id', axis=1)\n",
      "    \n",
      "    # Apply log transformation to numerical features\n",
      "    numerical_features = X_train.select_dtypes(include='number').columns\n",
      "    X_train[numerical_features] = np.log1p(X_train[numerical_features])\n",
      "    X_test[numerical_features] = np.log1p(X_test[numerical_features])\n",
      "    \n",
      "    # Apply standard scaling to numerical features\n",
      "    scaler = StandardScaler()\n",
      "    X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
      "    X_test[numerical_features] = scaler.transform(X_test[numerical_features])\n",
      "    \n",
      "    # Apply one-hot encoding to categorical features\n",
      "    categorical_features = X_train.select_dtypes(include='object').columns\n",
      "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
      "    transformer = ColumnTransformer(transformers=[('cat', encoder, categorical_features)], remainder='passthrough')\n",
      "    X_train = transformer.fit_transform(X_train)\n",
      "    X_test = transformer.transform(X_test)\n",
      "    \n",
      "    return X_train, y_train, X_test, y_test\n",
      "\n",
      "\n",
      "_, _, _, _ = preprocess(X_train, y_train, X_test, y_test)\n",
      "\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mSecond-stage Thoughts:\n",
      "1. To preprocess the dataset, I will first apply a log transformation to the numerical features to reduce skewness. Then, I will apply a standard scaler to the numerical features to scale them to a similar range. For categorical features, I will apply one-hot encoding using the `OneHotEncoder` class from scikit-learn. I will also drop the `id` column from the dataset as it is not useful for training the machine learning model.\n",
      "\n",
      "2. Here's the implementation of the `preprocess` function:\n",
      "\n",
      "```python\n",
      "def preprocess(X_train, y_train, X_test, y_test):\n",
      "    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
      "    from sklearn.compose import ColumnTransformer\n",
      "    \n",
      "    # Drop the id column\n",
      "    X_train = X_train.drop('id', axis=1)\n",
      "    X_test = X_test.drop('id', axis=1)\n",
      "    \n",
      "    # Apply log transformation to numerical features\n",
      "    numerical_features = X_train.select_dtypes(include='number').columns\n",
      "    X_train[numerical_features] = np.log1p(X_train[numerical_features])\n",
      "    X_test[numerical_features] = np.log1p(X_test[numerical_features])\n",
      "    \n",
      "    # Apply standard scaling to numerical features\n",
      "    scaler = StandardScaler()\n",
      "    X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
      "    X_test[numerical_features] = scaler.transform(X_test[numerical_features])\n",
      "    \n",
      "    # Apply one-hot encoding to categorical features\n",
      "    categorical_features = X_train.select_dtypes(include='object').columns\n",
      "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
      "    transformer = ColumnTransformer(transformers=[('cat', encoder, categorical_features)], remainder='passthrough')\n",
      "    X_train = transformer.fit_transform(X_train)\n",
      "    X_test = transformer.transform(X_test)\n",
      "    \n",
      "    return X_train, y_train, X_test, y_test\n",
      "```\n",
      "\n",
      "Note that the function imports the necessary packages within the function definition, as required. The `StandardScaler` and `OneHotEncoder` classes are imported from scikit-learn, and the `ColumnTransformer` class is imported from the `sklearn.compose` module.\n",
      "\n",
      "The function first drops the `id` column from both the training and test datasets using the `drop()` method of pandas dataframes.\n",
      "\n",
      "The log transformation is applied to numerical features using the `log1p()` function of numpy arrays, which calculates the natural logarithm of the input array plus 1. This is done to avoid taking the logarithm of zero or negative values.\n",
      "\n",
      "The standard scaling is applied to numerical features using the `fit_transform()` method of the `StandardScaler` class, which fits the scaler to the training data and transforms both the training and test data using the same scaler.\n",
      "\n",
      "The one-hot encoding is applied to categorical features using the `fit_transform()` method of the `ColumnTransformer` class, which applies the `OneHotEncoder` to the categorical features and leaves the numerical features unchanged. The `handle_unknown='ignore'` parameter is used to ignore unknown categories in the test data, which may not be present in the training data.\n",
      "\n",
      "Finally, the preprocessed datasets are returned as a tuple.\n",
      "\n",
      "Second-stage Conclusion:\n",
      "The `preprocess` function applies a log transformation and standard scaling to the numerical features, and one-hot encoding to the categorical features. The `id` column is dropped from the dataset. The function is concise and avoids potential data leakage issues.\n",
      "\n",
      "Processing....\n",
      "\n",
      "Process Completed !\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The response to your last comment is that the `preprocess` function applies a log transformation and standard scaling to the numerical features, and one-hot encoding to the categorical features. The `id` column is dropped from the dataset. The function is concise and avoids potential data leakage issues.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Pick Model\",\n",
      "    \"action_input\": \"\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"hyperopt\",\n",
      "    \"action_input\": \"\"\n",
      "}\u001b[0m\n",
      "Construct estimator with following:\n",
      "```\n",
      "import lightgbm as lgb\n",
      "\n",
      "clf = lgb.LGBMRegressor()\n",
      "```\n",
      "\n",
      "\n",
      "Construct params grid:\n",
      "{'learning_rate': [0.01, 0.05, 0.1, 0.2], 'n_estimators': [50, 100, 200, 500], 'max_depth': [3, 5, 7, 9], 'num_leaves': [15, 31, 50, 75], 'min_child_samples': [10, 20, 30, 50], 'reg_alpha': [0.0, 0.1, 0.5, 1.0], 'reg_lambda': [0.0, 0.1, 0.5, 1.0], 'colsample_bytree': [0.5, 0.7, 1.0], 'subsample': [0.5, 0.7, 1.0]}\n",
      "\n",
      "I would choose RandomizedSearchCV for this hyper parameter search on the LGBMRegressor model. This is because the parameter space is quite large with many possible combinations, and RandomizedSearchCV allows for a more efficient search by randomly sampling a subset of the parameter space. Additionally, the LGBMRegressor model is known to work well with randomized search algorithms.\n",
      "\n",
      "I would not choose GridSearchCV or ParameterGrid because they would require an exhaustive search over all possible combinations of parameters, which would be computationally expensive and time-consuming given the large parameter space. HalvingGridSearchCV and HalvingRandomSearchCV could be more efficient, but they may not be as effective at finding the optimal hyper parameters as RandomizedSearchCV. ParameterSampler is not as suitable for this problem because it generates parameters based on given distributions, which may not be appropriate for all hyper parameters in this case.\n",
      "\n",
      "Deciding search algo:\n",
      "RandomizedSearchCV\n",
      "\n",
      "\n",
      "Constructing search algo:\n",
      "```\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "hyper_algo = RandomizedSearchCV(clf, param_grid, random_state=0, n_iter=50, n_jobs=24)\n",
      "```\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2991522/1988691156.py\", line 50, in <cell line: 5>\n",
      "    clf.fit(status.X_train, status.y_train)\n",
      "  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/sklearn.py\", line 895, in fit\n",
      "    super().fit(X, y, sample_weight=sample_weight, init_score=init_score,\n",
      "  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/sklearn.py\", line 748, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/engine.py\", line 271, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/basic.py\", line 2605, in __init__\n",
      "    train_set.construct()\n",
      "  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/basic.py\", line 1815, in construct\n",
      "    self._lazy_init(self.data, label=self.label,\n",
      "  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/basic.py\", line 1557, in _lazy_init\n",
      "    self.set_label(label)\n",
      "  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/basic.py\", line 2163, in set_label\n",
      "    label = list_to_1d_numpy(_label_from_pandas(label), name='label')\n",
      "  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/basic.py\", line 177, in list_to_1d_numpy\n",
      "    return cast_numpy_array_to_dtype(data, dtype)\n",
      "  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/basic.py\", line 156, in cast_numpy_array_to_dtype\n",
      "    return array.astype(dtype=dtype, copy=False)\n",
      "ValueError: could not convert string to float: 'Class_1'\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import subprocess\n",
    "\n",
    "# for dataset, dataset_meta in data_loader.datasets.items():\n",
    "for dataset, dataset_meta in [(\"otto-group-product-classification-challenge\", data_loader.select(\"otto-group-product-classification-challenge\"))]:\n",
    "    \n",
    "    try:\n",
    "        print(dataset)\n",
    "        root = f\"kaggle/{dataset}/\"\n",
    "        \n",
    "        # if os.path.exists(os.path.join(root, 'submission.csv')):\n",
    "        #     continue\n",
    "\n",
    "        status.init()\n",
    "\n",
    "        chat_agent = ConversationBot()\n",
    "        chat_agent.run_text(f\"Can you load the {dataset} dataset ?\")\n",
    "\n",
    "        chat_agent = ConversationBot()\n",
    "        chat_agent.run_text(\"Can you explore the dataset ?\")\n",
    "        \n",
    "        try:\n",
    "            chat_agent = ConversationBot()\n",
    "            chat_agent.run_text(\"Can you process the dataset ?\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            chat_agent = ConversationBot()\n",
    "            chat_agent.run_text(\"Can you select the model ?\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            chat_agent = ConversationBot()\n",
    "            chat_agent.run_text(\"Can you run hyper parameter tuning ?\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        with open(os.path.join(root, \"status.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(status, f)\n",
    "            \n",
    "        from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "        best_param = status.hyper_result.best_params_\n",
    "        if status.model == \"LGBMRegressor\":\n",
    "            clf = LGBMRegressor(**best_param)\n",
    "        else:\n",
    "            clf = LGBMClassifier(**best_param)\n",
    "        clf.fit(status.X_train, status.y_train)\n",
    "        y_pred = clf.predict(status.X_test)\n",
    "            \n",
    "        df = pd.read_csv(os.path.join(root, \"sample_submission.csv\"))\n",
    "        label = dataset_meta['label']\n",
    "        df[label] = y_pred\n",
    "        df.to_csv(os.path.join(root, \"submission.csv\"), index=False)\n",
    "        \n",
    "        subprocess.call([\"kaggle\", \"competitions\", \"submit\", \"-c\", dataset, \"-f\", os.path.join(root, \"submission.csv\"), \"-m\", \"Message\"])\n",
    "        \n",
    "    except:\n",
    "        logging.exception(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "c34b590f-1dbe-4455-9a60-5b23630f7e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kaggle/santander-value-prediction-challenge/sttaus.pkl', \"rb\") as f:\n",
    "    ss = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "f895d51b-a48b-4580-910e-a0b0e04ed9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "2c78f071-5b29-46dc-bbfc-4c53b58656b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dataset', 'X', 'y', 'X_train', 'X_test', 'y_train', 'y_test', 'y_pred', 'start_explore_prompt', 'how_to_explore_prompt', 'explore_data_thought', 'hyperopt_param_thought', 'explore_result', 'explore', 'preprocess', 'model_select', 'hyperopt', 'explore_code', 'preprocess_code', 'model_select_code', 'hyperopt_code', 'model', 'best_param', 'dev', 'know_how_prompt', 'how_examine_dataset_thought', 'examine_dataset_thought', 'examine_dataset_code', 'model_selection_thought', 'model_selection_code', 'model_selection', 'explore_data_result', 'preprocess_data_thought', 'hyper_result'])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(ss).keys()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb7e6e07-2757-49f3-a9a5-fed0c4feb784",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def sss(estimator, X, y):\n",
    "    y_pred = estimator.predict_proba(X)[:, 1]\n",
    "    import pdb\n",
    "    pdb.set_trace()\n",
    "    return roc_auc_score(y, y_pred)\n",
    "\n",
    "\n",
    "clf = LGBMClassifier(**search.best_params_, n_jobs=24, objective='binary')\n",
    "clf.n_estimators = 300\n",
    "print(cross_val_score(clf, ss.X_train.drop(\"ID_code\", axis=1), ss.y_train, cv=5, scoring=sss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "ed04361d-2323-4a60-ba51-18b0bd1117df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = data_loader.select(ss.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "4b59a60f-4d91-42e9-80d4-5e1b661313a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def preprocess(X_train, y_train, X_test, y_test):\n",
      "    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
      "    from sklearn.compose import ColumnTransformer\n",
      "    \n",
      "    # Drop the id column\n",
      "    X_train = X_train.drop('id', axis=1)\n",
      "    X_test = X_test.drop('id', axis=1)\n",
      "    \n",
      "    # Apply log transformation to numerical features\n",
      "    numerical_features = X_train.select_dtypes(include='number').columns\n",
      "    X_train[numerical_features] = np.log1p(X_train[numerical_features])\n",
      "    X_test[numerical_features] = np.log1p(X_test[numerical_features])\n",
      "    \n",
      "    # Apply standard scaling to numerical features\n",
      "    scaler = StandardScaler()\n",
      "    X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
      "    X_test[numerical_features] = scaler.transform(X_test[numerical_features])\n",
      "    \n",
      "    # Apply one-hot encoding to categorical features\n",
      "    categorical_features = X_train.select_dtypes(include='object').columns\n",
      "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
      "    transformer = ColumnTransformer(transformers=[('cat', encoder, categorical_features)], remainder='passthrough')\n",
      "    X_train = transformer.fit_transform(X_train)\n",
      "    X_test = transformer.transform(X_test)\n",
      "    \n",
      "    return X_train, y_train, X_test, y_test\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ss.preprocess_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "e2dfb788-24c2-4452-9bc7-f8529ecdc2b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numerical_features = tmp['train'].drop('target', axis=1).select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = tmp['train'].drop('target', axis=1).select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "def preprocess(X_train, y_train, X_test, y_test):\n",
    "    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    \n",
    "    # Drop the id column\n",
    "    X_train = X_train.drop('id', axis=1)\n",
    "    X_test = X_test.drop('id', axis=1)\n",
    "    \n",
    "    # Apply log transformation to numerical features\n",
    "    numerical_features = X_train.select_dtypes(include='number').columns\n",
    "    X_train[numerical_features] = np.log1p(X_train[numerical_features])\n",
    "    X_test[numerical_features] = np.log1p(X_test[numerical_features])\n",
    "    \n",
    "    # Apply standard scaling to numerical features\n",
    "    scaler = StandardScaler()\n",
    "    X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
    "    X_test[numerical_features] = scaler.transform(X_test[numerical_features])\n",
    "    \n",
    "    # Apply one-hot encoding to categorical features\n",
    "    categorical_features = X_train.select_dtypes(include='object').columns\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "    transformer = ColumnTransformer(transformers=[('cat', encoder, categorical_features)], remainder='passthrough')\n",
    "    X_train = transformer.fit_transform(X_train)\n",
    "    X_test = transformer.transform(X_test)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "XX_train, yy_train, XX_test, yy_test = preprocess(\n",
    "    tmp['train'].drop('target', axis=1),\n",
    "    tmp['train']['target'],\n",
    "    tmp['test'],\n",
    "    tmp['train']['target'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "19b32a76-0b20-428f-87f5-e9d0a5af1a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = tmp['label']\n",
    "XX_train, yy_train, XX_test, yy_test = tmp['train'].drop(label, axis=1), tmp['train'][label], tmp['test'], tmp['train'][label]\n",
    "\n",
    "XX_train = XX_train.drop(categorical_features, axis=1)\n",
    "XX_test = XX_test.drop(categorical_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "6603fe08-9a26-49a4-b815-d6af32f72a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LGBMRegressor'"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "1bd7eee9-cd3d-4741-99e3-db4388062eeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.15957123e-04, 1.68350080e-01, 2.50881074e-01, ...,\n",
       "        1.47757859e-02, 1.74098274e-03, 4.04469186e-04],\n",
       "       [4.14503616e-03, 2.60322438e-02, 3.52852913e-03, ...,\n",
       "        1.13369802e-03, 2.12701965e-01, 3.55165387e-03],\n",
       "       [3.87158222e-05, 1.09200546e-04, 4.30082221e-05, ...,\n",
       "        8.06367867e-05, 1.11943770e-03, 1.98438703e-05],\n",
       "       ...,\n",
       "       [2.00692556e-03, 4.58627797e-01, 3.30580166e-01, ...,\n",
       "        1.69555895e-03, 8.39739922e-04, 3.06255080e-04],\n",
       "       [1.95217702e-04, 2.39660010e-01, 2.03668363e-02, ...,\n",
       "        4.55862051e-04, 1.62443239e-04, 7.01016561e-05],\n",
       "       [4.00434892e-04, 4.70130447e-01, 4.14784216e-01, ...,\n",
       "        5.57863915e-02, 6.41693763e-04, 2.53831368e-04]])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# XX_train, XX_test, yy_train, yy_test = train_test_split(ss.X_train.drop(\"ID_code\", axis=1), ss.y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "clf = LGBMClassifier(**ss.best_param, n_jobs=24)\n",
    "clf.fit(XX_train, yy_train)\n",
    "y_pred = clf.predict_proba(XX_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "ac29652f-5a96-4764-b669-9529ef1df41a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Class_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [284]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# import numpy as np\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# from sklearn.model_selection import train_test_split\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# XX_train, XX_test, yy_train, yy_test = train_test_split(ss.X_train.drop(\"ID_code\", axis=1), ss.y_train, test_size=0.1, random_state=42)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m clf \u001b[38;5;241m=\u001b[39m LGBMRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mss\u001b[38;5;241m.\u001b[39mbest_param, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myy_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(XX_test)\n\u001b[1;32m      8\u001b[0m y_pred\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/sklearn.py:895\u001b[0m, in \u001b[0;36mLGBMRegressor.fit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y,\n\u001b[1;32m    889\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, init_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    890\u001b[0m         eval_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    891\u001b[0m         eval_init_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    892\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m'\u001b[39m, feature_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, categorical_feature\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    893\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, init_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;124;03m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 895\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m                \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m                \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m                \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/sklearn.py:748\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    745\u001b[0m evals_result \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    746\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[0;32m--> 748\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evals_result:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evals_result \u001b[38;5;241m=\u001b[39m evals_result\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/engine.py:271\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# construct booster\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m     booster \u001b[38;5;241m=\u001b[39m \u001b[43mBooster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[1;32m    273\u001b[0m         booster\u001b[38;5;241m.\u001b[39mset_train_data_name(train_data_name)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/basic.py:2605\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[0;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[1;32m   2598\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_network(\n\u001b[1;32m   2599\u001b[0m         machines\u001b[38;5;241m=\u001b[39mmachines,\n\u001b[1;32m   2600\u001b[0m         local_listen_port\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_listen_port\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   2601\u001b[0m         listen_time_out\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_out\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m120\u001b[39m),\n\u001b[1;32m   2602\u001b[0m         num_machines\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_machines\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   2603\u001b[0m     )\n\u001b[1;32m   2604\u001b[0m \u001b[38;5;66;03m# construct booster object\u001b[39;00m\n\u001b[0;32m-> 2605\u001b[0m \u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2606\u001b[0m \u001b[38;5;66;03m# copy the parameters from train_set\u001b[39;00m\n\u001b[1;32m   2607\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(train_set\u001b[38;5;241m.\u001b[39mget_params())\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/basic.py:1815\u001b[0m, in \u001b[0;36mDataset.construct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1812\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_init_score_by_predictor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predictor, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, used_indices)\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1814\u001b[0m     \u001b[38;5;66;03m# create train\u001b[39;00m\n\u001b[0;32m-> 1815\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1816\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1817\u001b[0m \u001b[43m                    \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1818\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1819\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfree_raw_data:\n\u001b[1;32m   1821\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/basic.py:1557\u001b[0m, in \u001b[0;36mDataset._lazy_init\u001b[0;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[1;32m   1555\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot initialize Dataset from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1557\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_label() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1559\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel should not be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/basic.py:2163\u001b[0m, in \u001b[0;36mDataset.set_label\u001b[0;34m(self, label)\u001b[0m\n\u001b[1;32m   2161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel \u001b[38;5;241m=\u001b[39m label\n\u001b[1;32m   2162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2163\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[43mlist_to_1d_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_label_from_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_field(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, label)\n\u001b[1;32m   2165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_field(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# original values can be modified at cpp side\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/basic.py:177\u001b[0m, in \u001b[0;36mlist_to_1d_numpy\u001b[0;34m(data, dtype, name)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m\"\"\"Convert data to numpy 1-D array.\"\"\"\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_numpy_1d_array(data):\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcast_numpy_array_to_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_numpy_column_array(data):\n\u001b[1;32m    179\u001b[0m     _log_warning(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConverting column-vector to 1d array\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/basic.py:156\u001b[0m, in \u001b[0;36mcast_numpy_array_to_dtype\u001b[0;34m(array, dtype)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m dtype:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m array\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Class_1'"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# XX_train, XX_test, yy_train, yy_test = train_test_split(ss.X_train.drop(\"ID_code\", axis=1), ss.y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "clf = LGBMRegressor(**ss.best_param, n_jobs=24)\n",
    "clf.fit(XX_train, yy_train)\n",
    "y_pred = clf.predict(XX_test)\n",
    "y_pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "0c83699f-e44c-4d8a-beab-334023fd3f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5', 'Class_6',\n",
       "       'Class_7', 'Class_8', 'Class_9'], dtype=object)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "c8cd133b-6d6d-48b0-a1e4-db66f7b128f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/jason-tsai/.kaggle/kaggle.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 27.2M/27.2M [00:05<00:00, 4.78MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted to Otto Group Product Classification Challenge"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = f\"kaggle/{ss.dataset}/\"\n",
    "df = pd.read_csv(os.path.join(root, \"sampleSubmission.csv\"))\n",
    "# label = tmp['label']\n",
    "df[clf.classes_] = y_pred\n",
    "df.to_csv(os.path.join(root, \"submission.csv\"), index=False)\n",
    "\n",
    "import subprocess\n",
    "subprocess.call([\"kaggle\", \"competitions\", \"submit\", \"-c\", ss.dataset, \"-f\", os.path.join(root, \"submission.csv\"), \"-m\", \"Message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3dcfe30b-cf9e-4205-8738-a1a610e59720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/jason-tsai/.kaggle/kaggle.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.56M/2.56M [00:04<00:00, 564kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted to Santander Value Prediction Challenge"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(root, \"submission.csv\"))\n",
    "df['target'] = df['target'].apply(lambda x: 0 if x < 0 else x)\n",
    "df.to_csv(os.path.join(root, \"submission.csv\"), index=False)\n",
    "\n",
    "subprocess.call([\"kaggle\", \"competitions\", \"submit\", \"-c\", dataset, \"-f\", os.path.join(root, \"submission.csv\"), \"-m\", \"Message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8a52a55f-bb7b-44c2-b301-e829ce406432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'santander-customer-transaction-prediction'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8f98e588-29a5-49e6-b253-fcf55c2f5472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoMLGPT\n",
      "WineQT.csv\n",
      "archive.zip\n",
      "automl-tools.ipynb\n",
      "automl-tools3.ipynb\n",
      "flagged\n",
      "grid_search.ipynb\n",
      "kaggle\n",
      "langchain-agent-kaggle-Copy1.ipynb\n",
      "langchain-agent-kaggle.ipynb\n",
      "langchain-agent-kaggle3.ipynb\n",
      "langchain-agent.ipynb\n",
      "prompt\n",
      "submission.csv\n",
      "tabu.ipynb\n",
      "test.py\n",
      "utils.py\n",
      "venv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "subprocess.call([\"ls\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928ca0ca-05de-4074-86fa-8136d2814ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
