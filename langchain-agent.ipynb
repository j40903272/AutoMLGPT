{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b632fbe-16ae-4812-a408-9b375a577623",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, Union, List, Dict, Tuple\n",
    "\n",
    "# import torch\n",
    "import asyncio\n",
    "import logging\n",
    "from functools import wraps, partial\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed_all(seed)\n",
    "    return seed\n",
    "\n",
    "\n",
    "def prompts(name: str, description: str):\n",
    "    def decorator(func):\n",
    "        func.name = name\n",
    "        func.description = description\n",
    "        return func\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def extract_code(text):\n",
    "    texts = text.split(\"```\")\n",
    "    code = None\n",
    "    if len(texts) > 1:\n",
    "        code = texts[1]\n",
    "        code = code.strip(\"python\").strip(\"Python\")\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "254de8d7-b2a4-4e4a-8ea0-ac1f804d349b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Union, List, Dict, Tuple\n",
    "from pydantic import Field\n",
    "import ast\n",
    "import astunparse\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def prompts(name: str, description: str):\n",
    "    def decorator(func):\n",
    "        func.name = name\n",
    "        func.description = description\n",
    "        return func\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def execute(cmd: str, glo, loc):\n",
    "    tree = ast.parse(cmd)\n",
    "    module = ast.Module(tree.body[:-1], type_ignores=[])\n",
    "    exec(astunparse.unparse(module), self.globals, self.locals)  # type: ignore\n",
    "    module_end = ast.Module(tree.body[-1:], type_ignores=[])\n",
    "    module_end_str = astunparse.unparse(module_end)  # type: ignore\n",
    "    try:\n",
    "        return eval(module_end_str, self.globals, self.locals)\n",
    "    except Exception:\n",
    "        exec(module_end_str, self.globals, self.locals)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f8b8477-5a71-4722-9a68-88080362025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fa12ef6-f654-4fab-9e45-877bb83df79a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import logging\n",
    "import threading\n",
    "from typing import Union\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from langchain.chains.conversation.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    # ConversationBufferWindowMemory,\n",
    "    # ConversationSummaryBufferMemory\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents.tools import Tool\n",
    "from langchain.agents.conversational.output_parser import ConvoOutputParser\n",
    "from langchain.agents.conversational.prompt import FORMAT_INSTRUCTIONS\n",
    "from langchain.agents.conversational_chat.prompt import (\n",
    "    FORMAT_INSTRUCTIONS as CHAT_FORMAT_INSTRUCTIONS,\n",
    ")\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "from langchain.agents import AgentOutputParser\n",
    "\n",
    "# from api.services.utils import prompts, seed_everything, to_async\n",
    "# from api.conversation.callback import AsyncLoggerCallbackHandler, LoggerCallbackHandler\n",
    "\n",
    "\n",
    "\n",
    "class retryActionTool:\n",
    "    \"\"\"Retry when chatgpt response parsing failed.\"\"\"\n",
    "\n",
    "    @prompts(\n",
    "        name=\"RR\",\n",
    "        description=\"Do not use it.\",\n",
    "    )\n",
    "    def inference(self, query):\n",
    "        return f\"Modify the following text in ``` such that it will follow RESPONSE FORMAT INSTRUCTIONS:```{query}```\\nDo not use retry tool again.\"\n",
    "\n",
    "\n",
    "class RetryConvoOutputParser(ConvoOutputParser):\n",
    "    ai_prefix: str = \"AI\"\n",
    "\n",
    "    def get_format_instructions(self) -> str:\n",
    "        return FORMAT_INSTRUCTIONS\n",
    "\n",
    "    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\n",
    "        if f\"{self.ai_prefix}:\" in text:\n",
    "            return AgentFinish(\n",
    "                {\"output\": text.split(f\"{self.ai_prefix}:\")[-1].strip()}, text\n",
    "            )\n",
    "        regex = r\"Action: (.*?)[\\n]*Action Input: (.*)\"\n",
    "        match = re.search(regex, text)\n",
    "        if not match:\n",
    "            return AgentAction(\"retry\", \"\", text)\n",
    "            # raise OutputParserException(f\"Could not parse LLM output: `{text}`\")\n",
    "        action = match.group(1)\n",
    "        action_input = match.group(2)\n",
    "        return AgentAction(action.strip(), action_input.strip(\" \").strip('\"'), text)\n",
    "\n",
    "\n",
    "class RobustRetryConvoOutputParser(AgentOutputParser):\n",
    "    def get_format_instructions(self) -> str:\n",
    "        return CHAT_FORMAT_INSTRUCTIONS\n",
    "\n",
    "    def parse_json(self, text: str):\n",
    "        try:\n",
    "            return [\n",
    "                i\n",
    "                for i in re.findall(r\"\\s([{\\[].*?[}\\]])$\", f\" {text}\", flags=re.DOTALL)\n",
    "                if i\n",
    "            ][0]\n",
    "        except Exception:\n",
    "            return\n",
    "\n",
    "    def parse_markdown_code(self, text: str):\n",
    "        try:\n",
    "            for i in re.findall(\n",
    "                r\"`{3}([\\w]*)\\n([\\S\\s]+?)\\n`{3}\", text, flags=re.DOTALL\n",
    "            )[0]:\n",
    "                if i:\n",
    "                    tmp = self.parse_json(i)\n",
    "                    if tmp:\n",
    "                        return tmp\n",
    "        except Exception:\n",
    "            return\n",
    "\n",
    "    def parse_origin(self, text: str):\n",
    "        try:\n",
    "            cleaned_output = text.strip()\n",
    "            if \"```json\" in cleaned_output:\n",
    "                _, cleaned_output = cleaned_output.split(\"```json\")\n",
    "            if \"```\" in cleaned_output:\n",
    "                cleaned_output, _ = cleaned_output.split(\"```\")\n",
    "            if cleaned_output.startswith(\"```json\"):\n",
    "                cleaned_output = cleaned_output[len(\"```json\"):]\n",
    "            if cleaned_output.startswith(\"```\"):\n",
    "                cleaned_output = cleaned_output[len(\"```\"):]\n",
    "            if cleaned_output.endswith(\"```\"):\n",
    "                cleaned_output = cleaned_output[: -len(\"```\")]\n",
    "            return cleaned_output.strip()\n",
    "        except Exception:\n",
    "            return\n",
    "\n",
    "    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\n",
    "        try:\n",
    "            json_str = self.parse_json(text)\n",
    "            markdown_str = self.parse_markdown_code(text)\n",
    "            origin_str = self.parse_origin(text)\n",
    "\n",
    "            if json_str:\n",
    "                cleaned_output = json_str\n",
    "            elif markdown_str:\n",
    "                cleaned_output = markdown_str\n",
    "                if cleaned_output.startswith(\"```json\"):\n",
    "                    cleaned_output = cleaned_output[len(\"```json\"):]\n",
    "                if cleaned_output.startswith(\"```\"):\n",
    "                    cleaned_output = cleaned_output[len(\"```\"):]\n",
    "                if cleaned_output.endswith(\"```\"):\n",
    "                    cleaned_output = cleaned_output[: -len(\"```\")]\n",
    "            elif origin_str:\n",
    "                cleaned_output = origin_str\n",
    "            else:\n",
    "                cleaned_output = text.strip()\n",
    "\n",
    "            response = json.loads(cleaned_output)\n",
    "            action, action_input = response[\"action\"], response[\"action_input\"]\n",
    "            if action == \"Final Answer\":\n",
    "                return AgentFinish({\"output\": action_input}, text)\n",
    "            else:\n",
    "                return AgentAction(action, action_input, text)\n",
    "        except Exception:\n",
    "            if \"action\" not in text and \"action_input\" not in text:\n",
    "                return AgentFinish({\"output\": text}, text)\n",
    "            else:\n",
    "                logger.warning(f\"Not follow format instruction\\n{cleaned_output}\")\n",
    "                return AgentAction(\"RR\", \"\", text)\n",
    "\n",
    "        \n",
    "class pythonShellTool:\n",
    "\n",
    "    @prompts(\n",
    "        name=\"python_repl_ast\",\n",
    "        description=\"This is a python shell. Use this to execute python commands. \"\n",
    "        \"Input should be a valid python command. \"\n",
    "        \"The output of the command should not be too long.\"\n",
    "        \"The dataset is a dataframe assign to a variable df\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        try:\n",
    "            tree = ast.parse(query)\n",
    "            module = ast.Module(tree.body[:-1], type_ignores=[])\n",
    "            exec(astunparse.unparse(module), self.globals, self.locals)  # type: ignore\n",
    "            module_end = ast.Module(tree.body[-1:], type_ignores=[])\n",
    "            module_end_str = astunparse.unparse(module_end)  # type: ignore\n",
    "            try:\n",
    "                return eval(module_end_str, self.globals, self.locals)[:1024]\n",
    "            except Exception:\n",
    "                exec(module_end_str, self.globals, self.locals)\n",
    "                return \"\"\n",
    "        except Exception as e:\n",
    "            return str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f18ad817-5d64-4df1-9789-dc44c181ded2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LoadDataTool:\n",
    "    globals: Optional[Dict] = dict()\n",
    "    locals: Optional[Dict] = dict()\n",
    "\n",
    "    def __init__(self, status):\n",
    "        self.status = status\n",
    "        \n",
    "    @prompts(\n",
    "        name=\"Load\",\n",
    "        description=\"This is a tool to load dataset.\"\n",
    "        \"Input should be a dataset name.\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        \n",
    "        if self.status.dev:\n",
    "            name = \"WineQT\"\n",
    "            data = pd.read_csv(\"WineQT.csv\")\n",
    "            X = data.drop('quality', axis=1)\n",
    "            y = data['quality']\n",
    "            self.status.init_data(name, X, y, None, None)\n",
    "        else:\n",
    "            dataset = data_loader.select(query)\n",
    "            name = dataset['name']\n",
    "            label = dataset['label']\n",
    "            data = dataset['train']\n",
    "            X_train = data.drop(label, axis=1)\n",
    "            y_train = data[label]\n",
    "            data = dataset['test']\n",
    "            if label in data:\n",
    "                data = data.drop(label, axis=1)\n",
    "            X_test = data\n",
    "            y_test = pd.DataFrame(data={label: []*len(X_test)})\n",
    "            self.status.init_data(name, X, y)\n",
    "        \n",
    "        return f\"Load dataset {name} success !\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d4cbd73b-f37a-4e62-849b-bc4fff14fb8d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShowStatusTool:\n",
    "    globals: Optional[Dict] = dict()\n",
    "    locals: Optional[Dict] = dict()\n",
    "\n",
    "    def __init__(self, status):\n",
    "        self.status = status\n",
    "        \n",
    "    @prompts(\n",
    "        name=\"show Status\",\n",
    "        description=\"This is a tool to show current status.\"\n",
    "        \"Input should be an empty string.\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        return str({\n",
    "            'dataset': status.dataset,\n",
    "            'explore': status.explore,\n",
    "            'preprocess': status.preprocess,\n",
    "            'model_select': status.model_select,\n",
    "            'hyperopt': status.hyperopt,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "0fdfe72d-3216-49b7-826e-62cb9c81110c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "def examine_dataset(X, y):\n",
      "    # Check for missing values\n",
      "    if X.isnull().sum().sum() > 0:\n",
      "        result = \"There are missing values in the dataset. You need to decide how to handle them before choosing a model.\"\n",
      "    else:\n",
      "        result = \"There are no missing values in the dataset.\"\n",
      "    \n",
      "    # Check for outliers\n",
      "    if (X.dtypes == np.number).any():\n",
      "        num_cols = X.select_dtypes(include=np.number).columns\n",
      "        if (X[num_cols].apply(lambda x: np.abs(x - x.mean()) / x.std() > 3)).sum().sum() > 0:\n",
      "            result += \"\\nThere are outliers in the dataset. You need to decide how to handle them before choosing a model.\"\n",
      "        else:\n",
      "            result += \"\\nThere are no outliers in the dataset.\"\n",
      "    \n",
      "    # Check for class imbalance\n",
      "    if y.nunique() > 2:\n",
      "        class_counts = y.value_counts()\n",
      "        class_ratio = class_counts / class_counts.sum()\n",
      "        if (class_ratio < 0.05).any():\n",
      "            result += \"\\nThere is class imbalance in the dataset. You need to decide how to handle it before choosing a model.\"\n",
      "        else:\n",
      "            result += \"\\nThere is no class imbalance in the dataset.\"\n",
      "    \n",
      "    # Check for correlation\n",
      "    if (X.corr() > 0.8).sum().sum() > X.shape[1]:\n",
      "        result += \"\\nThere are highly correlated features in the dataset. You need to decide how to handle them before choosing a model.\"\n",
      "    else:\n",
      "        result += \"\\nThere are no highly correlated features in the dataset.\"\n",
      "    \n",
      "    # Check for distribution\n",
      "    if (X.dtypes == np.number).any():\n",
      "        num_cols = X.select_dtypes(include=np.number).columns\n",
      "        skewness = X[num_cols].skew()\n",
      "        if (skewness > 1).any():\n",
      "            result += \"\\nThere are features with skewed distribution in the dataset. You need to decide how to handle them before choosing a model.\"\n",
      "        else:\n",
      "            result += \"\\nAll features have approximately normal distribution.\"\n",
      "    \n",
      "    # Check for scaling\n",
      "    if (X.dtypes == np.number).any():\n",
      "        num_cols = X.select_dtypes(include=np.number).columns\n",
      "        if (X[num_cols].max() - X[num_cols].min() > 10).any():\n",
      "            result += \"\\nThere are features with different scales in the dataset. You need to decide how to handle them before choosing a model.\"\n",
      "        else:\n",
      "            result += \"\\nAll features have similar scales.\"\n",
      "    \n",
      "    return result\n",
      "```\n",
      "\n",
      "Example usage:\n",
      "\n",
      "```python\n",
      "X = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [0.1, 0.2, 0.3, 0.4, 0.5], 'C': ['a', 'b', 'c', 'd', 'e']})\n",
      "y = pd.Series([0, 1, 0, 1, 0])\n",
      "result = examine_dataset(X, y)\n",
      "print(result)\n",
      "```\n",
      "\n",
      "Output:\n",
      "\n",
      "```\n",
      "There are no missing values in the dataset.\n",
      "There are no outliers in the dataset.\n",
      "There is no class imbalance in the dataset.\n",
      "There are no highly correlated features in the dataset.\n",
      "All features have approximately normal distribution.\n",
      "All features have similar scales.\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"AI:\\n\\n```python\\nfrom lightgbm import LGBMClassifier\\n\\ndef construct_model():\\n    clf = LGBMClassifier()\\n    return clf\\n```\\n\\nExample usage:\\n\\n```python\\nclf = construct_model()\\nprint(clf)\\n```\\n\\nOutput:\\n\\n```\\nLGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\\n               importance_type='split', learning_rate=0.1, max_depth=-1,\\n               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\\n               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\\n               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\\n               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\\n```\\n\\n\""
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ModelSelectTool:\n",
    "    globals: Optional[Dict] = dict()\n",
    "    locals: Optional[Dict] = dict()\n",
    "\n",
    "    def __init__(self, status):\n",
    "        self.status = status\n",
    "        self.X_train = status.X_train\n",
    "        self.y_train = status.y_train\n",
    "        self.locals = {\"X_train\": self.X_train, 'y_train': self.y_train}\n",
    "        self.globals = {\"np\": np, 'pd': pd}\n",
    "        self.model_list = []\n",
    "        \n",
    "        from sklearn import ensemble, linear_model\n",
    "        import lightgbm\n",
    "        for module in [ensemble, linear_model, lightgbm]:\n",
    "            for name, func in vars(module).items():\n",
    "                if name[0].isupper():\n",
    "                    self.globals[name] = func\n",
    "                    self.model_list.append(name)\n",
    "        \n",
    "    @prompts(\n",
    "        name=\"Pick Model\",\n",
    "        description=\"This is a tool for deciding which model to use for a machine learning task.\"\n",
    "        \"Output is description about how to pick a model.\"\n",
    "        \"Input should be an empty string.\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        \n",
    "        if not self.status.dataset:\n",
    "            return \"Pick a dataset first.\"\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            know_how_prompt = coding_agent.run(\"How to decide which machine learning model to use ?\")\n",
    "            self.know_how_prompt = know_how_prompt\n",
    "\n",
    "            how_examine_dataset_thought = coding_agent.run(know_how_prompt + f\"\"\"\n",
    "            Here is some information about the feature dataframe `X`:\n",
    "            \\\"\\\"\\\"\n",
    "            - First five rows:\n",
    "            {self.X_train.head(5)}\n",
    "\n",
    "            - Column types:\n",
    "            {self.X_train.dtypes}\n",
    "            \\\"\\\"\\\"\n",
    "\n",
    "            Here is some information about the label series `y`:\n",
    "            \\\"\\\"\\\"\n",
    "            - First five rows:\n",
    "            {self.y_train.head(5)}\n",
    "\n",
    "            - Column types:\n",
    "            {self.y_train.dtypes}\n",
    "            \\\"\\\"\\\"\n",
    "\n",
    "            Tell me how to examine the dataset that can help you decide which model to use.\n",
    "            \"\"\")\n",
    "            self.how_examine_dataset_thought = how_examine_dataset_thought\n",
    "            # print(how_examine_dataset_thought)\n",
    "\n",
    "            examine_dataset_prompt = f\"\"\"Thoughts:\\n{how_examine_dataset_thought}\\n\n",
    "            Write a Python function called `examine_dataset` to examine the dataset.\n",
    "            Import any package you need beforehand.\n",
    "            Do not include visualization and plotting.\n",
    "            Make it as simple as possible.\n",
    "            The function will be called using `result = examine_dataset(X, y)`, and all findings must be stored in a string variable `result`.\n",
    "            Write the function as simple and short as possible.\n",
    "            Show results in markdown.\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "            examine_dataset_thought = coding_agent.run(examine_dataset_prompt)\n",
    "            print(examine_dataset_thought)\n",
    "\n",
    "            examine_dataset_code = extract_code(examine_dataset_thought)\n",
    "            self.examine_dataset_thought = examine_dataset_thought\n",
    "            self.examine_dataset_code = examine_dataset_code\n",
    "            # print(examine_dataset_code)\n",
    "\n",
    "            exec(examine_dataset_code, self.globals, self.locals)\n",
    "            examine_dataset_result = self.locals[\"examine_dataset\"](self.X_train, self.y_train)\n",
    "            self.examine_dataset_result = examine_dataset_result\n",
    "\n",
    "            final_prompt = f\"\"\"\n",
    "            Human:\\n{examine_dataset_prompt}\n",
    "            AI:\\n{examine_dataset_thought}\n",
    "            Human:\\nExecute result: {examine_dataset_result}\n",
    "\n",
    "            Which model will you use to achieve the best prediction results according to the examine result?\n",
    "            Also consider those models that are fast such as lightgbm.\n",
    "            These are some potential model you can choose:\n",
    "            {self.model_list}\n",
    "\n",
    "            Write a Python function called `construct_model` to create the model instance.\n",
    "            Initiate the model with no parameters.\n",
    "            Import any package you need beforehand.\n",
    "            Do not include visualization and plotting.\n",
    "            The function will be called using `clf = construct_model()`, and the constructed model will be stored in a string variable `clf`.\n",
    "            Show results in markdown.\n",
    "            \"\"\"\n",
    "            model_selection_thought = coding_agent.run(final_prompt)\n",
    "            self.model_selection_thought = model_selection_thought\n",
    "            # print(model_selection_thought)\n",
    "            model_selection_code = extract_code(model_selection_thought)\n",
    "            self.model_selection_code = model_selection_code\n",
    "            exec(model_selection_code, self.globals, self.locals)\n",
    "            self.clf = self.locals['construct_model']()\n",
    "\n",
    "            self.update_status()\n",
    "            return model_selection_thought\n",
    "        except:\n",
    "            return \"\"\"\n",
    "            \n",
    "            \"\"\"\n",
    "    \n",
    "    def update_status(self):\n",
    "        self.status.know_how_prompt = self.know_how_prompt\n",
    "        self.status.how_examine_dataset_thought = self.how_examine_dataset_thought\n",
    "        self.status.examine_dataset_thought = self.examine_dataset_thought\n",
    "        self.status.examine_dataset_code = self.examine_dataset_code\n",
    "        self.status.model_selection_thought = self.model_selection_thought\n",
    "        self.status.model_selection_code = self.model_selection_code\n",
    "        self.status.model = self.clf.__class__.__name__\n",
    "        self.status.model_selection = True\n",
    "\n",
    "        \n",
    "ModelSelectTool(status).inference(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "579f8fa8-72a1-48a5-bb92-f1df1c911907",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ExploreDataTool:\n",
    "    globals: Optional[Dict] = dict()\n",
    "    locals: Optional[Dict] = dict()\n",
    "\n",
    "    def __init__(self, status):\n",
    "        self.status = status\n",
    "        self.X = status.X\n",
    "        self.y = status.y\n",
    "        self.locals = {\"X\": self.X, 'y': self.y, 'np': np}\n",
    "        self.globals = {\"np\": np, 'pd': pd}\n",
    "        \n",
    "    @prompts(\n",
    "        name=\"Explore\",\n",
    "        description=\"This is a tool for dataset exploring.\"\n",
    "        \"Useful when the user ask for you to explore the dataset.\"\n",
    "        \"Output is the thought from exploring the dataset.\"\n",
    "        \"Input should be an empty string.\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        \n",
    "        if not self.status.dataset:\n",
    "            return \"Pick a dataset first.\"\n",
    "        \n",
    "        start_explore_prompt = f\"\"\"\n",
    "You are a data scientist. You are now performing data analysis before training a machine learning model.\n",
    "Your task is to tell me how the data could be preprocessed and to give me the corresponding Python code.\n",
    "The preprocessing includes, but is not limited to, missing value handling, scaling, normalization, or encoding.\n",
    "The task will be performed in two stages.\n",
    "In the first stage, you will decide which techniques are required to preprocess the dataset by investigating the dataset.\n",
    "In the second stage, you will preprocess the dataset by implementing the functions.\n",
    "\n",
    "Here is some information about the feature dataframe `X`:\n",
    "\\\"\\\"\\\"\n",
    "- First five rows:\n",
    "{self.X.head(5)}\n",
    "\n",
    "- Column types:\n",
    "{self.X.dtypes}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Here is some information about the label series `y`:\n",
    "\\\"\\\"\\\"\n",
    "- First five rows:\n",
    "{self.y.head(5)}\n",
    "\n",
    "- Column types:\n",
    "{self.y.dtypes}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "        how_to_explore_prompt = \"\"\"\n",
    "Now, let's move on to the first stage:\n",
    "1. Explain how you will explore the dataset in order to select appropriate preprocessing techniques.\n",
    "2. Write a Python function called `explore` to explore the dataset.\n",
    "   Import any package you need beforehand.\n",
    "   The function will be called using `result = explore(X, y)`, and all findings must be stored in a string variable `result`.\n",
    "\n",
    "Here are some handy packages in the Python environment:\n",
    "```\n",
    "numpy==1.24.3\n",
    "pandas==2.0.1\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "        prompt = \"\\n\\n\".join([start_explore_prompt, how_to_explore_prompt])\n",
    "        self.start_explore_prompt = start_explore_prompt\n",
    "        self.how_to_explore_prompt = how_to_explore_prompt\n",
    "        # print(prompt)\n",
    "\n",
    "        explore_data_thought = coding_agent.run(prompt)\n",
    "        self.explore_data_thought = explore_data_thought\n",
    "        # print(explore_data_thought)\n",
    "        \n",
    "        explore_code = extract_code(explore_data_thought)\n",
    "        self.explore_code = explore_code\n",
    "        # print(explore_code + '\\n')\n",
    "        self.retry_execute(explore_data_thought, explore_code)\n",
    "        \n",
    "        explore_data_result = self.locals[\"explore\"](self.X, self.y)\n",
    "        self.explore_data_result = explore_data_result\n",
    "        # print(explore_data_result)\n",
    "        \n",
    "        self.update_status()\n",
    "        return explore_data_result\n",
    "    \n",
    "    def update_status(self):\n",
    "        self.status.start_explore_prompt = self.start_explore_prompt\n",
    "        self.status.how_to_explore_prompt = self.how_to_explore_prompt\n",
    "        self.status.explore_code = self.explore_code\n",
    "        self.status.explore_data_result = self.explore_data_result\n",
    "        self.status.explore_data_thought = self.explore_data_thought\n",
    "        self.status.explore = True\n",
    "    \n",
    "    def retry_execute(self, prompt: str, code: str, retry: int = 3):\n",
    "        for _ in range(retry):\n",
    "            try:\n",
    "                exec(code, self.globals, self.locals)\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "                fix_code = coding_agent.run(f\"{prompt}\\n\\nExecute result:\\n{traceback.format_exc()}\\n\\nModify the code and fix the error.\")\n",
    "                code = extract_code(fix_code)\n",
    "                print(f\"Fix{_}:\\n{code}\\n\")\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "class ProcessDataTool:\n",
    "    globals: Optional[Dict] = dict()\n",
    "    locals: Optional[Dict] = dict()\n",
    "\n",
    "    def __init__(self, status):\n",
    "        self.status = status\n",
    "        self.X = status.X\n",
    "        self.y = status.y\n",
    "        self.locals = {\"X\": self.X, 'y': self.y, 'np': np}\n",
    "        self.globals = {\n",
    "            \"np\": np,\n",
    "            'pd': pd,\n",
    "        }\n",
    "        \n",
    "        from imblearn import over_sampling, under_sampling\n",
    "        from sklearn import preprocessing, impute, model_selection\n",
    "        for module in [preprocessing, impute, over_sampling, under_sampling, model_selection]:\n",
    "            for name, func in vars(module).items():\n",
    "                if name[0].isupper():\n",
    "                    self.globals[name] = func\n",
    "\n",
    "    @prompts(\n",
    "        name=\"Preprocess\",\n",
    "        description=\"This is a tool for preprocess the dataset.\"\n",
    "        \"Useful when you want to process the dataset.\"\n",
    "        \"Useful when you want to preprocess the dataset.\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        if not self.status.dataset:\n",
    "            return \"Pick a dataset first.\"\n",
    "        print()\n",
    "        \n",
    "        explore_data_result = self.status.explore_data_result\n",
    "        start_explore_prompt = self.status.start_explore_prompt\n",
    "        how_to_explore_prompt = self.status.how_to_explore_prompt\n",
    "        explore_data_thought = self.status.explore_data_thought\n",
    "\n",
    "        preprocess_prompt = \"\"\"\n",
    "        The exploration results from the first stage are shown above.\n",
    "        Now, let's move on to the second stage:\n",
    "        1. Provide a conclusion based on the exploration results.\n",
    "        2. Based on the conclusion, specify the preprocessing techniques to be employed for the dataset and briefly describe their implementations.\n",
    "        3. Write a Python function called `preprocess` that will assist you in preprocessing the data.\n",
    "           Import any package you need beforehand.\n",
    "           The function will be called using `proc_X_train, proc_y_train, proc_X_test, proc_y_test = preprocess(X_train, y_train, X_test, y_test)`, \n",
    "           where both the parameters and return values are eithor Pandas dataframes or series.\n",
    "           Be aware of potential data leakage issues.\n",
    "\n",
    "        Here are some handy packages in the Python environment:\n",
    "        ```\n",
    "        numpy==1.24.3\n",
    "        pandas==2.0.1\n",
    "        ```\n",
    "        No additional packages can be used. Return results about the second stage.\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        prompt = \"\\n\\n\".join([\n",
    "            start_explore_prompt,\n",
    "            how_to_explore_prompt,\n",
    "            f\"Your Response:\\n{explore_data_thought}\",\n",
    "            f\"Code Execution Results:\\n{explore_data_result}\",\n",
    "            preprocess_prompt]\n",
    "        )\n",
    "        preprocess_data_thought = coding_agent.run(prompt)\n",
    "        self.preprocess_data_thought = preprocess_data_thought\n",
    "        # print(preprocess_data_thought)\n",
    "        preprocess_code = extract_code(preprocess_data_thought)\n",
    "        self.preprocess_code = preprocess_code\n",
    "        # print(preprocess_code)\n",
    "        \n",
    "        best_result = {\n",
    "            'score': 100,\n",
    "            'code': preprocess_code,\n",
    "            'msg': ''\n",
    "        }\n",
    "        \n",
    "        for _ in range(5):\n",
    "            try:\n",
    "                print(preprocess_code)\n",
    "                exec(preprocess_code, self.globals, self.locals)\n",
    "                result = self.evaluate()\n",
    "                \n",
    "                if result['llm'] < best_result['score']:\n",
    "                    best_result['score'] = result['llm']\n",
    "                    best_result['msg'] = result['msg']\n",
    "                    best_result['code'] = preprocess_code\n",
    "                    \n",
    "                    self.status.X_train = result['processed_data'][0]\n",
    "                    self.status.X_test = result['processed_data'][2]\n",
    "                    self.status.y_train = result['processed_data'][1]\n",
    "                    self.status.y_test = result['processed_data'][3]\n",
    "                \n",
    "                fix_code = coding_agent.run(f\"{prompt}\\n\\nExecute result:\\n{result['msg']}\\n\\nYour result is LLm result. Try to improve the preprocess function.\")\n",
    "                preprocess_code = extract_code(fix_code)\n",
    "                # print(f\"Revise{_}:\\n{preprocess_code}\\n\")\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "                trace = traceback.format_exc()\n",
    "                print(trace)\n",
    "                fix_code = coding_agent.run(f\"{prompt}\\n\\nExecute result:\\n{str(e)}\\n\\nModify the code and fix the error. Try not to use the module that causes error. Try to simpify the function.\")\n",
    "                preprocess_code = extract_code(fix_code)\n",
    "                # print(f\"Fix{_}:\\n{preprocess_code}\\n\")\n",
    "                \n",
    "        print(best_result)\n",
    "        self.update_status()\n",
    "        return f\"{preprocess_data_thought}\\n\\nProcessing....\\n\\nProcess Completed !\\n{best_result['msg']}\"\n",
    "    \n",
    "    def update_status(self):\n",
    "        self.status.preprocess_data_thought = self.preprocess_data_thought\n",
    "        self.status.preprocess_code = self.preprocess_code\n",
    "        self.status.X_train = self.locals['X_train']\n",
    "        self.status.X_test = self.locals['X_test']\n",
    "        self.status.y_train = self.locals['y_train']\n",
    "        self.status.y_test = self.locals['y_test']\n",
    "        self.status.preprocess = True\n",
    "        \n",
    "    def evaluate(self):\n",
    "\n",
    "        def common_preprocess(X_train, y_train, X_test, y_test):\n",
    "            # TODO: Missing value dropping\n",
    "\n",
    "            # TODO: Categorical feature encoding\n",
    "\n",
    "            # Scale numerical variables using StandardScaler\n",
    "            scaler = StandardScaler()\n",
    "            X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "            X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "            return X_train, y_train, X_test, y_test\n",
    "        \n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        from lightgbm import LGBMRegressor, early_stopping\n",
    "\n",
    "        seed = 42\n",
    "\n",
    "        def evaluate(X_train, y_train, X_test, y_test, prep_func=None):\n",
    "            processed_data = (X_train, y_train, X_test, y_test)\n",
    "            if prep_func:\n",
    "                X_train, y_train, X_test, y_test = prep_func(X_train, y_train, X_test, y_test)\n",
    "                processed_data = (X_train, y_train, X_test, y_test)\n",
    "\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=seed)\n",
    "            clf = LGBMRegressor(random_state=seed, **self.status.best_param)\n",
    "            clf.fit(X_train, y_train, eval_set=(X_valid, y_valid), callbacks=[early_stopping(stopping_rounds=10)])\n",
    "            y_pred = clf.predict(X_test)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            return mse, processed_data\n",
    "\n",
    "        from copy import deepcopy\n",
    "        X_train = deepcopy(self.status.X_train)\n",
    "        X_test = deepcopy(self.status.X_test)\n",
    "        y_train = deepcopy(self.status.y_train)\n",
    "        y_test = deepcopy(self.status.y_test)\n",
    "        self.locals[\"X_train\"] = X_train\n",
    "        self.locals[\"X_test\"] = X_test\n",
    "        self.locals[\"y_train\"] = y_train\n",
    "        self.locals[\"y_test\"] = y_test\n",
    "        \n",
    "        # LLM\n",
    "        mse_llm, processed_data = evaluate(X_train, y_train, X_test, y_test, self.locals['preprocess'])\n",
    "\n",
    "        # Common\n",
    "        mse_com, _ = evaluate(X_train, y_train, X_test, y_test, common_preprocess)\n",
    "\n",
    "        # Raw\n",
    "        mse_raw, _ = evaluate(X_train, y_train, X_test, y_test)\n",
    "\n",
    "        msg = f\"No preprocess: {mse_raw}, Common preprocess: {mse_com}, LLM preprocess: {mse_llm}\"\n",
    "        print(msg)\n",
    "        return {\n",
    "            'msg': msg,\n",
    "            'no': mse_raw,\n",
    "            'common': mse_com,\n",
    "            'llm': mse_llm,\n",
    "            'processed_data': processed_data\n",
    "        }"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9df1b8a-5641-4b18-a91a-12e5adec780a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def gogogo():\n",
    "    print(\"====== gogogo ==========\")\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from lightgbm import LGBMRegressor, early_stopping\n",
    "    X_train = status.X_train\n",
    "    y_train = status.y_train\n",
    "    X_test = status.X_test\n",
    "    y_test = status.y_test\n",
    "    print(status.best_param)\n",
    "    seed = 42\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=seed)\n",
    "    clf = LGBMRegressor(random_state=seed, **status.best_param)\n",
    "    clf.fit(X_train, y_train, eval_set=(X_valid, y_valid), callbacks=[early_stopping(stopping_rounds=10)])\n",
    "    y_pred = clf.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(mse)\n",
    "    print(\"==============\")\n",
    "    \n",
    "gogogo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4830633-5499-44f3-bbc5-1741353fcb17",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts.chat import (\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "human_message_prompt = HumanMessagePromptTemplate(\n",
    "        prompt=PromptTemplate(\n",
    "            template=\"{text}\",\n",
    "            input_variables=[\"text\"],\n",
    "        )\n",
    "    )\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt_template)#, memory=ConversationBufferMemory())\n",
    "\n",
    "\n",
    "\n",
    "class CodingAgent():\n",
    "    def __init__(self, chain):\n",
    "        self.chain = chain\n",
    "    @functools.lru_cache(100)\n",
    "    def run(self, text):\n",
    "        return self.chain.run(text)\n",
    "    \n",
    "    \n",
    "coding_agent = CodingAgent(chain)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d1d0b2d-534c-45fe-948f-1b3d5d0b8339",
   "metadata": {},
   "source": [
    "coding_agent.run.cache_clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8abb5492-1b89-40cf-8fbb-ce80a398d801",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pdb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class E2EHyperOptTool:\n",
    "    globals: Optional[Dict] = dict()\n",
    "    locals: Optional[Dict] = dict()\n",
    "\n",
    "    def __init__(self, status):\n",
    "        self.status = status\n",
    "        self.X_train = status.X_train\n",
    "        self.y_train = status.y_train\n",
    "        self.X_test = status.X_test\n",
    "        self.y_test = status.y_test\n",
    "        # self.locals = {\"X\": X, 'Y': Y,}\n",
    "\n",
    "    @prompts(\n",
    "        name=\"hyperopt\",\n",
    "        description=\"This is a tool for hyper parameter tuning.\"\n",
    "        \"Input should be an empty string.\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        if not self.status.dataset:\n",
    "            return \"Pick a dataset first.\"\n",
    "        if not self.status.model:\n",
    "            msg = \"You have not select the model. Use LGBMRegressor for this task.\"\n",
    "            self.status.model = \"LGBMRegressor\"\n",
    "        \n",
    "        clf = self.construct_estimator(self.status.model)\n",
    "        param_grid = self.construct_param_grid(clf)\n",
    "        search_algo = self.get_search_algo(clf, param_grid)\n",
    "        clff = self.construct_search_algo(search_algo)\n",
    "        self.clff = clff\n",
    "        search = clff.fit(self.X_train, self.y_train)\n",
    "        self.search = search\n",
    "        \n",
    "        print(f\"Best search score: {search.best_score_}\")\n",
    "        score = self.evaluate(clf, search)\n",
    "\n",
    "        self.update_status()\n",
    "        return f\"{self.reason}\\n\\nFinal score after hyper parameter tuning is {score}\"\n",
    "    \n",
    "    def update_status(self):\n",
    "        self.status.hyperopt = True\n",
    "        self.status.hyper_result = self.search\n",
    "        self.status.hyperopt_param_thought = self.reason\n",
    "        \n",
    "    def evaluate(self, clf, search):\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        clff = clf.__class__(**search.best_params_)\n",
    "        clff.fit(self.X_train, self.y_train)\n",
    "        y_pred = clff.predict(self.X_test)\n",
    "        score = mean_squared_error(self.y_test, y_pred)\n",
    "        print(f\"Testing score: {score}\")\n",
    "        return score\n",
    "    \n",
    "    def construct_search_algo(self, search_algo):\n",
    "        # construct search instance\n",
    "        construct_search_prompt = f\"\"\"\n",
    "        Give me two lines of python code.\n",
    "        Frist, from lib import {search_algo.strip()}.\n",
    "        Second, instatiate an instance with {search_algo.strip()}(clf, param_grid, random_state=0, n_iter=10) and assign to hyper_algo variable.\n",
    "        \"\"\"\n",
    "        codes = coding_agent.run(construct_search_prompt)\n",
    "        print(f\"\\nConstructing search algo:\\n```\\n{codes}\\n```\\n\")\n",
    "        res = self.execute(codes)\n",
    "        if 'hyper_algo' not in self.locals:\n",
    "            raise Exception(res)\n",
    "        return self.locals['hyper_algo']\n",
    "\n",
    "    def get_search_algo(self, clf, param_grid):\n",
    "        search_algo_reason_prompt = f\"\"\"\n",
    "        You are now performing a hyper parameter search on a machine learning model.\n",
    "        Your goal is to decide the search algorithm base on model type and parameter space.\n",
    "\n",
    "\n",
    "        The given model is a {clf.__class__.__name__}.\n",
    "        The paramters grid is {param_grid}.\n",
    "        The avaiable search algorithm is as follow:\n",
    "\n",
    "        1. GridSearchCV: Exhaustive search over specified parameter values for an estimator.\n",
    "        2. HalvingGridSearchCV: Search over specified parameter values with successive halving.\n",
    "        3. ParameterGrid: Grid of parameters with a discrete number of values for each.\n",
    "        4. ParameterSampler: Generator on parameters sampled from given distributions.\n",
    "        5. RandomizedSearchCV: Randomized search on hyper parameters.\n",
    "        6. HalvingRandomSearchCV: Randomized search on hyper parameters with successive halving.\n",
    "\n",
    "        Think about which algorithm to use base on the model, parameter space and algorithm description.\n",
    "        Tell me which algorithm you have choosen and why.\n",
    "        List all the other algorithms you have choosen not to use and why.\n",
    "        \"\"\"\n",
    "        \n",
    "        reason = coding_agent.run(search_algo_reason_prompt)\n",
    "        self.reason = reason\n",
    "        print(reason)\n",
    "\n",
    "        search_algo = coding_agent.run(f\"{reason}\\n\\nAccording to above, give me the algorithm name of the choosen algorithm in a single token.\")\n",
    "        print(f\"\\nDeciding search algo:\\n{search_algo}\\n\")\n",
    "        return search_algo\n",
    "\n",
    "    def construct_param_grid(self, clf):\n",
    "        param_grid_prompt = f\"\"\"\n",
    "        You are now performing a hyper parameter search on a machine learning model.\n",
    "        Your goal is to construct the paramter space to search.\n",
    "\n",
    "        For example:\n",
    "        The model is a LinearSVR.\n",
    "        The parameters are {LinearSVR().get_params()}\n",
    "        The parameter grid can be defined as:\n",
    "        ```\n",
    "        param_grid = {{'C': [1, 10, 100, 1000], 'tol': [0.001, 0.0001]}}\n",
    "        ```\n",
    "        Remeber to define the space only on important features. \n",
    "\n",
    "\n",
    "        Now.\n",
    "        The given model is a {clf.__class__.__name__}.\n",
    "        The paramters are {clf.get_params()}.\n",
    "        The detail description about the parameters is as follow:\n",
    "        {clf.__doc__}\n",
    "\n",
    "        Construct the parameter grid you will use for hyper parameter in json format:\n",
    "        \"\"\"\n",
    "\n",
    "        param_grid = coding_agent.run(param_grid_prompt)\n",
    "        param_grid = json.loads(param_grid)\n",
    "        print(f\"\\nConstruct params grid:\\n{param_grid}\\n\")\n",
    "        self.locals['param_grid'] = param_grid\n",
    "        return param_grid\n",
    "        \n",
    "    def construct_estimator(self, estimator_name: str):\n",
    "        # construct clf instance\n",
    "        construct_estimator_prompt = f\"Give me two lines of python code. Construct a {estimator_name.strip()} estimator and assign the estimator clf variable.\"\n",
    "        codes = coding_agent.run(construct_estimator_prompt)\n",
    "        print(f\"\\nConstruct estimator with following:\\n```\\n{codes}\\n```\\n\")\n",
    "        res = self.execute(codes)\n",
    "        if 'clf' not in self.locals:\n",
    "            raise Exception(res)\n",
    "        return self.locals['clf']\n",
    "        \n",
    "    def execute(self, cmd: str):\n",
    "        try:\n",
    "            tree = ast.parse(cmd)\n",
    "            module = ast.Module(tree.body[:-1], type_ignores=[])\n",
    "            exec(astunparse.unparse(module), self.globals, self.locals)  # type: ignore\n",
    "            module_end = ast.Module(tree.body[-1:], type_ignores=[])\n",
    "            module_end_str = astunparse.unparse(module_end)  # type: ignore\n",
    "            try:\n",
    "                return eval(module_end_str, self.globals, self.locals)\n",
    "            except Exception:\n",
    "                exec(module_end_str, self.globals, self.locals)\n",
    "                return \"\"\n",
    "        except Exception as e:\n",
    "            return str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "e72df5c3-a5f6-4c71-b052-1db08579cbd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConversationBot(ABC):\n",
    "\n",
    "    def __init__(self):\n",
    "        seed_everything(0)\n",
    "        # self.handler = LoggerCallbackHandler()\n",
    "        # self.async_handler = AsyncLoggerCallbackHandler()\n",
    "        self.llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\", output_key=\"output\", return_messages=True\n",
    "        )\n",
    "        self.state = []\n",
    "        self.tools = self.load_tools()\n",
    "\n",
    "        # retry when llm output parsing error\n",
    "        retrytool = retryActionTool()\n",
    "        func = retrytool.inference\n",
    "        self.tools.append(\n",
    "            Tool(\n",
    "                name=func.name,\n",
    "                description=func.description,\n",
    "                # coroutine=to_async(func),\n",
    "                func=func,\n",
    "            )\n",
    "        )\n",
    "        self.output_parser = RobustRetryConvoOutputParser()\n",
    "        self.agent = self.init_agent()\n",
    "\n",
    "    def run_text(self, text: str, state):\n",
    "        logger.info(f\"User: {text}\")\n",
    "        res = self.agent({\"input\": text})\n",
    "        response = res[\"output\"]\n",
    "        self.state += [(text, response)]\n",
    "        logger.info(\n",
    "            f\"\\nProcessed run_text, Input text: {text}\\nCurrent state: {self.state}\\n\"\n",
    "        )\n",
    "        return self.state, self.state\n",
    "\n",
    "    async def arun_text(self, text: str):\n",
    "        logger.info(f\"User: {text}\")\n",
    "        res = await self.agent.acall({\"input\": text})\n",
    "        response = res[\"output\"]\n",
    "        self.state += [(text, response)]\n",
    "        logger.info(\n",
    "            f\"\\nProcessed run_text, Input text: {text}\\nCurrent state: {self.state}\\n\"\n",
    "        )\n",
    "        return self.state, self.state\n",
    "\n",
    "    def _clear(self):\n",
    "        self.memory.clear()\n",
    "        self.state = []\n",
    "\n",
    "    def init_agent(self):\n",
    "        input_variables = [\"input\", \"agent_scratchpad\", \"chat_history\"]\n",
    "        return initialize_agent(\n",
    "            self.tools,\n",
    "            self.llm,\n",
    "            agent=\"chat-conversational-react-description\",\n",
    "            verbose=True,\n",
    "            memory=self.memory,\n",
    "            return_intermediate_steps=True,\n",
    "            max_iterations=5,\n",
    "            # max_execution_time=60,\n",
    "            agent_kwargs={\n",
    "                \"prefix\": AUTOML_CHATGPT_PREFIX,\n",
    "                \"format_instructions\": AUTOML_CHATGPT_FORMAT_INSTRUCTIONS,\n",
    "                \"suffix\": AUTOML_CHATGPT_SUFFIX,\n",
    "                \"input_variables\": input_variables,\n",
    "                \"output_parser\": self.output_parser,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def load_tools(self):\n",
    "        tools = []\n",
    "        X, X_train, X_test, y, y_train, y_test = self.load_artifacts()\n",
    "\n",
    "        for tool in [\n",
    "            # pythonShellTool(),\n",
    "            E2EHyperOptTool(status),\n",
    "            # HyperOptTool(),\n",
    "            # ConstructSearchTool(),\n",
    "            # ConstructParamGridTool(),\n",
    "            # ConstructEstimatorTool(),\n",
    "            # FinalTool(X, Y),\n",
    "            ShowStatusTool(status),\n",
    "            ModelSelectTool(status),\n",
    "            ExploreDataTool(status),\n",
    "            ProcessDataTool(status),\n",
    "            LoadDataTool(status),\n",
    "        ]:\n",
    "            func = tool.inference\n",
    "            tools.append(\n",
    "                Tool(\n",
    "                    name=func.name,\n",
    "                    description=func.description,\n",
    "                    # coroutine=to_async(func),\n",
    "                    func=func,\n",
    "                )\n",
    "            )\n",
    "        logger.info(f\"tools: {[i.name for i in tools]}\")\n",
    "        return tools\n",
    "\n",
    "    def load_artifacts(self):\n",
    "        data = pd.read_csv(\"WineQT.csv\")\n",
    "        X = data.drop('quality', axis=1)\n",
    "        y = data['quality']\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        # X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "        return X, X_train, X_test, y, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef371d4b-adea-4524-bbfe-d2635cbaa7f9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from prompt import AUTOML_CHATGPT_PREFIX, AUTOML_CHATGPT_FORMAT_INSTRUCTIONS, AUTOML_CHATGPT_SUFFIX\n",
    "from langchain.agents.initialize import initialize_agent\n",
    "from langchain.agents.tools import Tool\n",
    "from sklearn.svm import LinearSVR\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from scipy.stats import boxcox\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0155fe39-8191-441c-9f4a-6eeaa45c3ed8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "chat_agent = ConversationBot()\n",
    "X, X_train, X_test, y, y_train, y_test = chat_agent.load_artifacts()\n",
    "\n",
    "clf = LGBMRegressor()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "96a6cdbe-3beb-4b45-8f19-50803da3436b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Status():\n",
    "    \n",
    "    def __init__(self, dev=True):\n",
    "        self.init()\n",
    "        self.dev = dev\n",
    "        \n",
    "    def init_data(self, dataset, X_train, X_test, y_train, y_test):\n",
    "        self.init()\n",
    "        self.dataset = dataset\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.1, random_state=42)\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "    \n",
    "    def init(self):\n",
    "        \n",
    "        self.dataset = \"\"\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        \n",
    "        self.start_explore_prompt = \"\"\n",
    "        self.how_to_explore_prompt = \"\"\n",
    "        self.explore_data_thought = \"\"\n",
    "        self.hyperopt_param_thought = \"\"\n",
    "        \n",
    "        self.explore_result = \"Nothing special.\"\n",
    "        \n",
    "        self.explore = False\n",
    "        self.preprocess = False\n",
    "        self.model_select = False\n",
    "        self.hyperopt = False\n",
    "        \n",
    "        self.explore_code = \"\"\n",
    "        self.preprocess_code = \"\"\n",
    "        self.model_select_code = \"\"\n",
    "        self.hyperopt_code = \"\"\n",
    "        \n",
    "        self.model = \"\"\n",
    "        self.best_param = {}\n",
    "\n",
    "status = Status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bf955478-9990-468d-bbb0-b5573c4470eb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [74]\u001b[0m, in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(e)\n\u001b[1;32m     50\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m DataLoader(chain)\n\u001b[0;32m---> 51\u001b[0m \u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m data_loader\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallstate\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [74]\u001b[0m, in \u001b[0;36mDataLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(train_path)\n\u001b[1;32m     35\u001b[0m test_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkaggle\u001b[39m\u001b[38;5;124m'\u001b[39m, dirn, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m test_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(train_df\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(test_df\u001b[38;5;241m.\u001b[39mcolumns))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets[dirn] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: train_df,\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m: test_df,\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: label,\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: dirn\n\u001b[1;32m     45\u001b[0m }\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/pandas/io/parsers/readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1255\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1253\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1255\u001b[0m     index, columns, col_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 225\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    227\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:805\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:883\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:1026\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:1072\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:1147\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/pandas/core/dtypes/common.py:1429\u001b[0m, in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[38;5;66;03m# Note: if other EA dtypes are ever held in HybridBlock, exclude those\u001b[39;00m\n\u001b[1;32m   1421\u001b[0m     \u001b[38;5;66;03m#  here too.\u001b[39;00m\n\u001b[1;32m   1422\u001b[0m     \u001b[38;5;66;03m# NB: need to check DatetimeTZDtype and not is_datetime64tz_dtype\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m     \u001b[38;5;66;03m#  to exclude ArrowTimestampUSDtype\u001b[39;00m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1425\u001b[0m         dtype, (DatetimeTZDtype, PeriodDtype)\n\u001b[1;32m   1426\u001b[0m     )\n\u001b[0;32m-> 1429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_extension_array_dtype\u001b[39m(arr_or_dtype) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m   1430\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;124;03m    Check if an object is a pandas extension array type.\u001b[39;00m\n\u001b[1;32m   1432\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[1;32m   1473\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1474\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(arr_or_dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, arr_or_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class DataLoader():\n",
    "    \n",
    "    def __init__(self, chain):\n",
    "        self.datasets = {}\n",
    "        self.chain = chain\n",
    "        \n",
    "    def select(self, name: str):\n",
    "        if name not in self.datasets:\n",
    "            prompt = f\"\"\"\n",
    "            These are the datasets we have:\n",
    "            {self.datasets.keys()}\n",
    "            If someone ask for {name}, what is the most possible dataset that is referred to ?\n",
    "            Give me the response in one single word:\n",
    "            \"\"\"\n",
    "            print(prompt)\n",
    "            name = self.chain.run(prompt)\n",
    "            return self.datasets[name]\n",
    "        else:\n",
    "            return self.datasets[name]\n",
    "        \n",
    "    def show(self):\n",
    "        print(self.datasets.keys())\n",
    "    \n",
    "    def load(self):\n",
    "        \n",
    "        for dirn in os.listdir('kaggle'):\n",
    "            \n",
    "            if not os.path.isdir(os.path.join('kaggle', dirn)):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                train_path = os.path.join('kaggle', dirn, 'train.csv')\n",
    "                train_df = pd.read_csv(train_path)\n",
    "\n",
    "                test_path = os.path.join('kaggle', dirn, 'test.csv')\n",
    "                test_df = pd.read_csv(test_path)\n",
    "\n",
    "                label = list(set(train_df.columns) - set(test_df.columns))[0]\n",
    "\n",
    "                self.datasets[dirn] = {\n",
    "                    'train': train_df,\n",
    "                    'test': test_df,\n",
    "                    'label': label,\n",
    "                    'name': dirn\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                \n",
    "                \n",
    "data_loader = DataLoader(chain)\n",
    "data_loader.load()\n",
    "data_loader.select('allstate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fb79fc-bf38-421e-bc3d-751c145a96d1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class E2EPreprocessTool:\n",
    "    globals: Optional[Dict] = dict()\n",
    "    locals: Optional[Dict] = dict()\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.locals = {\"X\": X, 'y': y, 'np': np}\n",
    "        self.globals = {\n",
    "            \"np\": np,\n",
    "            'pd': pd,\n",
    "            # 'StandardScaler': StandardScaler,\n",
    "            # 'PowerTransformer': PowerTransformer,\n",
    "            # 'boxcox': boxcox,\n",
    "            # 'SMOTE': SMOTE,\n",
    "            # 'RandomOverSampler': RandomOverSampler\n",
    "        }\n",
    "\n",
    "    @prompts(\n",
    "        name=\"preprocess\",\n",
    "        description=\"This is a tool for dataset preprocessing.\"\n",
    "        \"Input should be a dataset name.\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        explore_prompts = self.explore_dataset(query)\n",
    "        self.preprocess_dataset(explore_prompts)\n",
    "        # self.evaluate()\n",
    "        \n",
    "    def retry_execute(self, prompt: str, code: str, retry: int = 3):\n",
    "        for _ in range(retry):\n",
    "            try:\n",
    "                exec(code, self.globals, self.locals)\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "                fix_code = coding_agent.run(f\"{prompt}\\n\\nExecute result:\\n{traceback.format_exc()}\\n\\nModify the code and fix the error.\")\n",
    "                code = extract_code(fix_code)\n",
    "                print(f\"Fix{_}:\\n{code}\\n\")\n",
    "        \n",
    "    def evaluate(self):\n",
    "\n",
    "        def common_preprocess(X_train, y_train, X_test, y_test):\n",
    "            # TODO: Missing value dropping\n",
    "\n",
    "            # TODO: Categorical feature encoding\n",
    "\n",
    "            # Scale numerical variables using StandardScaler\n",
    "            scaler = StandardScaler()\n",
    "            X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "            X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "            return X_train, y_train, X_test, y_test\n",
    "        \n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        from lightgbm import LGBMRegressor, early_stopping\n",
    "\n",
    "        seed = 42\n",
    "\n",
    "        def evaluate(X_train, y_train, X_test, y_test, prep_func=None):\n",
    "            if prep_func:\n",
    "                X_train, y_train, X_test, y_test = prep_func(X_train, y_train, X_test, y_test)\n",
    "\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=seed)\n",
    "            clf = LGBMRegressor(random_state=seed)\n",
    "            clf.fit(X_train, y_train, eval_set=(X_valid, y_valid), callbacks=[early_stopping(stopping_rounds=10)])\n",
    "            y_pred = clf.predict(X_test)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "            return mse\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.2, random_state=seed)\n",
    "\n",
    "\n",
    "        # LLM\n",
    "        mse_llm = evaluate(X_train, y_train, X_test, y_test, self.locals['preprocess'])\n",
    "\n",
    "        # Common\n",
    "        mse_com = evaluate(X_train, y_train, X_test, y_test, common_preprocess)\n",
    "\n",
    "        # Raw\n",
    "        mse_raw = evaluate(X_train, y_train, X_test, y_test)\n",
    "\n",
    "        msg = f\"No preprocess: {mse_raw}, Common preprocess: {mse_com}, LLM preprocess: {mse_llm}\"\n",
    "        print(msg)\n",
    "        return {\n",
    "            'msg': msg,\n",
    "            'no': mse_raw,\n",
    "            'common': mse_com,\n",
    "            'llm': mse_llm\n",
    "        }\n",
    "        \n",
    "    def preprocess_dataset(self, explore_prompts: str):\n",
    "        start_explore_prompt, how_to_explore_prompt, explore_data_thought, explore_data_result = explore_prompts\n",
    "        \n",
    "        preprocess_prompt = \"\"\"\n",
    "        The exploration results from the first stage are shown above.\n",
    "        Now, let's move on to the second stage:\n",
    "        1. Provide a conclusion based on the exploration results.\n",
    "        2. Based on the conclusion, specify the preprocessing techniques to be employed for the dataset and briefly describe their implementations.\n",
    "        3. Write a Python function called `preprocess` that will assist you in preprocessing the data.\n",
    "           Import any package you need beforehand.\n",
    "           The function will be called using `proc_X_train, proc_y_train, proc_X_test, proc_y_test = preprocess(X_train, y_train, X_test, y_test)`, \n",
    "           where both the parameters and return values are eithor Pandas dataframes or series.\n",
    "           Be aware of potential data leakage issues.\n",
    "\n",
    "        Here are some handy packages in the Python environment:\n",
    "        ```\n",
    "        numpy==1.24.3\n",
    "        pandas==2.0.1\n",
    "        ```\n",
    "        No additional packages can be used.\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        scikit-learn==1.2.2\n",
    "        scipy==1.10.1\n",
    "        imbalanced-learn==0.10.1\n",
    "        \"\"\"\n",
    "\n",
    "        prompt = \"\\n\\n\".join([\n",
    "            start_explore_prompt,\n",
    "            how_to_explore_prompt,\n",
    "            f\"Your Response:\\n{explore_data_thought}\",\n",
    "            f\"Code Execution Results:\\n{explore_data_result}\",\n",
    "            preprocess_prompt]\n",
    "        )\n",
    "        preprocess_data_thought = coding_agent.run(prompt)\n",
    "        print(preprocess_data_thought)\n",
    "        preprocess_code = extract_code(preprocess_data_thought)\n",
    "        self.preprocess_code = preprocess_code\n",
    "        \n",
    "        best_result = {\n",
    "            'score': 100,\n",
    "            'code': preprocess_code\n",
    "        }\n",
    "        \n",
    "        for _ in range(5):\n",
    "            try:\n",
    "                exec(preprocess_code, self.globals, self.locals)\n",
    "                result = self.evaluate()\n",
    "                \n",
    "                if result['llm'] < best_result['score']:\n",
    "                    best_result['score'] = result['llm']\n",
    "                    best_result['code'] = preprocess_code\n",
    "                \n",
    "                fix_code = coding_agent.run(f\"{prompt}\\n\\nExecute result:\\n{result['msg']}\\n\\nYour result is LLm result. Try to improve the preprocess function.\")\n",
    "                preprocess_code = extract_code(fix_code)\n",
    "                print(f\"Revise{_}:\\n{preprocess_code}\\n\")\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "                trace = traceback.format_exc()\n",
    "                print(trace)\n",
    "                fix_code = coding_agent.run(f\"{prompt}\\n\\nExecute result:\\n{str(e)}\\n\\nModify the code and fix the error. Try not to use the module that causes error. Try to simpify the function.\")\n",
    "                preprocess_code = extract_code(fix_code)\n",
    "                print(f\"Fix{_}:\\n{preprocess_code}\\n\")\n",
    "        \n",
    "        # self.retry_execute(preprocess_data_thought, preprocess_code)\n",
    "\n",
    "        \n",
    "    def explore_dataset(self, query: str):\n",
    "        start_explore_prompt = f\"\"\"\n",
    "You are a data scientist. You are now performing data analysis before training a machine learning model.\n",
    "Your task is to tell me how the data could be preprocessed and to give me the corresponding Python code.\n",
    "The preprocessing includes, but is not limited to, missing value handling, scaling, normalization, or encoding.\n",
    "The task will be performed in two stages.\n",
    "In the first stage, you will decide which techniques are required to preprocess the dataset by investigating the dataset.\n",
    "In the second stage, you will preprocess the dataset by implementing the functions.\n",
    "\n",
    "Here is some information about the feature dataframe `X`:\n",
    "\\\"\\\"\\\"\n",
    "- First five rows:\n",
    "{self.X.head(5)}\n",
    "\n",
    "- Column types:\n",
    "{self.X.dtypes}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Here is some information about the label series `y`:\n",
    "\\\"\\\"\\\"\n",
    "- First five rows:\n",
    "{self.y.head(5)}\n",
    "\n",
    "- Column types:\n",
    "{self.y.dtypes}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "        how_to_explore_prompt = \"\"\"\n",
    "Now, let's move on to the first stage:\n",
    "1. Explain how you will explore the dataset in order to select appropriate preprocessing techniques.\n",
    "2. Write a Python function called `explore` to explore the dataset.\n",
    "   Import any package you need beforehand.\n",
    "   The function will be called using `result = explore(X, y)`, and all findings must be stored in a string variable `result`.\n",
    "\n",
    "Here are some handy packages in the Python environment:\n",
    "```\n",
    "numpy==1.24.3\n",
    "pandas==2.0.1\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "        prompt = \"\\n\\n\".join([start_explore_prompt, how_to_explore_prompt])\n",
    "        self.prompt = prompt\n",
    "        print(prompt)\n",
    "\n",
    "        explore_data_thought = coding_agent.run(prompt)\n",
    "        self.explore_data_thought = explore_data_thought\n",
    "        print(explore_data_thought)\n",
    "        \n",
    "        explore_code = extract_code(explore_data_thought)\n",
    "        self.explore_code = explore_code\n",
    "        print(explore_code + '\\n')\n",
    "        self.retry_execute(explore_data_thought, explore_code)\n",
    "        \n",
    "        explore_data_result = self.locals[\"explore\"](self.X, self.y)\n",
    "        self.explore_data_result = explore_data_result\n",
    "        print(explore_data_result)\n",
    "        \n",
    "        return (start_explore_prompt, how_to_explore_prompt, explore_data_thought, explore_data_result)\n",
    "    \n",
    "    \n",
    "preprocess_tool = E2EPreprocessTool(X, y)\n",
    "preprocess_tool.inference(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3d1c67-2bf7-40c4-97ff-b377a6153426",
   "metadata": {},
   "source": [
    "# Demo start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "6b7628c5-ab46-4c4a-9b1c-6a8c43f6b969",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Load\",\n",
      "    \"action_input\": \"dataset_name\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mLoad dataset WineQT success !\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The dataset WineQT has been successfully loaded.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('The dataset WineQT has been successfully loaded.',\n",
       " [('Can you load the dataset ?',\n",
       "   'The dataset WineQT has been successfully loaded.')])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_agent = ConversationBot()\n",
    "chat_agent.run_text(\"Can you load the dataset ?\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13edb545-ac4a-4f86-b028-ff80d2efe7b0",
   "metadata": {},
   "source": [
    "# Demo 1: Can you explore the dataset ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "34b1cd82-cb8c-4c11-a14f-37e6f83f33a4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Explore\",\n",
      "    \"action_input\": \"\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mThere are no missing values in the dataset.\n",
      "There are 415 outliers in the dataset. You may need to handle them before preprocessing.\n",
      "Some features are highly skewed. You may need to apply scaling or normalization.\n",
      "All features are numerical.\n",
      "There is no high correlation between features.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The dataset has no missing values, 415 outliers, some highly skewed features that may require scaling or normalization, all features are numerical, and there is no high correlation between features.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('The dataset has no missing values, 415 outliers, some highly skewed features that may require scaling or normalization, all features are numerical, and there is no high correlation between features.',\n",
       " [('Can you explore the dataset ?',\n",
       "   'The dataset has no missing values, 415 outliers, some highly skewed features that may require scaling or normalization, all features are numerical, and there is no high correlation between features.')])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_agent = ConversationBot()\n",
    "chat_agent.run_text(\"Can you explore the dataset ?\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b70ed09-bf75-4653-8b1f-0993a9f0ab78",
   "metadata": {},
   "source": [
    "# Demo 2: Can you preprocess the dataset ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "26a0e3a0-bd54-42c7-81b6-a6b8effb4309",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Preprocess\",\n",
      "    \"action_input\": \"\"\n",
      "}\u001b[0m\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "def preprocess(X_train, y_train, X_test, y_test):\n",
      "    # Remove outliers\n",
      "    quantiles = X_train.quantile([0.25, 0.75])\n",
      "    IQR = quantiles.loc[0.75] - quantiles.loc[0.25]\n",
      "    lower_bound = quantiles.loc[0.25] - 1.5 * IQR\n",
      "    upper_bound = quantiles.loc[0.75] + 1.5 * IQR\n",
      "    X_train = X_train[(X_train >= lower_bound) & (X_train <= upper_bound)].dropna()\n",
      "    y_train = y_train[X_train.index]\n",
      "    X_test = X_test[(X_test >= lower_bound) & (X_test <= upper_bound)].dropna()\n",
      "    y_test = y_test[X_test.index]\n",
      "    \n",
      "    # Apply Box-Cox transformation to skewed features\n",
      "    skewness = X_train.skew()\n",
      "    skewed_features = skewness[abs(skewness) > 1].index\n",
      "    for feature in skewed_features:\n",
      "        X_train[feature], _ = pd.Series(np.boxcox(X_train[feature] + 1))\n",
      "        X_test[feature], _ = pd.Series(np.boxcox(X_test[feature] + 1))\n",
      "    \n",
      "    return X_train, y_train, X_test, y_test\n",
      "\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's l2: 0.417969\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[33]\tvalid_0's l2: 0.279241\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's l2: 0.282781\n",
      "No preprocess: 0.30902833514370315, Common preprocess: 0.32204435141295495, LLM preprocess: 0.2724000804561878\n",
      "====== gogogo ==========\n",
      "{}\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's l2: 0.417969\n",
      "0.2724000804561878\n",
      "==============\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "def preprocess(X_train, y_train, X_test, y_test):\n",
      "    # Outlier handling\n",
      "    for col in X_train.columns:\n",
      "        q_05 = X_train[col].quantile(0.05)\n",
      "        q_95 = X_train[col].quantile(0.95)\n",
      "        X_train[col] = np.where(X_train[col] < q_05, q_05, np.where(X_train[col] > q_95, q_95, X_train[col]))\n",
      "        X_test[col] = np.where(X_test[col] < q_05, q_05, np.where(X_test[col] > q_95, q_95, X_test[col]))\n",
      "    \n",
      "    # Scaling\n",
      "    skewed_cols = ['residual sugar', 'free sulfur dioxide', 'total sulfur dioxide', 'sulphates', 'alcohol']\n",
      "    for col in skewed_cols:\n",
      "        X_train[col] = np.log(X_train[col])\n",
      "        X_test[col] = np.log(X_test[col])\n",
      "    \n",
      "    return X_train, y_train, X_test, y_test\n",
      "\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's l2: 0.420484\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[35]\tvalid_0's l2: 0.413562\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's l2: 0.420484\n",
      "No preprocess: 0.26457902890194207, Common preprocess: 0.25990127753381176, LLM preprocess: 0.26457902890194207\n",
      "====== gogogo ==========\n",
      "{}\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's l2: 0.420484\n",
      "0.26457902890194207\n",
      "==============\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "def preprocess(X_train, y_train, X_test, y_test):\n",
      "    # Remove outliers using the IQR method\n",
      "    quantiles = X_train.quantile([0.25, 0.75])\n",
      "    IQR = quantiles.loc[0.75] - quantiles.loc[0.25]\n",
      "    X_train = X_train[~((X_train < (quantiles.loc[0.25] - 1.5 * IQR)) | (X_train > (quantiles.loc[0.75] + 1.5 * IQR))).any(axis=1)]\n",
      "    y_train = y_train[X_train.index]\n",
      "    \n",
      "    # Scale the features using the standard scaler method\n",
      "    scaler = StandardScaler()\n",
      "    X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
      "    X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
      "    \n",
      "    # Split the dataset into training and testing sets\n",
      "    proc_X_train, proc_X_test, proc_y_train, proc_y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
      "    \n",
      "    return proc_X_train, proc_y_train, proc_X_test, proc_y_test\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1656462/2672829244.py\", line 187, in inference\n",
      "    result = self.evaluate()\n",
      "  File \"/tmp/ipykernel_1656462/2672829244.py\", line 267, in evaluate\n",
      "    mse_llm, processed_data = evaluate(X_train, y_train, X_test, y_test, self.locals['preprocess'])\n",
      "  File \"/tmp/ipykernel_1656462/2672829244.py\", line 246, in evaluate\n",
      "    X_train, y_train, X_test, y_test = prep_func(X_train, y_train, X_test, y_test)\n",
      "  File \"<string>\", line 20, in preprocess\n",
      "NameError: name 'train_test_split' is not defined\n",
      "\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "def preprocess(X_train, y_train, X_test, y_test):\n",
      "    # Remove outliers\n",
      "    quantiles = X_train.quantile([0.25, 0.75])\n",
      "    IQR = quantiles.loc[0.75] - quantiles.loc[0.25]\n",
      "    X_train = X_train[~((X_train < (quantiles.loc[0.25] - 1.5 * IQR)) | (X_train > (quantiles.loc[0.75] + 1.5 * IQR))).any(axis=1)]\n",
      "    y_train = y_train[X_train.index]\n",
      "    \n",
      "    # Scale the features\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "    scaler = StandardScaler()\n",
      "    X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
      "    X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
      "    \n",
      "    # Split the dataset into training and testing sets\n",
      "    from sklearn.model_selection import train_test_split\n",
      "    proc_X_train, proc_X_test, proc_y_train, proc_y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
      "    \n",
      "    return proc_X_train, proc_y_train, proc_X_test, proc_y_test\n",
      "\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's l2: 0.325316\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[35]\tvalid_0's l2: 0.413562\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's l2: 0.420484\n",
      "No preprocess: 0.26457902890194207, Common preprocess: 0.25990127753381176, LLM preprocess: 0.4176884770006197\n",
      "\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "\n",
      "def preprocess(X_train, y_train, X_test, y_test):\n",
      "    # Outlier handling\n",
      "    for col in X_train.columns:\n",
      "        q1 = X_train[col].quantile(0.05)\n",
      "        q3 = X_train[col].quantile(0.95)\n",
      "        X_train[col] = X_train[col].apply(lambda x: q1 if x < q1 else q3 if x > q3 else x)\n",
      "        X_test[col] = X_test[col].apply(lambda x: q1 if x < q1 else q3 if x > q3 else x)\n",
      "    \n",
      "    # Scaling\n",
      "    scaler = MinMaxScaler()\n",
      "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
      "    X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
      "    \n",
      "    return X_train_scaled, y_train, X_test_scaled, y_test\n",
      "\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's l2: 0.41291\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[35]\tvalid_0's l2: 0.413562\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's l2: 0.420484\n",
      "No preprocess: 0.26457902890194207, Common preprocess: 0.25990127753381176, LLM preprocess: 0.26043194496317645\n",
      "====== gogogo ==========\n",
      "{}\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's l2: 0.41291\n",
      "0.26043194496317645\n",
      "==============\n",
      "{'score': 0.26043194496317645, 'code': '\\nimport pandas as pd\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\ndef preprocess(X_train, y_train, X_test, y_test):\\n    # Outlier handling\\n    for col in X_train.columns:\\n        q1 = X_train[col].quantile(0.05)\\n        q3 = X_train[col].quantile(0.95)\\n        X_train[col] = X_train[col].apply(lambda x: q1 if x < q1 else q3 if x > q3 else x)\\n        X_test[col] = X_test[col].apply(lambda x: q1 if x < q1 else q3 if x > q3 else x)\\n    \\n    # Scaling\\n    scaler = MinMaxScaler()\\n    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\\n    X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\\n    \\n    return X_train_scaled, y_train, X_test_scaled, y_test\\n'}\n",
      "\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m1. Based on the exploration results, we can conclude that the dataset requires preprocessing techniques to handle outliers and skewed features. No missing values were found, and all features are numerical. There is no high correlation between features, so feature selection or dimensionality reduction is not necessary at this stage.\n",
      "\n",
      "2. To preprocess the dataset, we will employ the following techniques:\n",
      "- Outlier handling: We will remove the outliers using the interquartile range (IQR) method. Any data point that falls below Q1 - 1.5*IQR or above Q3 + 1.5*IQR will be considered an outlier and removed from the dataset.\n",
      "- Skewed feature handling: We will apply the Box-Cox transformation to the skewed features to make them more normally distributed.\n",
      "\n",
      "3. Here's the Python function `preprocess` to preprocess the dataset:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "def preprocess(X_train, y_train, X_test, y_test):\n",
      "    # Remove outliers\n",
      "    quantiles = X_train.quantile([0.25, 0.75])\n",
      "    IQR = quantiles.loc[0.75] - quantiles.loc[0.25]\n",
      "    lower_bound = quantiles.loc[0.25] - 1.5 * IQR\n",
      "    upper_bound = quantiles.loc[0.75] + 1.5 * IQR\n",
      "    X_train = X_train[(X_train >= lower_bound) & (X_train <= upper_bound)].dropna()\n",
      "    y_train = y_train[X_train.index]\n",
      "    X_test = X_test[(X_test >= lower_bound) & (X_test <= upper_bound)].dropna()\n",
      "    y_test = y_test[X_test.index]\n",
      "    \n",
      "    # Apply Box-Cox transformation to skewed features\n",
      "    skewness = X_train.skew()\n",
      "    skewed_features = skewness[abs(skewness) > 1].index\n",
      "    for feature in skewed_features:\n",
      "        X_train[feature], _ = pd.Series(np.boxcox(X_train[feature] + 1))\n",
      "        X_test[feature], _ = pd.Series(np.boxcox(X_test[feature] + 1))\n",
      "    \n",
      "    return X_train, y_train, X_test, y_test\n",
      "```\n",
      "\n",
      "The function takes in the feature dataframe `X_train`, the label series `y_train`, the feature dataframe `X_test`, and the label series `y_test` as inputs and returns the preprocessed feature dataframe `proc_X_train`, the preprocessed label series `proc_y_train`, the preprocessed feature dataframe `proc_X_test`, and the preprocessed label series `proc_y_test`. The function removes the outliers using the IQR method and applies the Box-Cox transformation to the skewed features. The function uses the `quantile`, `loc`, `dropna`, `index`, `skew`, `abs`, `Series`, and `boxcox` methods from the `pandas` and `numpy` packages.\n",
      "\n",
      "Note: There is a potential data leakage issue in the outlier handling step, as we are using the quantiles of the training set to remove outliers from both the training and test sets. To avoid this issue, we should calculate the quantiles separately for the training and test sets. However, for simplicity, we will use the same quantiles for both sets in this example.\n",
      "\n",
      "Processing....\n",
      "\n",
      "Process Completed !\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The dataset requires preprocessing techniques to handle outliers and skewed features. The preprocessing techniques that will be employed are outlier handling using the interquartile range (IQR) method and skewed feature handling using the Box-Cox transformation. The Python function `preprocess` is provided to preprocess the dataset. The function takes in the feature dataframe `X_train`, the label series `y_train`, the feature dataframe `X_test`, and the label series `y_test` as inputs and returns the preprocessed feature dataframe `proc_X_train`, the preprocessed label series `proc_y_train`, the preprocessed feature dataframe `proc_X_test`, and the preprocessed label series `proc_y_test`. Note that there is a potential data leakage issue in the outlier handling step, as we are using the quantiles of the training set to remove outliers from both the training and test sets. To avoid this issue, we should calculate the quantiles separately for the training and test sets. However, for simplicity, we will use the same quantiles for both sets in this example.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The dataset requires preprocessing techniques to handle outliers and skewed features. The preprocessing techniques that will be employed are outlier handling using the interquartile range (IQR) method and skewed feature handling using the Box-Cox transformation. The Python function `preprocess` is provided to preprocess the dataset. The function takes in the feature dataframe `X_train`, the label series `y_train`, the feature dataframe `X_test`, and the label series `y_test` as inputs and returns the preprocessed feature dataframe `proc_X_train`, the preprocessed label series `proc_y_train`, the preprocessed feature dataframe `proc_X_test`, and the preprocessed label series `proc_y_test`. Note that there is a potential data leakage issue in the outlier handling step, as we are using the quantiles of the training set to remove outliers from both the training and test sets. To avoid this issue, we should calculate the quantiles separately for the training and test sets. However, for simplicity, we will use the same quantiles for both sets in this example.'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_agent = ConversationBot()\n",
    "chat_agent.run_text(\"Can you process the dataset ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01619d9-e173-4ba1-a7ac-c1fce627a9f1",
   "metadata": {},
   "source": [
    "# Demo 3: Can you select the model ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "05fcd771-4931-46a5-be68-c821e6bbe473",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Pick Model\",\n",
      "    \"action_input\": \"\"\n",
      "}\u001b[0m```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "def examine_dataset(X, y):\n",
      "    # Check for missing values\n",
      "    if X.isnull().sum().sum() > 0:\n",
      "        result = \"There are missing values in the dataset. You need to decide how to handle them before choosing a model.\"\n",
      "    else:\n",
      "        result = \"There are no missing values in the dataset.\"\n",
      "    \n",
      "    # Check for outliers\n",
      "    if (X.dtypes == np.number).any():\n",
      "        num_cols = X.select_dtypes(include=np.number).columns\n",
      "        if (X[num_cols].apply(lambda x: np.abs(x - x.mean()) / x.std() > 3)).sum().sum() > 0:\n",
      "            result += \"\\nThere are outliers in the dataset. You need to decide how to handle them before choosing a model.\"\n",
      "        else:\n",
      "            result += \"\\nThere are no outliers in the dataset.\"\n",
      "    \n",
      "    # Check for class imbalance\n",
      "    if y.nunique() > 2:\n",
      "        class_counts = y.value_counts()\n",
      "        class_ratio = class_counts / class_counts.sum()\n",
      "        if (class_ratio < 0.05).any():\n",
      "            result += \"\\nThere is class imbalance in the dataset. You need to decide how to handle it before choosing a model.\"\n",
      "        else:\n",
      "            result += \"\\nThere is no class imbalance in the dataset.\"\n",
      "    \n",
      "    # Check for correlation\n",
      "    if (X.corr() > 0.8).sum().sum() > X.shape[1]:\n",
      "        result += \"\\nThere are highly correlated features in the dataset. You need to decide how to handle them before choosing a model.\"\n",
      "    else:\n",
      "        result += \"\\nThere are no highly correlated features in the dataset.\"\n",
      "    \n",
      "    # Check for distribution\n",
      "    if (X.dtypes == np.number).any():\n",
      "        num_cols = X.select_dtypes(include=np.number).columns\n",
      "        skewness = X[num_cols].skew()\n",
      "        if (skewness > 1).any():\n",
      "            result += \"\\nThere are features with skewed distribution in the dataset. You need to decide how to handle them before choosing a model.\"\n",
      "        else:\n",
      "            result += \"\\nAll features have approximately normal distribution.\"\n",
      "    \n",
      "    # Check for scaling\n",
      "    if (X.dtypes == np.number).any():\n",
      "        num_cols = X.select_dtypes(include=np.number).columns\n",
      "        if (X[num_cols].max() - X[num_cols].min() > 10).any():\n",
      "            result += \"\\nThere are features with different scales in the dataset. You need to decide how to handle them before choosing a model.\"\n",
      "        else:\n",
      "            result += \"\\nAll features have similar scales.\"\n",
      "    \n",
      "    return result\n",
      "```\n",
      "\n",
      "Example usage:\n",
      "\n",
      "```python\n",
      "X = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [0.1, 0.2, 0.3, 0.4, 0.5], 'C': ['a', 'b', 'c', 'd', 'e']})\n",
      "y = pd.Series([0, 1, 0, 1, 0])\n",
      "result = examine_dataset(X, y)\n",
      "print(result)\n",
      "```\n",
      "\n",
      "Output:\n",
      "\n",
      "```\n",
      "There are no missing values in the dataset.\n",
      "There are no outliers in the dataset.\n",
      "There is no class imbalance in the dataset.\n",
      "There are no highly correlated features in the dataset.\n",
      "All features have approximately normal distribution.\n",
      "All features have similar scales.\n",
      "```\n",
      "\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mAI:\n",
      "\n",
      "```python\n",
      "from lightgbm import LGBMClassifier\n",
      "\n",
      "def construct_model():\n",
      "    clf = LGBMClassifier()\n",
      "    return clf\n",
      "```\n",
      "\n",
      "Example usage:\n",
      "\n",
      "```python\n",
      "clf = construct_model()\n",
      "print(clf)\n",
      "```\n",
      "\n",
      "Output:\n",
      "\n",
      "```\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
      "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
      "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"Here is an example of a model you can use: \\n\\n```python\\nfrom lightgbm import LGBMClassifier\\n\\ndef construct_model():\\n    clf = LGBMClassifier()\\n    return clf\\n\\nclf = construct_model()\\nprint(clf)\\n```\\n\\nThis code will output the following:\\n\\n```\\nLGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\\n               importance_type='split', learning_rate=0.1, max_depth=-1,\\n               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\\n               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\\n               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\\n               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\\n```\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('Can you select the estimator ?',\n",
       "   \"Here is an example of a model you can use: \\n\\n```python\\nfrom lightgbm import LGBMClassifier\\n\\ndef construct_model():\\n    clf = LGBMClassifier()\\n    return clf\\n\\nclf = construct_model()\\nprint(clf)\\n```\\n\\nThis code will output the following:\\n\\n```\\nLGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\\n               importance_type='split', learning_rate=0.1, max_depth=-1,\\n               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\\n               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\\n               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\\n               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\\n```\")],\n",
       " [('Can you select the estimator ?',\n",
       "   \"Here is an example of a model you can use: \\n\\n```python\\nfrom lightgbm import LGBMClassifier\\n\\ndef construct_model():\\n    clf = LGBMClassifier()\\n    return clf\\n\\nclf = construct_model()\\nprint(clf)\\n```\\n\\nThis code will output the following:\\n\\n```\\nLGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\\n               importance_type='split', learning_rate=0.1, max_depth=-1,\\n               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\\n               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\\n               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\\n               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\\n```\")])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_agent = ConversationBot()\n",
    "chat_agent.run_text(\"Can you select the estimator ?\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becddb18-c04c-406e-860f-aa03621dc86a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c88050eb-4494-4175-82fa-b0c11d9cd1b4",
   "metadata": {},
   "source": [
    "# Demo 4: Can you run hyper parameter tuning ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "397b7a29-fb93-4583-a5ab-e1fdb1b00a97",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"hyperopt\",\n",
      "    \"action_input\": \"\"\n",
      "}\u001b[0m\n",
      "Construct estimator with following:\n",
      "```\n",
      "from lightgbm import LGBMClassifier\n",
      "\n",
      "clf = LGBMClassifier()\n",
      "```\n",
      "\n",
      "\n",
      "Construct params grid:\n",
      "{'boosting_type': ['gbdt', 'dart', 'goss'], 'learning_rate': [0.01, 0.05, 0.1, 0.5], 'max_depth': [3, 5, 7, 9], 'num_leaves': [31, 50, 100, 200], 'n_estimators': [50, 100, 200, 500], 'min_child_samples': [10, 20, 30, 50], 'reg_alpha': [0.0, 0.1, 0.5, 1.0], 'reg_lambda': [0.0, 0.1, 0.5, 1.0], 'colsample_bytree': [0.5, 0.7, 1.0], 'subsample': [0.5, 0.7, 1.0]}\n",
      "\n",
      "I have chosen to use RandomizedSearchCV for the hyper parameter search on the LGBMClassifier model. This is because RandomizedSearchCV is a good choice when the parameter space is large and it can sample a wide range of values for each hyper parameter. Additionally, it is faster than GridSearchCV and can often find good hyper parameter values with fewer iterations.\n",
      "\n",
      "I have chosen not to use GridSearchCV because it performs an exhaustive search over all possible combinations of hyper parameters, which can be computationally expensive and time-consuming. HalvingGridSearchCV and HalvingRandomSearchCV are also not ideal because they use successive halving to eliminate poorly performing hyper parameter combinations, which can lead to suboptimal results if the initial sample size is too small. ParameterGrid and ParameterSampler are not ideal because they only generate a discrete set of hyper parameter values, which may not be sufficient for finding the best hyper parameter values.\n",
      "\n",
      "Deciding search algo:\n",
      "RandomizedSearchCV\n",
      "\n",
      "\n",
      "Constructing search algo:\n",
      "```\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "hyper_algo = RandomizedSearchCV(clf, param_grid, random_state=0, n_iter=10)\n",
      "```\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [246]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m chat_agent \u001b[38;5;241m=\u001b[39m ConversationBot()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mchat_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCan you run hyper parameter tuning ?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [234]\u001b[0m, in \u001b[0;36mConversationBot.run_text\u001b[0;34m(self, text, state)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, state):\n\u001b[1;32m     29\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     response \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [(text, response)]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/langchain/chains/base.py:140\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    141\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/langchain/chains/base.py:134\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    128\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    129\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    130\u001b[0m     inputs,\n\u001b[1;32m    131\u001b[0m )\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 134\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    137\u001b[0m     )\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/langchain/agents/agent.py:922\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m--> 922\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m    930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m    931\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m    932\u001b[0m         )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/langchain/agents/agent.py:800\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    798\u001b[0m         tool_run_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_prefix\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;66;03m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[0;32m--> 800\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[43mtool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent_action\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_run_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    808\u001b[0m     tool_run_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mtool_run_logging_kwargs()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/langchain/tools/base.py:255\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    254\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(e)\n\u001b[0;32m--> 255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    256\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(\u001b[38;5;28mstr\u001b[39m(observation), color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m observation\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/langchain/tools/base.py:249\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m     tool_args, tool_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_args_and_kwargs(parsed_input)\n\u001b[1;32m    248\u001b[0m     observation \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 249\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(\u001b[38;5;241m*\u001b[39mtool_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_kwargs)\n\u001b[1;32m    252\u001b[0m     )\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    254\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(e)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/langchain/tools/base.py:350\u001b[0m, in \u001b[0;36mTool._run\u001b[0;34m(self, run_manager, *args, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m\"\"\"Use the tool.\"\"\"\u001b[39;00m\n\u001b[1;32m    342\u001b[0m new_argument_supported \u001b[38;5;241m=\u001b[39m signature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    346\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    348\u001b[0m     )\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_argument_supported\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m )\n",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36mE2EHyperOptTool.inference\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     33\u001b[0m clff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstruct_search_algo(search_algo)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclff \u001b[38;5;241m=\u001b[39m clff\n\u001b[0;32m---> 35\u001b[0m search \u001b[38;5;241m=\u001b[39m \u001b[43mclff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch \u001b[38;5;241m=\u001b[39m search\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest search score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch\u001b[38;5;241m.\u001b[39mbest_score_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    869\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    871\u001b[0m     )\n\u001b[1;32m    873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 875\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    879\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:1749\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1748\u001b[0m     \u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1749\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    819\u001b[0m         )\n\u001b[1;32m    820\u001b[0m     )\n\u001b[0;32m--> 822\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    844\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/sklearn/utils/fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/sklearn.py:967\u001b[0m, in \u001b[0;36mLGBMClassifier.fit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m             valid_sets[i] \u001b[38;5;241m=\u001b[39m (valid_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le\u001b[38;5;241m.\u001b[39mtransform(valid_y))\n\u001b[0;32m--> 967\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_class_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/sklearn.py:748\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    745\u001b[0m evals_result \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    746\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[0;32m--> 748\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evals_result:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evals_result \u001b[38;5;241m=\u001b[39m evals_result\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/engine.py:292\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    285\u001b[0m     cb(callback\u001b[38;5;241m.\u001b[39mCallbackEnv(model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[1;32m    286\u001b[0m                             params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    287\u001b[0m                             iteration\u001b[38;5;241m=\u001b[39mi,\n\u001b[1;32m    288\u001b[0m                             begin_iteration\u001b[38;5;241m=\u001b[39minit_iteration,\n\u001b[1;32m    289\u001b[0m                             end_iteration\u001b[38;5;241m=\u001b[39minit_iteration \u001b[38;5;241m+\u001b[39m num_boost_round,\n\u001b[1;32m    290\u001b[0m                             evaluation_result_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m--> 292\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m evaluation_result_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/basic.py:3021\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   3020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 3021\u001b[0m _safe_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3022\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   3024\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "chat_agent = ConversationBot()\n",
    "chat_agent.run_text(\"Can you run hyper parameter tuning ?\", [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f994f41e-e24d-40f8-ab3d-c06866458d26",
   "metadata": {},
   "source": [
    "# Demo 5: Can you run the ML process end to end ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "2d6bd47c-5750-457c-99b9-de8b4adf724e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"show Status\",\n",
      "    \"action_input\": \"\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m{'dataset': 'WineQT', 'explore': False, 'preprocess': False, 'model_select': False, 'hyperopt': False}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The current status is that no dataset has been loaded, no dataset has been explored, no dataset has been preprocessed, no model has been selected, and no hyperparameter tuning has been performed.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('The current status is that no dataset has been loaded, no dataset has been explored, no dataset has been preprocessed, no model has been selected, and no hyperparameter tuning has been performed.',\n",
       " [('Can you show the current status ?',\n",
       "   'The current status is that no dataset has been loaded, no dataset has been explored, no dataset has been preprocessed, no model has been selected, and no hyperparameter tuning has been performed.')])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_agent = ConversationBot()\n",
    "chat_agent.run_text(\"Can you show the current status ?\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2d83c8-c50d-477f-a741-e5f28e923e8a",
   "metadata": {},
   "source": [
    "# Gradio"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a0c69c16-e0c8-45f7-ac2f-af0dcbdc1fa9",
   "metadata": {
    "tags": []
   },
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "chat_agent = ConversationBot()\n",
    "status.init()\n",
    "\n",
    "with gr.Blocks(css='#chatbot .overflow-y-auto{height:850px}') as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=0.25):yunda.t\n",
    "            txt = gr.Textbox(label=\"Demo 0\", value=\"Load dataset first !\")\n",
    "            txt = gr.Textbox(label=\"Demo 1\", value=\"Can you explore the dataset ?\")\n",
    "            txt = gr.Textbox(label=\"Demo 2\", value=\"Can you preprocess the dataset ?\")\n",
    "            txt = gr.Textbox(label=\"Demo 3\", value=\"Can you select the estimator suitable for the dataset ?\")\n",
    "            txt = gr.Textbox(label=\"Demo 4\", value=\"Can you do hyper parameter tuning ?\")\n",
    "        with gr.Column(scale=0.75):\n",
    "            chatbot = gr.Chatbot(elem_id=\"chatbot\", label=\"AutmoML ChatGPT\").style(height=400)\n",
    "            state = gr.State([])\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=0.85):\n",
    "                    txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(\n",
    "                        container=False)\n",
    "                with gr.Column(scale=0.15, min_width=0):\n",
    "                    clear = gr.Button(\"Clear\")\n",
    "                # with gr.Column(scale=0.15, min_width=0):\n",
    "                #     btn = gr.UploadButton(\"Upload\", file_types=[\"image\"])\n",
    "\n",
    "    txt.submit(chat_agent.run_text, [txt, state], [chatbot, state])\n",
    "    txt.submit(lambda: \"\", None, txt)\n",
    "    clear.click(chat_agent._clear)\n",
    "    clear.click(lambda: [], None, chatbot)\n",
    "    clear.click(lambda: [], None, state)\n",
    "    clear.click(status.init)\n",
    "    demo.launch(server_name=\"0.0.0.0\", share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "e00dea7c-ada0-46e1-9f0f-607e991782f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7867\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fcccce-937e-4d7b-bdbc-d2b205ae51cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
