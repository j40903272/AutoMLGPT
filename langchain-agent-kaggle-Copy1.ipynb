{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b632fbe-16ae-4812-a408-9b375a577623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, Union, List, Dict, Tuple\n",
    "\n",
    "# import torch\n",
    "import asyncio\n",
    "import logging\n",
    "from functools import wraps, partial\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed_all(seed)\n",
    "    return seed\n",
    "\n",
    "\n",
    "def prompts(name: str, description: str):\n",
    "    def decorator(func):\n",
    "        func.name = name\n",
    "        func.description = description\n",
    "        return func\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def extract_code(text):\n",
    "    texts = text.split(\"```\")\n",
    "    code = None\n",
    "    if len(texts) > 1:\n",
    "        code = texts[1]\n",
    "        code = code.strip(\"python\").strip(\"Python\")\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "254de8d7-b2a4-4e4a-8ea0-ac1f804d349b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Union, List, Dict, Tuple\n",
    "from pydantic import Field\n",
    "import ast\n",
    "import astunparse\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def prompts(name: str, description: str):\n",
    "    def decorator(func):\n",
    "        func.name = name\n",
    "        func.description = description\n",
    "        return func\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def execute(cmd: str, glo, loc):\n",
    "    tree = ast.parse(cmd)\n",
    "    module = ast.Module(tree.body[:-1], type_ignores=[])\n",
    "    exec(astunparse.unparse(module), self.globals, self.locals)  # type: ignore\n",
    "    module_end = ast.Module(tree.body[-1:], type_ignores=[])\n",
    "    module_end_str = astunparse.unparse(module_end)  # type: ignore\n",
    "    try:\n",
    "        return eval(module_end_str, self.globals, self.locals)\n",
    "    except Exception:\n",
    "        exec(module_end_str, self.globals, self.locals)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d2b53ea-2062-4c56-b75f-45d0d51392d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['OPENAI_API_KEY'] = \"\"\n",
    "os.environ['OPENAI_API_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fa12ef6-f654-4fab-9e45-877bb83df79a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import logging\n",
    "import threading\n",
    "from typing import Union\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from langchain.chains.conversation.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    # ConversationBufferWindowMemory,\n",
    "    # ConversationSummaryBufferMemory\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents.tools import Tool\n",
    "from langchain.agents.conversational.output_parser import ConvoOutputParser\n",
    "from langchain.agents.conversational.prompt import FORMAT_INSTRUCTIONS\n",
    "from langchain.agents.conversational_chat.prompt import (\n",
    "    FORMAT_INSTRUCTIONS as CHAT_FORMAT_INSTRUCTIONS,\n",
    ")\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "from langchain.agents import AgentOutputParser\n",
    "\n",
    "# from api.services.utils import prompts, seed_everything, to_async\n",
    "# from api.conversation.callback import AsyncLoggerCallbackHandler, LoggerCallbackHandler\n",
    "\n",
    "\n",
    "\n",
    "class retryActionTool:\n",
    "    \"\"\"Retry when chatgpt response parsing failed.\"\"\"\n",
    "\n",
    "    @prompts(\n",
    "        name=\"RR\",\n",
    "        description=\"Do not use it.\",\n",
    "    )\n",
    "    def inference(self, query):\n",
    "        return f\"Modify the following text in ``` such that it will follow RESPONSE FORMAT INSTRUCTIONS:```{query}```\\nDo not use retry tool again.\"\n",
    "\n",
    "\n",
    "class RetryConvoOutputParser(ConvoOutputParser):\n",
    "    ai_prefix: str = \"AI\"\n",
    "\n",
    "    def get_format_instructions(self) -> str:\n",
    "        return FORMAT_INSTRUCTIONS\n",
    "\n",
    "    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\n",
    "        if f\"{self.ai_prefix}:\" in text:\n",
    "            return AgentFinish(\n",
    "                {\"output\": text.split(f\"{self.ai_prefix}:\")[-1].strip()}, text\n",
    "            )\n",
    "        regex = r\"Action: (.*?)[\\n]*Action Input: (.*)\"\n",
    "        match = re.search(regex, text)\n",
    "        if not match:\n",
    "            return AgentAction(\"retry\", \"\", text)\n",
    "            # raise OutputParserException(f\"Could not parse LLM output: `{text}`\")\n",
    "        action = match.group(1)\n",
    "        action_input = match.group(2)\n",
    "        return AgentAction(action.strip(), action_input.strip(\" \").strip('\"'), text)\n",
    "\n",
    "\n",
    "class RobustRetryConvoOutputParser(AgentOutputParser):\n",
    "    def get_format_instructions(self) -> str:\n",
    "        return CHAT_FORMAT_INSTRUCTIONS\n",
    "\n",
    "    def parse_json(self, text: str):\n",
    "        try:\n",
    "            return [\n",
    "                i\n",
    "                for i in re.findall(r\"\\s([{\\[].*?[}\\]])$\", f\" {text}\", flags=re.DOTALL)\n",
    "                if i\n",
    "            ][0]\n",
    "        except Exception:\n",
    "            return\n",
    "\n",
    "    def parse_markdown_code(self, text: str):\n",
    "        try:\n",
    "            for i in re.findall(\n",
    "                r\"`{3}([\\w]*)\\n([\\S\\s]+?)\\n`{3}\", text, flags=re.DOTALL\n",
    "            )[0]:\n",
    "                if i:\n",
    "                    tmp = self.parse_json(i)\n",
    "                    if tmp:\n",
    "                        return tmp\n",
    "        except Exception:\n",
    "            return\n",
    "\n",
    "    def parse_origin(self, text: str):\n",
    "        try:\n",
    "            cleaned_output = text.strip()\n",
    "            if \"```json\" in cleaned_output:\n",
    "                _, cleaned_output = cleaned_output.split(\"```json\")\n",
    "            if \"```\" in cleaned_output:\n",
    "                cleaned_output, _ = cleaned_output.split(\"```\")\n",
    "            if cleaned_output.startswith(\"```json\"):\n",
    "                cleaned_output = cleaned_output[len(\"```json\"):]\n",
    "            if cleaned_output.startswith(\"```\"):\n",
    "                cleaned_output = cleaned_output[len(\"```\"):]\n",
    "            if cleaned_output.endswith(\"```\"):\n",
    "                cleaned_output = cleaned_output[: -len(\"```\")]\n",
    "            return cleaned_output.strip()\n",
    "        except Exception:\n",
    "            return\n",
    "\n",
    "    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\n",
    "        try:\n",
    "            json_str = self.parse_json(text)\n",
    "            markdown_str = self.parse_markdown_code(text)\n",
    "            origin_str = self.parse_origin(text)\n",
    "\n",
    "            if json_str:\n",
    "                cleaned_output = json_str\n",
    "            elif markdown_str:\n",
    "                cleaned_output = markdown_str\n",
    "                if cleaned_output.startswith(\"```json\"):\n",
    "                    cleaned_output = cleaned_output[len(\"```json\"):]\n",
    "                if cleaned_output.startswith(\"```\"):\n",
    "                    cleaned_output = cleaned_output[len(\"```\"):]\n",
    "                if cleaned_output.endswith(\"```\"):\n",
    "                    cleaned_output = cleaned_output[: -len(\"```\")]\n",
    "            elif origin_str:\n",
    "                cleaned_output = origin_str\n",
    "            else:\n",
    "                cleaned_output = text.strip()\n",
    "\n",
    "            response = json.loads(cleaned_output)\n",
    "            action, action_input = response[\"action\"], response[\"action_input\"]\n",
    "            if action == \"Final Answer\":\n",
    "                return AgentFinish({\"output\": action_input}, text)\n",
    "            else:\n",
    "                return AgentAction(action, action_input, text)\n",
    "        except Exception:\n",
    "            if \"action\" not in text and \"action_input\" not in text:\n",
    "                return AgentFinish({\"output\": text}, text)\n",
    "            else:\n",
    "                logger.warning(f\"Not follow format instruction\\n{cleaned_output}\")\n",
    "                return AgentAction(\"RR\", \"\", text)\n",
    "\n",
    "        \n",
    "class pythonShellTool:\n",
    "\n",
    "    @prompts(\n",
    "        name=\"python_repl_ast\",\n",
    "        description=\"This is a python shell. Use this to execute python commands. \"\n",
    "        \"Input should be a valid python command. \"\n",
    "        \"The output of the command should not be too long.\"\n",
    "        \"The dataset is a dataframe assign to a variable df\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        try:\n",
    "            tree = ast.parse(query)\n",
    "            module = ast.Module(tree.body[:-1], type_ignores=[])\n",
    "            exec(astunparse.unparse(module), self.globals, self.locals)  # type: ignore\n",
    "            module_end = ast.Module(tree.body[-1:], type_ignores=[])\n",
    "            module_end_str = astunparse.unparse(module_end)  # type: ignore\n",
    "            try:\n",
    "                return eval(module_end_str, self.globals, self.locals)[:1024]\n",
    "            except Exception:\n",
    "                exec(module_end_str, self.globals, self.locals)\n",
    "                return \"\"\n",
    "        except Exception as e:\n",
    "            return str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4830633-5499-44f3-bbc5-1741353fcb17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts.chat import (\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "human_message_prompt = HumanMessagePromptTemplate(\n",
    "        prompt=PromptTemplate(\n",
    "            template=\"{text}\",\n",
    "            input_variables=[\"text\"],\n",
    "        )\n",
    "    )\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt_template)#, memory=ConversationBufferMemory())\n",
    "\n",
    "\n",
    "\n",
    "class CodingAgent():\n",
    "    def __init__(self, chain):\n",
    "        self.chain = chain\n",
    "    @functools.lru_cache(100)\n",
    "    def run(self, text):\n",
    "        return self.chain.run(text)\n",
    "    \n",
    "    \n",
    "coding_agent = CodingAgent(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef371d4b-adea-4524-bbfe-d2635cbaa7f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from prompt import AUTOML_CHATGPT_PREFIX, AUTOML_CHATGPT_FORMAT_INSTRUCTIONS, AUTOML_CHATGPT_SUFFIX\n",
    "from langchain.agents.initialize import initialize_agent\n",
    "from langchain.agents.tools import Tool\n",
    "from sklearn.svm import LinearSVR\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from scipy.stats import boxcox\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96a6cdbe-3beb-4b45-8f19-50803da3436b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Status():\n",
    "    \n",
    "    def __init__(self, dev=True):\n",
    "        self.init()\n",
    "        self.dev = dev\n",
    "        \n",
    "    def init_data(self, dataset, X_train, X_test, y_train, y_test):\n",
    "        self.init()\n",
    "        self.dataset = dataset\n",
    "        self.X = X_train\n",
    "        self.y = y_train\n",
    "        \n",
    "        if self.dev:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.1, random_state=42)\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "    \n",
    "    def init(self):\n",
    "        \n",
    "        self.dataset = \"\"\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.y_pred = None\n",
    "        \n",
    "        self.start_explore_prompt = \"\"\n",
    "        self.how_to_explore_prompt = \"\"\n",
    "        self.explore_data_thought = \"\"\n",
    "        self.hyperopt_param_thought = \"\"\n",
    "        \n",
    "        self.explore_result = \"Nothing special.\"\n",
    "        \n",
    "        self.explore = False\n",
    "        self.preprocess = False\n",
    "        self.model_select = False\n",
    "        self.hyperopt = False\n",
    "        \n",
    "        self.explore_code = \"\"\n",
    "        self.preprocess_code = \"\"\n",
    "        self.model_select_code = \"\"\n",
    "        self.hyperopt_code = \"\"\n",
    "        \n",
    "        self.model = \"\"\n",
    "        self.best_param = {}\n",
    "\n",
    "status = Status(dev=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf955478-9990-468d-bbb0-b5573c4470eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['santander-customer-satisfaction', 'porto-seguro-safe-driver-prediction', 'santander-customer-transaction-prediction', 'house-prices-advanced-regression-techniques', 'santander-value-prediction-challenge', 'bnp-paribas-cardif-claims-management', 'ieee-fraud-detection', 'allstate-claims-severity', 'otto-group-product-classification-challenge', 'mercedes-benz-greener-manufacturing', '.ipynb_checkpoints']\n",
      "[Errno 2] No such file or directory: 'kaggle/ieee-fraud-detection/train.csv'\n",
      "None complete.\n",
      "house-prices-advanced-regression-techniques complete.\n",
      "mercedes-benz-greener-manufacturing complete.[Errno 2] No such file or directory: 'kaggle/.ipynb_checkpoints/train.csv'\n",
      "\n",
      "None complete.\n",
      "otto-group-product-classification-challenge complete.\n",
      "allstate-claims-severity complete.\n",
      "bnp-paribas-cardif-claims-management complete.\n",
      "porto-seguro-safe-driver-prediction complete.\n",
      "santander-customer-satisfaction complete.\n",
      "santander-customer-transaction-prediction complete.\n",
      "santander-value-prediction-challenge complete.\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "class DataLoader():\n",
    "    \n",
    "    def __init__(self, chain):\n",
    "        self.datasets = {}\n",
    "        self.chain = chain\n",
    "        \n",
    "    def select(self, name: str):\n",
    "        if name not in self.datasets:\n",
    "            prompt = f\"\"\"\n",
    "            These are the datasets we have:\n",
    "            {self.datasets.keys()}\n",
    "            If someone ask for {name}, what is the most possible dataset that is referred to ?\n",
    "            Give me the response in one single word:\n",
    "            \"\"\"\n",
    "            print(prompt)\n",
    "            name = self.chain.run(prompt)\n",
    "            return self.datasets[name]\n",
    "        else:\n",
    "            return self.datasets[name]\n",
    "        \n",
    "    def show(self):\n",
    "        print(self.datasets.keys())\n",
    "        \n",
    "    def load_doc(self, dirn: str):\n",
    "        try:\n",
    "            train_path = os.path.join('kaggle', dirn, 'train.csv')\n",
    "            train_df = pd.read_csv(train_path)\n",
    "\n",
    "            test_path = os.path.join('kaggle', dirn, 'test.csv')\n",
    "            test_df = pd.read_csv(test_path)\n",
    "\n",
    "            label = list(set(train_df.columns) - set(test_df.columns))[0]\n",
    "\n",
    "            self.datasets[dirn] = {\n",
    "                'train': train_df,\n",
    "                'test': test_df,\n",
    "                'label': label,\n",
    "                'name': dirn\n",
    "            }\n",
    "            return dirn\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    def load(self):\n",
    "        dir_list = []\n",
    "        for dirn in os.listdir('kaggle'):\n",
    "            if not os.path.isdir(os.path.join('kaggle', dirn)):\n",
    "                continue\n",
    "            dir_list.append(dirn)\n",
    "    \n",
    "        print(dir_list)\n",
    "        with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "            future_to_url = [executor.submit(self.load_doc, dirn) for dirn in dir_list]\n",
    "            for future in as_completed(future_to_url):\n",
    "                dirn = future.result()\n",
    "                print(f\"{dirn} complete.\")\n",
    "                \n",
    "                \n",
    "data_loader = DataLoader(chain)\n",
    "data_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f18ad817-5d64-4df1-9789-dc44c181ded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadDataTool:\n",
    "    globals: Optional[Dict] = dict()\n",
    "    locals: Optional[Dict] = dict()\n",
    "\n",
    "    def __init__(self, status):\n",
    "        self.status = status\n",
    "        \n",
    "    @prompts(\n",
    "        name=\"Load\",\n",
    "        description=\"This is a tool to load dataset.\"\n",
    "        \"Input should be a dataset name.\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        \n",
    "        if self.status.dev:\n",
    "            name = \"WineQT\"\n",
    "            data = pd.read_csv(\"WineQT.csv\")\n",
    "            X = data.drop('quality', axis=1)\n",
    "            y = data['quality']\n",
    "            self.status.init_data(name, X, y, None, None)\n",
    "        else:\n",
    "            dataset = data_loader.select(query)\n",
    "            name = dataset['name']\n",
    "            label = dataset['label']\n",
    "            data = dataset['train']\n",
    "            X_train = data.drop(label, axis=1)\n",
    "            y_train = data[label]\n",
    "            data = dataset['test']\n",
    "            if label in data:\n",
    "                data = data.drop(label, axis=1)\n",
    "            X_test = data\n",
    "            y_test = pd.DataFrame(data={label: [0]*len(X_test)})[label]\n",
    "            self.status.init_data(name, X_train, X_test, y_train, y_test)\n",
    "            print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "        \n",
    "        return f\"Load dataset {name} success !\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "579f8fa8-72a1-48a5-bb92-f1df1c911907",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ExploreDataTool:\n",
    "    globals: Optional[Dict] = dict()\n",
    "    locals: Optional[Dict] = dict()\n",
    "\n",
    "    def __init__(self, status):\n",
    "        self.status = status\n",
    "        self.X = status.X\n",
    "        self.y = status.y\n",
    "        self.locals = {\"X\": self.X, 'y': self.y, 'np': np}\n",
    "        self.globals = {\"np\": np, 'pd': pd}\n",
    "        \n",
    "    @prompts(\n",
    "        name=\"Explore\",\n",
    "        description=\"This is a tool for dataset exploring.\"\n",
    "        \"Useful when the user ask for you to explore the dataset.\"\n",
    "        \"Output is the thought from exploring the dataset.\"\n",
    "        \"Input should be an empty string.\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        \n",
    "        if not self.status.dataset:\n",
    "            return \"Pick a dataset first.\"\n",
    "        \n",
    "        start_explore_prompt = f\"\"\"\n",
    "You are a data scientist. You are now performing data analysis before training a machine learning model.\n",
    "Your task is to tell me how the data could be preprocessed and to give me the corresponding Python code.\n",
    "The preprocessing includes, but is not limited to, missing value handling, scaling, normalization, or encoding.\n",
    "The task will be performed in two stages.\n",
    "In the first stage, you will decide which techniques are required to preprocess the dataset by investigating the dataset.\n",
    "In the second stage, you will preprocess the dataset by implementing the functions.\n",
    "\n",
    "Here is some information about the feature dataframe `X`:\n",
    "\\\"\\\"\\\"\n",
    "- First five rows:\n",
    "{self.X.head(5)}\n",
    "\n",
    "- Column types:\n",
    "{self.X.dtypes}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Here is some information about the label series `y`:\n",
    "\\\"\\\"\\\"\n",
    "- First five rows:\n",
    "{self.y.head(5)}\n",
    "\n",
    "- Column types:\n",
    "{self.y.dtypes}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "        how_to_explore_prompt = \"\"\"\n",
    "Now, let's move on to the first stage:\n",
    "1. Explain how you will explore the dataset in order to select appropriate preprocessing techniques.\n",
    "2. Write a Python function called `explore` to explore the dataset.\n",
    "   Import any package you need beforehand.\n",
    "   Do not print a long list.\n",
    "   The function will be called using `result = explore(X, y)`, and all findings must be stored in a string variable `result`.\n",
    "\n",
    "Here are some handy packages in the Python environment:\n",
    "```\n",
    "numpy==1.24.3\n",
    "pandas==2.0.1\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "        prompt = \"\\n\\n\".join([start_explore_prompt, how_to_explore_prompt])\n",
    "        self.start_explore_prompt = start_explore_prompt\n",
    "        self.how_to_explore_prompt = how_to_explore_prompt\n",
    "        # print(prompt)\n",
    "\n",
    "        explore_data_thought = coding_agent.run(prompt)\n",
    "        self.explore_data_thought = explore_data_thought\n",
    "        # print(explore_data_thought)\n",
    "        \n",
    "        explore_code = extract_code(explore_data_thought)\n",
    "        self.explore_code = explore_code\n",
    "        # print(explore_code + '\\n')\n",
    "        self.retry_execute(explore_data_thought, explore_code)\n",
    "        \n",
    "        explore_data_result = self.locals[\"explore\"](self.X, self.y)\n",
    "        self.explore_data_result = explore_data_result\n",
    "        # print(explore_data_result)\n",
    "        \n",
    "        self.update_status()\n",
    "        return explore_data_result\n",
    "    \n",
    "    def update_status(self):\n",
    "        self.status.start_explore_prompt = self.start_explore_prompt\n",
    "        self.status.how_to_explore_prompt = self.how_to_explore_prompt\n",
    "        self.status.explore_code = self.explore_code\n",
    "        self.status.explore_data_result = self.explore_data_result\n",
    "        self.status.explore_data_thought = self.explore_data_thought\n",
    "        self.status.explore = True\n",
    "    \n",
    "    def retry_execute(self, prompt: str, code: str, retry: int = 3):\n",
    "        for _ in range(retry):\n",
    "            try:\n",
    "                exec(code, self.globals, self.locals)\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "                fix_code = coding_agent.run(f\"{prompt}\\n\\nExecute result:\\n{traceback.format_exc()}\\n\\nModify the code and fix the error.\")\n",
    "                code = extract_code(fix_code)\n",
    "                print(f\"Fix{_}:\\n{code}\\n\")\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "class ProcessDataTool:\n",
    "    globals: Optional[Dict] = dict()\n",
    "    locals: Optional[Dict] = dict()\n",
    "\n",
    "    def __init__(self, status):\n",
    "        self.status = status\n",
    "        self.X = status.X\n",
    "        self.y = status.y\n",
    "        self.locals = {\"X\": self.X, 'y': self.y, 'np': np}\n",
    "        self.globals = {\n",
    "            \"np\": np,\n",
    "            'pd': pd,\n",
    "        }\n",
    "        \n",
    "        from scipy import stats\n",
    "        from imblearn import over_sampling, under_sampling\n",
    "        from sklearn import preprocessing, impute, model_selection, feature_selection, compose\n",
    "        for module in [preprocessing, impute, over_sampling, under_sampling, model_selection, feature_selection, compose, stats]:\n",
    "            for name, func in vars(module).items():\n",
    "                if name[0].isupper():\n",
    "                    self.globals[name] = func\n",
    "\n",
    "    @prompts(\n",
    "        name=\"Preprocess\",\n",
    "        description=\"This is a tool for preprocess the dataset.\"\n",
    "        \"Useful when you want to process the dataset.\"\n",
    "        \"Useful when you want to preprocess the dataset.\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        if not self.status.dataset:\n",
    "            return \"Pick a dataset first.\"\n",
    "        print()\n",
    "        \n",
    "        explore_data_result = self.status.explore_data_result\n",
    "        start_explore_prompt = self.status.start_explore_prompt\n",
    "        how_to_explore_prompt = self.status.how_to_explore_prompt\n",
    "        explore_data_thought = self.status.explore_data_thought\n",
    "\n",
    "        preprocess_prompt = \"\"\"\n",
    "        The exploration results from the first stage are shown above.\n",
    "        Now, let's move on to the second stage:\n",
    "        1. Provide a conclusion based on the exploration results.\n",
    "        2. Based on the conclusion, specify the preprocessing techniques to be employed for the dataset and briefly describe their implementations.\n",
    "        3. Write a Python function called `preprocess` that will assist you in preprocessing the data.\n",
    "           Keep the function as simple as possible.\n",
    "           Import any package you need beforehand.\n",
    "           The function will be called using `proc_X_train, proc_y_train, proc_X_test, proc_y_test = preprocess(X_train, y_train, X_test, y_test)`, \n",
    "           where both the parameters and return values are eithor Pandas dataframes or series.\n",
    "           Be aware of potential data leakage issues.\n",
    "\n",
    "        These are the only packages in the Python environment:\n",
    "        ```\n",
    "        numpy==1.24.3\n",
    "        pandas==2.0.1\n",
    "        ```\n",
    "        No additional packages can be used. Return results about the second stage.\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        prompt = \"\\n\\n\".join([\n",
    "            # start_explore_prompt,\n",
    "            # how_to_explore_prompt,\n",
    "            f\"Your Response:\\n{explore_data_thought}\",\n",
    "            f\"Code Execution Results:\\n{explore_data_result}\",\n",
    "            preprocess_prompt]\n",
    "        )\n",
    "        preprocess_data_thought = coding_agent.run(prompt)\n",
    "        self.preprocess_data_thought = preprocess_data_thought\n",
    "        # print(preprocess_data_thought)\n",
    "        preprocess_code = extract_code(preprocess_data_thought)\n",
    "        self.preprocess_code = preprocess_code\n",
    "        # print(preprocess_code)\n",
    "        \n",
    "        best_result = {\n",
    "            'score': 100,\n",
    "            'code': preprocess_code\n",
    "        }\n",
    "        \n",
    "        for _ in range(5):\n",
    "            try:\n",
    "                print(preprocess_code)\n",
    "                exec(preprocess_code, self.globals, self.locals)\n",
    "                result = self.evaluate()\n",
    "                \n",
    "                if result['llm'] < best_result['score']:\n",
    "                    best_result['score'] = result['llm']\n",
    "                    best_result['code'] = preprocess_code\n",
    "                    \n",
    "                    self.status.X_train = result['processed_data'][0]\n",
    "                    self.status.X_test = result['processed_data'][2]\n",
    "                    self.status.y_train = result['processed_data'][1]\n",
    "                    self.status.y_test = result['processed_data'][3]\n",
    "                \n",
    "                fix_code = coding_agent.run(f\"{prompt}\\n\\nExecute result:\\n{result['msg']}\\n\\nYour result is LLm result. Try to improve the preprocess function.\")\n",
    "                preprocess_code = extract_code(fix_code)\n",
    "                # print(f\"Revise{_}:\\n{preprocess_code}\\n\")\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "                trace = traceback.format_exc()\n",
    "                print(trace)\n",
    "                fix_code = coding_agent.run(f\"{prompt}\\n\\nExecute result:\\n{str(trace)[:512]}\\n\\nModify the code and fix the error. Try not to use the module that causes error. Simplify the preprocess function as much as posiible.\")\n",
    "                preprocess_code = extract_code(fix_code)\n",
    "                # print(f\"Fix{_}:\\n{preprocess_code}\\n\")\n",
    "                \n",
    "        print(best_result)\n",
    "        self.update_status()\n",
    "        return f\"{preprocess_data_thought}\\n\\nProcessing....\\n\\nProcess Completed !\"\n",
    "    \n",
    "    def update_status(self):\n",
    "        self.status.preprocess_data_thought = self.preprocess_data_thought\n",
    "        self.status.preprocess_code = self.preprocess_code\n",
    "        self.status.X_train = self.locals['X_train']\n",
    "        self.status.X_test = self.locals['X_test']\n",
    "        self.status.y_train = self.locals['y_train']\n",
    "        self.status.y_test = self.locals['y_test']\n",
    "        self.status.preprocess = True\n",
    "        \n",
    "    def evaluate(self):\n",
    "        \n",
    "        print('start evaluate')\n",
    "\n",
    "        def common_preprocess(X_train, y_train, X_test, y_test):\n",
    "            # TODO: Missing value dropping\n",
    "\n",
    "            # TODO: Categorical feature encoding\n",
    "\n",
    "            # Scale numerical variables using StandardScaler\n",
    "            scaler = StandardScaler()\n",
    "            X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "            X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "            return X_train, y_train, X_test, y_test\n",
    "        \n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        from lightgbm import LGBMRegressor, early_stopping\n",
    "\n",
    "        seed = 42\n",
    "\n",
    "        def evaluate(X_train, y_train, X_test, y_test, prep_func=None):\n",
    "            processed_data = (X_train, y_train, X_test, y_test)\n",
    "            if prep_func:\n",
    "                try:\n",
    "                    X_train, y_train, X_test, y_test = prep_func(X_train, y_train, X_test, y_test)\n",
    "                except Exception as e:\n",
    "                    import traceback\n",
    "                    trace = traceback.format_exc()\n",
    "                    print(trace)\n",
    "                    raise Exception(trace)\n",
    "                processed_data = (X_train, y_train, X_test, y_test)\n",
    "\n",
    "            X_train = X_train.select_dtypes(exclude=['object'])\n",
    "            X_test = X_test.select_dtypes(exclude=['object'])\n",
    "            print(X_train.shape. X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=seed)\n",
    "            clf = LGBMRegressor(random_state=seed, **self.status.best_param)\n",
    "            clf.fit(X_train, y_train, eval_set=(X_valid, y_valid), callbacks=[early_stopping(stopping_rounds=10)])\n",
    "            y_pred = clf.predict(X_test)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            return mse, processed_data\n",
    "\n",
    "        from copy import deepcopy\n",
    "        X_train = deepcopy(self.status.X_train)\n",
    "        X_test = deepcopy(self.status.X_test)\n",
    "        y_train = deepcopy(self.status.y_train)\n",
    "        y_test = deepcopy(self.status.y_test)\n",
    "        self.locals[\"X_train\"] = X_train\n",
    "        self.locals[\"X_test\"] = X_test\n",
    "        self.locals[\"y_train\"] = y_train\n",
    "        self.locals[\"y_test\"] = y_test\n",
    "        \n",
    "        # LLM\n",
    "        mse_llm, processed_data = evaluate(X_train, y_train, X_test, y_test, self.locals['preprocess'])\n",
    "\n",
    "        # Common\n",
    "        # mse_com, _ = evaluate(X_train, y_train, X_test, y_test, common_preprocess)\n",
    "\n",
    "        # Raw\n",
    "        mse_raw, _ = evaluate(X_train, y_train, X_test, y_test)\n",
    "        mse_com = mse_raw\n",
    "\n",
    "        msg = f\"No preprocess: {mse_raw}, Common preprocess: {mse_com}, LLM preprocess: {mse_llm}\"\n",
    "        print(msg)\n",
    "        return {\n",
    "            'msg': msg,\n",
    "            'no': mse_raw,\n",
    "            'common': mse_com,\n",
    "            'llm': mse_llm,\n",
    "            'processed_data': processed_data\n",
    "        }"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9df1b8a-5641-4b18-a91a-12e5adec780a",
   "metadata": {
    "tags": []
   },
   "source": [
    "def gogogo():\n",
    "    print(\"====== gogogo ==========\")\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from lightgbm import LGBMRegressor, early_stopping\n",
    "    X_train = status.X_train\n",
    "    y_train = status.y_train\n",
    "    X_test = status.X_test\n",
    "    y_test = status.y_test\n",
    "    print(status.best_param)\n",
    "    seed = 42\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=seed)\n",
    "    clf = LGBMRegressor(random_state=seed, **status.best_param)\n",
    "    clf.fit(X_train, y_train, eval_set=(X_valid, y_valid), callbacks=[early_stopping(stopping_rounds=10)])\n",
    "    y_pred = clf.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(mse)\n",
    "    print(\"==============\")\n",
    "    \n",
    "gogogo()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d1d0b2d-534c-45fe-948f-1b3d5d0b8339",
   "metadata": {},
   "source": [
    "coding_agent.run.cache_clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8abb5492-1b89-40cf-8fbb-ce80a398d801",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pdb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class E2EHyperOptTool:\n",
    "    globals: Optional[Dict] = dict()\n",
    "    locals: Optional[Dict] = dict()\n",
    "\n",
    "    def __init__(self, status):\n",
    "        self.status = status\n",
    "        self.X_train = status.X_train\n",
    "        self.y_train = status.y_train\n",
    "        self.X_test = status.X_test\n",
    "        self.y_test = status.y_test\n",
    "        # self.locals = {\"X\": X, 'Y': Y,}\n",
    "\n",
    "    @prompts(\n",
    "        name=\"hyperopt\",\n",
    "        description=\"This is a tool for hyper parameter tuning.\"\n",
    "        \"Input should be an empty string.\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        if not self.status.dataset:\n",
    "            return \"Pick a dataset first.\"\n",
    "        if not self.status.model:\n",
    "            msg = \"You have not select the model. Use LGBMRegressor for this task.\"\n",
    "            self.status.model = \"LGBMRegressor\"\n",
    "        \n",
    "        clf = self.construct_estimator(self.status.model)\n",
    "        param_grid = self.construct_param_grid(clf)\n",
    "        search_algo = self.get_search_algo(clf, param_grid)\n",
    "        clff = self.construct_search_algo(search_algo)\n",
    "        self.clff = clff\n",
    "        \n",
    "        X_train = self.X_train.select_dtypes(exclude=['object'])\n",
    "        search = clff.fit(X_train, self.y_train)\n",
    "        self.search = search\n",
    "        \n",
    "        print(f\"Best search score: {search.best_score_}\")\n",
    "        score = self.evaluate(clf, search)\n",
    "\n",
    "        self.update_status()\n",
    "        return f\"{self.reason}\\n\\nFinal score after hyper parameter tuning is {score}\"\n",
    "    \n",
    "    def update_status(self):\n",
    "        self.status.hyperopt = True\n",
    "        self.status.hyper_result = self.search\n",
    "        self.status.hyperopt_param_thought = self.reason\n",
    "        self.status.y_pred = self.y_pred\n",
    "        \n",
    "    def evaluate(self, clf, search):\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        \n",
    "        X_train = self.X_train.select_dtypes(exclude=['object'])\n",
    "        X_test = self.X_test.select_dtypes(exclude=['object'])\n",
    "        \n",
    "        clff = clf.__class__(**search.best_params_)\n",
    "        clff.fit(X_train, self.y_train)\n",
    "        y_pred = clff.predict(X_test)\n",
    "        self.y_pred = y_pred\n",
    "        score = mean_squared_error(self.y_test, y_pred)\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "        print(f\"Testing score: {score}\")\n",
    "        return score\n",
    "    \n",
    "    def construct_search_algo(self, search_algo):\n",
    "        # construct search instance\n",
    "        construct_search_prompt = f\"\"\"\n",
    "        Give me two lines of python code.\n",
    "        Frist, from lib import {search_algo.strip()}.\n",
    "        Second, instatiate an instance with {search_algo.strip()}(clf, param_grid, random_state=0, n_iter=15) and assign to hyper_algo variable.\n",
    "        \"\"\"\n",
    "        codes = coding_agent.run(construct_search_prompt)\n",
    "        print(f\"\\nConstructing search algo:\\n```\\n{codes}\\n```\\n\")\n",
    "        res = self.execute(codes)\n",
    "        if 'hyper_algo' not in self.locals:\n",
    "            raise Exception(res)\n",
    "        return self.locals['hyper_algo']\n",
    "\n",
    "    def get_search_algo(self, clf, param_grid):\n",
    "        search_algo_reason_prompt = f\"\"\"\n",
    "        You are now performing a hyper parameter search on a machine learning model.\n",
    "        Your goal is to decide the search algorithm base on model type and parameter space.\n",
    "\n",
    "\n",
    "        The given model is a {clf.__class__.__name__}.\n",
    "        The paramters grid is {param_grid}.\n",
    "        The avaiable search algorithm is as follow:\n",
    "\n",
    "        1. GridSearchCV: Exhaustive search over specified parameter values for an estimator.\n",
    "        2. HalvingGridSearchCV: Search over specified parameter values with successive halving.\n",
    "        3. ParameterGrid: Grid of parameters with a discrete number of values for each.\n",
    "        4. ParameterSampler: Generator on parameters sampled from given distributions.\n",
    "        5. RandomizedSearchCV: Randomized search on hyper parameters.\n",
    "        6. HalvingRandomSearchCV: Randomized search on hyper parameters with successive halving.\n",
    "\n",
    "        Think about which algorithm to use base on the model, parameter space and algorithm description.\n",
    "        Tell me which algorithm you have choosen and why.\n",
    "        List all the other algorithms you have choosen not to use and why.\n",
    "        \"\"\"\n",
    "        \n",
    "        reason = coding_agent.run(search_algo_reason_prompt)\n",
    "        self.reason = reason\n",
    "        print(reason)\n",
    "\n",
    "        search_algo = coding_agent.run(f\"{reason}\\n\\nAccording to above, give me the algorithm name of the choosen algorithm in a single token.\")\n",
    "        print(f\"\\nDeciding search algo:\\n{search_algo}\\n\")\n",
    "        return search_algo\n",
    "\n",
    "    def construct_param_grid(self, clf):\n",
    "        param_grid_prompt = f\"\"\"\n",
    "        You are now performing a hyper parameter search on a machine learning model.\n",
    "        Your goal is to construct the paramter space to search.\n",
    "\n",
    "        For example:\n",
    "        The model is a LinearSVR.\n",
    "        The parameters are {LinearSVR().get_params()}\n",
    "        The parameter grid can be defined as:\n",
    "        ```\n",
    "        param_grid = {{'C': [1, 10, 100, 1000], 'tol': [0.001, 0.0001]}}\n",
    "        ```\n",
    "        Remeber to define the space only on important features. \n",
    "\n",
    "\n",
    "        Now.\n",
    "        The given model is a {clf.__class__.__name__}.\n",
    "        The paramters are {clf.get_params()}.\n",
    "        The detail description about the parameters is as follow:\n",
    "        {clf.__doc__}\n",
    "\n",
    "        Construct the parameter grid you will use for hyper parameter in json format:\n",
    "        \"\"\"\n",
    "\n",
    "        param_grid = coding_agent.run(param_grid_prompt)\n",
    "        param_grid = json.loads(param_grid)\n",
    "        print(f\"\\nConstruct params grid:\\n{param_grid}\\n\")\n",
    "        self.locals['param_grid'] = param_grid\n",
    "        return param_grid\n",
    "        \n",
    "    def construct_estimator(self, estimator_name: str):\n",
    "        # construct clf instance\n",
    "        construct_estimator_prompt = f\"Give me two lines of python code. Construct a {estimator_name.strip()} estimator and assign the estimator clf variable.\"\n",
    "        codes = coding_agent.run(construct_estimator_prompt)\n",
    "        print(f\"\\nConstruct estimator with following:\\n```\\n{codes}\\n```\\n\")\n",
    "        res = self.execute(codes)\n",
    "        if 'clf' not in self.locals:\n",
    "            raise Exception(res)\n",
    "        return self.locals['clf']\n",
    "        \n",
    "    def execute(self, cmd: str):\n",
    "        try:\n",
    "            tree = ast.parse(cmd)\n",
    "            module = ast.Module(tree.body[:-1], type_ignores=[])\n",
    "            exec(astunparse.unparse(module), self.globals, self.locals)  # type: ignore\n",
    "            module_end = ast.Module(tree.body[-1:], type_ignores=[])\n",
    "            module_end_str = astunparse.unparse(module_end)  # type: ignore\n",
    "            try:\n",
    "                return eval(module_end_str, self.globals, self.locals)\n",
    "            except Exception:\n",
    "                exec(module_end_str, self.globals, self.locals)\n",
    "                return \"\"\n",
    "        except Exception as e:\n",
    "            return str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e72df5c3-a5f6-4c71-b052-1db08579cbd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConversationBot(ABC):\n",
    "\n",
    "    def __init__(self):\n",
    "        seed_everything(0)\n",
    "        # self.handler = LoggerCallbackHandler()\n",
    "        # self.async_handler = AsyncLoggerCallbackHandler()\n",
    "        self.llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\", output_key=\"output\", return_messages=True\n",
    "        )\n",
    "        self.state = []\n",
    "        self.tools = self.load_tools()\n",
    "\n",
    "        # retry when llm output parsing error\n",
    "        retrytool = retryActionTool()\n",
    "        func = retrytool.inference\n",
    "        self.tools.append(\n",
    "            Tool(\n",
    "                name=func.name,\n",
    "                description=func.description,\n",
    "                # coroutine=to_async(func),\n",
    "                func=func,\n",
    "            )\n",
    "        )\n",
    "        self.output_parser = RobustRetryConvoOutputParser()\n",
    "        self.agent = self.init_agent()\n",
    "\n",
    "    def run_text(self, text: str):\n",
    "        logger.info(f\"User: {text}\")\n",
    "        res = self.agent({\"input\": text})\n",
    "        response = res[\"output\"]\n",
    "        self.state += [(text, response)]\n",
    "        logger.info(\n",
    "            f\"\\nProcessed run_text, Input text: {text}\\nCurrent state: {self.state}\\n\"\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    async def arun_text(self, text: str):\n",
    "        logger.info(f\"User: {text}\")\n",
    "        res = await self.agent.acall({\"input\": text})\n",
    "        response = res[\"output\"]\n",
    "        self.state += [(text, response)]\n",
    "        logger.info(\n",
    "            f\"\\nProcessed run_text, Input text: {text}\\nCurrent state: {self.state}\\n\"\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _clear(self):\n",
    "        self.memory.clear\n",
    "\n",
    "    def init_agent(self):\n",
    "        input_variables = [\"input\", \"agent_scratchpad\", \"chat_history\"]\n",
    "        return initialize_agent(\n",
    "            self.tools,\n",
    "            self.llm,\n",
    "            agent=\"chat-conversational-react-description\",\n",
    "            verbose=True,\n",
    "            memory=self.memory,\n",
    "            return_intermediate_steps=True,\n",
    "            max_iterations=5,\n",
    "            # max_execution_time=60,\n",
    "            agent_kwargs={\n",
    "                \"prefix\": AUTOML_CHATGPT_PREFIX,\n",
    "                \"format_instructions\": AUTOML_CHATGPT_FORMAT_INSTRUCTIONS,\n",
    "                \"suffix\": AUTOML_CHATGPT_SUFFIX,\n",
    "                \"input_variables\": input_variables,\n",
    "                \"output_parser\": self.output_parser,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def load_tools(self):\n",
    "        tools = []\n",
    "        X, X_train, X_test, y, y_train, y_test = self.load_artifacts()\n",
    "\n",
    "        for tool in [\n",
    "            # pythonShellTool(),\n",
    "            E2EHyperOptTool(status),\n",
    "            # HyperOptTool(),\n",
    "            # ConstructSearchTool(),\n",
    "            # ConstructParamGridTool(),\n",
    "            # ConstructEstimatorTool(),\n",
    "            # FinalTool(X, Y),\n",
    "            ExploreDataTool(status),\n",
    "            ProcessDataTool(status),\n",
    "            LoadDataTool(status),\n",
    "        ]:\n",
    "            func = tool.inference\n",
    "            tools.append(\n",
    "                Tool(\n",
    "                    name=func.name,\n",
    "                    description=func.description,\n",
    "                    # coroutine=to_async(func),\n",
    "                    func=func,\n",
    "                )\n",
    "            )\n",
    "        logger.info(f\"tools: {[i.name for i in tools]}\")\n",
    "        return tools\n",
    "\n",
    "    def load_artifacts(self):\n",
    "        data = pd.read_csv(\"WineQT.csv\")\n",
    "        X = data.drop('quality', axis=1)\n",
    "        y = data['quality']\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        # X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "        return X, X_train, X_test, y, y_train, y_test"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0155fe39-8191-441c-9f4a-6eeaa45c3ed8",
   "metadata": {
    "tags": []
   },
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "chat_agent = ConversationBot()\n",
    "X, X_train, X_test, y, y_train, y_test = chat_agent.load_artifacts()\n",
    "\n",
    "clf = LGBMRegressor()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fb79fc-bf38-421e-bc3d-751c145a96d1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class E2EPreprocessTool:\n",
    "    globals: Optional[Dict] = dict()\n",
    "    locals: Optional[Dict] = dict()\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.locals = {\"X\": X, 'y': y, 'np': np}\n",
    "        self.globals = {\n",
    "            \"np\": np,\n",
    "            'pd': pd,\n",
    "            # 'StandardScaler': StandardScaler,\n",
    "            # 'PowerTransformer': PowerTransformer,\n",
    "            # 'boxcox': boxcox,\n",
    "            # 'SMOTE': SMOTE,\n",
    "            # 'RandomOverSampler': RandomOverSampler\n",
    "        }\n",
    "\n",
    "    @prompts(\n",
    "        name=\"preprocess\",\n",
    "        description=\"This is a tool for dataset preprocessing.\"\n",
    "        \"Input should be a dataset name.\"\n",
    "    )\n",
    "    def inference(self, query: str):\n",
    "        explore_prompts = self.explore_dataset(query)\n",
    "        self.preprocess_dataset(explore_prompts)\n",
    "        # self.evaluate()\n",
    "        \n",
    "    def retry_execute(self, prompt: str, code: str, retry: int = 3):\n",
    "        for _ in range(retry):\n",
    "            try:\n",
    "                exec(code, self.globals, self.locals)\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "                fix_code = coding_agent.run(f\"{prompt}\\n\\nExecute result:\\n{traceback.format_exc()}\\n\\nModify the code and fix the error.\")\n",
    "                code = extract_code(fix_code)\n",
    "                print(f\"Fix{_}:\\n{code}\\n\")\n",
    "        \n",
    "    def evaluate(self):\n",
    "\n",
    "        def common_preprocess(X_train, y_train, X_test, y_test):\n",
    "            # TODO: Missing value dropping\n",
    "\n",
    "            # TODO: Categorical feature encoding\n",
    "\n",
    "            # Scale numerical variables using StandardScaler\n",
    "            scaler = StandardScaler()\n",
    "            X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "            X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "            return X_train, y_train, X_test, y_test\n",
    "        \n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        from lightgbm import LGBMRegressor, early_stopping\n",
    "\n",
    "        seed = 42\n",
    "\n",
    "        def evaluate(X_train, y_train, X_test, y_test, prep_func=None):\n",
    "            if prep_func:\n",
    "                X_train, y_train, X_test, y_test = prep_func(X_train, y_train, X_test, y_test)\n",
    "\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=seed)\n",
    "            clf = LGBMRegressor(random_state=seed)\n",
    "            clf.fit(X_train, y_train, eval_set=(X_valid, y_valid), callbacks=[early_stopping(stopping_rounds=10)])\n",
    "            y_pred = clf.predict(X_test)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "            return mse\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.2, random_state=seed)\n",
    "\n",
    "\n",
    "        # LLM\n",
    "        mse_llm = evaluate(X_train, y_train, X_test, y_test, self.locals['preprocess'])\n",
    "\n",
    "        # Common\n",
    "        mse_com = evaluate(X_train, y_train, X_test, y_test, common_preprocess)\n",
    "\n",
    "        # Raw\n",
    "        mse_raw = evaluate(X_train, y_train, X_test, y_test)\n",
    "\n",
    "        msg = f\"No preprocess: {mse_raw}, Common preprocess: {mse_com}, LLM preprocess: {mse_llm}\"\n",
    "        print(msg)\n",
    "        return {\n",
    "            'msg': msg,\n",
    "            'no': mse_raw,\n",
    "            'common': mse_com,\n",
    "            'llm': mse_llm\n",
    "        }\n",
    "        \n",
    "    def preprocess_dataset(self, explore_prompts: str):\n",
    "        start_explore_prompt, how_to_explore_prompt, explore_data_thought, explore_data_result = explore_prompts\n",
    "        \n",
    "        preprocess_prompt = \"\"\"\n",
    "        The exploration results from the first stage are shown above.\n",
    "        Now, let's move on to the second stage:\n",
    "        1. Provide a conclusion based on the exploration results.\n",
    "        2. Based on the conclusion, specify the preprocessing techniques to be employed for the dataset and briefly describe their implementations.\n",
    "        3. Write a Python function called `preprocess` that will assist you in preprocessing the data.\n",
    "           Import any package you need beforehand.\n",
    "           The function will be called using `proc_X_train, proc_y_train, proc_X_test, proc_y_test = preprocess(X_train, y_train, X_test, y_test)`, \n",
    "           where both the parameters and return values are eithor Pandas dataframes or series.\n",
    "           Be aware of potential data leakage issues.\n",
    "\n",
    "        Here are some handy packages in the Python environment:\n",
    "        ```\n",
    "        numpy==1.24.3\n",
    "        pandas==2.0.1\n",
    "        ```\n",
    "        No additional packages can be used.\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        scikit-learn==1.2.2\n",
    "        scipy==1.10.1\n",
    "        imbalanced-learn==0.10.1\n",
    "        \"\"\"\n",
    "\n",
    "        prompt = \"\\n\\n\".join([\n",
    "            start_explore_prompt,\n",
    "            how_to_explore_prompt,\n",
    "            f\"Your Response:\\n{explore_data_thought}\",\n",
    "            f\"Code Execution Results:\\n{explore_data_result}\",\n",
    "            preprocess_prompt]\n",
    "        )\n",
    "        preprocess_data_thought = coding_agent.run(prompt)\n",
    "        print(preprocess_data_thought)\n",
    "        preprocess_code = extract_code(preprocess_data_thought)\n",
    "        self.preprocess_code = preprocess_code\n",
    "        \n",
    "        best_result = {\n",
    "            'score': 100,\n",
    "            'code': preprocess_code\n",
    "        }\n",
    "        \n",
    "        for _ in range(5):\n",
    "            try:\n",
    "                exec(preprocess_code, self.globals, self.locals)\n",
    "                result = self.evaluate()\n",
    "                \n",
    "                if result['llm'] < best_result['score']:\n",
    "                    best_result['score'] = result['llm']\n",
    "                    best_result['code'] = preprocess_code\n",
    "                \n",
    "                fix_code = coding_agent.run(f\"{prompt}\\n\\nExecute result:\\n{result['msg']}\\n\\nYour result is LLm result. Try to improve the preprocess function.\")\n",
    "                preprocess_code = extract_code(fix_code)\n",
    "                print(f\"Revise{_}:\\n{preprocess_code}\\n\")\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "                trace = traceback.format_exc()\n",
    "                print(trace)\n",
    "                fix_code = coding_agent.run(f\"{prompt}\\n\\nExecute result:\\n{str(e)}\\n\\nModify the code and fix the error. Try not to use the module that causes error. Try to simpify the function.\")\n",
    "                preprocess_code = extract_code(fix_code)\n",
    "                print(f\"Fix{_}:\\n{preprocess_code}\\n\")\n",
    "        \n",
    "        # self.retry_execute(preprocess_data_thought, preprocess_code)\n",
    "\n",
    "        \n",
    "    def explore_dataset(self, query: str):\n",
    "        start_explore_prompt = f\"\"\"\n",
    "You are a data scientist. You are now performing data analysis before training a machine learning model.\n",
    "Your task is to tell me how the data could be preprocessed and to give me the corresponding Python code.\n",
    "The preprocessing includes, but is not limited to, missing value handling, scaling, normalization, or encoding.\n",
    "The task will be performed in two stages.\n",
    "In the first stage, you will decide which techniques are required to preprocess the dataset by investigating the dataset.\n",
    "In the second stage, you will preprocess the dataset by implementing the functions.\n",
    "\n",
    "Here is some information about the feature dataframe `X`:\n",
    "\\\"\\\"\\\"\n",
    "- First five rows:\n",
    "{self.X.head(5)}\n",
    "\n",
    "- Column types:\n",
    "{self.X.dtypes}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Here is some information about the label series `y`:\n",
    "\\\"\\\"\\\"\n",
    "- First five rows:\n",
    "{self.y.head(5)}\n",
    "\n",
    "- Column types:\n",
    "{self.y.dtypes}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "        how_to_explore_prompt = \"\"\"\n",
    "Now, let's move on to the first stage:\n",
    "1. Explain how you will explore the dataset in order to select appropriate preprocessing techniques.\n",
    "2. Write a Python function called `explore` to explore the dataset.\n",
    "   Import any package you need beforehand.\n",
    "   The function will be called using `result = explore(X, y)`, and all findings must be stored in a string variable `result`.\n",
    "\n",
    "Here are some handy packages in the Python environment:\n",
    "```\n",
    "numpy==1.24.3\n",
    "pandas==2.0.1\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "        prompt = \"\\n\\n\".join([start_explore_prompt, how_to_explore_prompt])\n",
    "        self.prompt = prompt\n",
    "        print(prompt)\n",
    "\n",
    "        explore_data_thought = coding_agent.run(prompt)\n",
    "        self.explore_data_thought = explore_data_thought\n",
    "        print(explore_data_thought)\n",
    "        \n",
    "        explore_code = extract_code(explore_data_thought)\n",
    "        self.explore_code = explore_code\n",
    "        print(explore_code + '\\n')\n",
    "        self.retry_execute(explore_data_thought, explore_code)\n",
    "        \n",
    "        explore_data_result = self.locals[\"explore\"](self.X, self.y)\n",
    "        self.explore_data_result = explore_data_result\n",
    "        print(explore_data_result)\n",
    "        \n",
    "        return (start_explore_prompt, how_to_explore_prompt, explore_data_thought, explore_data_result)\n",
    "    \n",
    "    \n",
    "preprocess_tool = E2EPreprocessTool(X, y)\n",
    "preprocess_tool.inference(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3d1c67-2bf7-40c4-97ff-b377a6153426",
   "metadata": {},
   "source": [
    "# Demo start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b7628c5-ab46-4c4a-9b1c-6a8c43f6b969",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Load\",\n",
      "    \"action_input\": \"mercedes-benz-greener-manufacturing\"\n",
      "}\n",
      "```\u001b[0m(4209, 377) (4209, 377) (4209,) (4209,)\n",
      "\n",
      "Observation: \u001b[31;1m\u001b[1;3mLoad dataset mercedes-benz-greener-manufacturing success !\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The mercedes-benz-greener-manufacturing dataset has been successfully loaded.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The mercedes-benz-greener-manufacturing dataset has been successfully loaded.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_agent = ConversationBot()\n",
    "chat_agent.run_text(\"Can you load the mercedes-benz-greener-manufacturing dataset ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13edb545-ac4a-4f86-b028-ff80d2efe7b0",
   "metadata": {},
   "source": [
    "# Demo 1: Can you explore the dataset ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34b1cd82-cb8c-4c11-a14f-37e6f83f33a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Explore\",\n",
      "    \"action_input\": \"\"\n",
      "}\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:31: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mThere are no missing values in the dataset.\n",
      "There are outliers in the following columns: [('X10', 56), ('X12', 316), ('X13', 244), ('X15', 2), ('X16', 11), ('X17', 32), ('X18', 33), ('X19', 419), ('X20', 601), ('X21', 11), ('X22', 366), ('X23', 87), ('X24', 8), ('X26', 21), ('X28', 137), ('X29', 181), ('X30', 19), ('X31', 978), ('X32', 47), ('X33', 1), ('X34', 23), ('X35', 978), ('X36', 19), ('X37', 978), ('X38', 140), ('X39', 1), ('X40', 3), ('X41', 48), ('X42', 1), ('X43', 304), ('X44', 48), ('X47', 54), ('X48', 94), ('X49', 514), ('X50', 901), ('X52', 178), ('X53', 29), ('X54', 183), ('X55', 22), ('X56', 89), ('X57', 56), ('X59', 3), ('X60', 6), ('X61', 194), ('X62', 25), ('X63', 48), ('X65', 9), ('X66', 114), ('X67', 8), ('X68', 309), ('X69', 126), ('X70', 337), ('X71', 436), ('X73', 84), ('X74', 3), ('X75', 152), ('X76', 183), ('X77', 53), ('X78', 24), ('X79', 106), ('X80', 223), ('X81', 964), ('X82', 72), ('X83', 5), ('X84', 436), ('X86', 6), ('X87', 4), ('X88', 30), ('X89', 3), ('X90', 31), ('X91', 7), ('X92', 4), ('X94', 31), ('X95', 1), ('X96', 1018), ('X97', 18), ('X98', 242), ('X99', 36), ('X101', 271), ('X102', 29), ('X103', 907), ('X104', 8), ('X105', 10), ('X106', 55), ('X108', 62), ('X109', 170), ('X110', 4), ('X111', 106), ('X112', 12), ('X113', 94), ('X114', 615), ('X116', 828), ('X117', 207), ('X120', 178), ('X122', 30), ('X123', 11), ('X124', 2), ('X125', 13), ('X126', 164), ('X128', 175), ('X129', 516), ('X130', 175), ('X131', 112), ('X133', 523), ('X134', 94), ('X135', 114), ('X136', 183), ('X138', 172), ('X139', 381), ('X140', 170), ('X141', 60), ('X142', 967), ('X143', 161), ('X144', 808), ('X145', 6), ('X146', 172), ('X147', 94), ('X148', 189), ('X150', 874), ('X151', 360), ('X152', 136), ('X153', 3), ('X154', 879), ('X155', 322), ('X158', 967), ('X159', 57), ('X160', 5), ('X161', 832), ('X162', 172), ('X164', 263), ('X165', 19), ('X166', 140), ('X167', 4), ('X169', 28), ('X170', 102), ('X172', 25), ('X173', 41), ('X174', 73), ('X175', 94), ('X176', 72), ('X177', 211), ('X179', 202), ('X180', 665), ('X181', 395), ('X182', 447), ('X183', 17), ('X184', 6), ('X185', 79), ('X189', 356), ('X190', 1), ('X192', 10), ('X195', 49), ('X196', 43), ('X197', 136), ('X198', 97), ('X199', 12), ('X200', 28), ('X201', 748), ('X202', 1016), ('X203', 71), ('X204', 1), ('X205', 1), ('X206', 81), ('X207', 1), ('X208', 265), ('X209', 427), ('X210', 1), ('X211', 63), ('X212', 23), ('X213', 8), ('X214', 29), ('X215', 415), ('X216', 25), ('X217', 31), ('X219', 284), ('X221', 34), ('X222', 94), ('X225', 408), ('X226', 136), ('X227', 13), ('X228', 164), ('X229', 168), ('X230', 22), ('X231', 68), ('X232', 181), ('X234', 849), ('X236', 2), ('X237', 28), ('X238', 353), ('X239', 29), ('X240', 12), ('X241', 409), ('X242', 31), ('X243', 30), ('X244', 436), ('X245', 3), ('X247', 1016), ('X248', 6), ('X249', 32), ('X252', 3), ('X253', 6), ('X254', 22), ('X255', 82), ('X256', 308), ('X257', 1), ('X258', 10), ('X259', 1), ('X260', 1), ('X262', 6), ('X263', 181), ('X264', 166), ('X265', 398), ('X266', 6), ('X267', 38), ('X269', 2), ('X270', 1), ('X271', 9), ('X272', 158), ('X274', 42), ('X276', 162), ('X277', 6), ('X278', 2), ('X279', 181), ('X280', 1), ('X281', 11), ('X282', 17), ('X283', 592), ('X284', 173), ('X285', 866), ('X286', 230), ('X287', 67), ('X288', 1), ('X291', 44), ('X292', 38), ('X294', 526), ('X295', 1), ('X296', 1), ('X298', 19), ('X299', 19), ('X300', 871), ('X301', 197), ('X302', 48), ('X304', 319), ('X305', 56), ('X306', 184), ('X307', 9), ('X308', 40), ('X309', 30), ('X310', 11), ('X312', 18), ('X315', 121), ('X316', 823), ('X317', 32), ('X318', 3), ('X319', 2), ('X320', 30), ('X321', 1005), ('X322', 92), ('X323', 39), ('X325', 24), ('X326', 136), ('X327', 540), ('X328', 169), ('X331', 236), ('X332', 3), ('X333', 101), ('X335', 15), ('X336', 536), ('X338', 29), ('X339', 1), ('X340', 94), ('X341', 34), ('X342', 94), ('X343', 330), ('X344', 36), ('X345', 94), ('X346', 200), ('X348', 222), ('X349', 189), ('X352', 228), ('X353', 9), ('X354', 854), ('X356', 757), ('X357', 5), ('X359', 134), ('X360', 322), ('X361', 143), ('X363', 1036), ('X364', 12), ('X365', 12), ('X366', 5), ('X367', 218), ('X368', 264), ('X369', 2), ('X370', 28), ('X371', 60), ('X372', 2), ('X373', 81), ('X374', 957), ('X376', 241), ('X378', 87), ('X379', 40), ('X380', 34), ('X382', 32), ('X383', 7), ('X384', 2), ('X385', 6)]. You may need to remove or transform them.\n",
      "Some features are highly skewed. You may need to apply a transformation or normalization.\n",
      "There are 8 categorical features. You may need to encode them using one-hot or label encoding.\n",
      "There are highly correlated features: ['X35', 'X37', 'X39', 'X54', 'X61', 'X66', 'X76', 'X84', 'X89', 'X90', 'X94', 'X98', 'X99', 'X101', 'X102', 'X111', 'X113', 'X118', 'X119', 'X120', 'X122', 'X126', 'X128', 'X129', 'X130', 'X132', 'X134', 'X136', 'X137', 'X140', 'X146', 'X147', 'X148', 'X150', 'X157', 'X158', 'X161', 'X162', 'X166', 'X170', 'X171', 'X172', 'X178', 'X179', 'X183', 'X186', 'X187', 'X191', 'X194', 'X198', 'X199', 'X202', 'X205', 'X213', 'X214', 'X215', 'X216', 'X217', 'X219', 'X222', 'X226', 'X227', 'X229', 'X232', 'X238', 'X239', 'X241', 'X242', 'X243', 'X244', 'X245', 'X246', 'X247', 'X248', 'X249', 'X250', 'X251', 'X253', 'X254', 'X261', 'X262', 'X263', 'X264', 'X265', 'X266', 'X267', 'X272', 'X276', 'X277', 'X279', 'X296', 'X299', 'X302', 'X304', 'X308', 'X311', 'X314', 'X317', 'X320', 'X321', 'X324', 'X325', 'X326', 'X328', 'X337', 'X346', 'X348', 'X349', 'X352', 'X356', 'X357', 'X358', 'X360', 'X362', 'X363', 'X364', 'X365', 'X367', 'X368', 'X370', 'X371', 'X373', 'X374', 'X376', 'X378', 'X379', 'X382', 'X385']. You may need to remove them.\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The dataset has no missing values, but there are outliers in some columns and some features are highly skewed. There are also 8 categorical features that may need to be encoded using one-hot or label encoding. Additionally, there are highly correlated features that may need to be removed. These findings were obtained from exploring the dataset.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The dataset has no missing values, but there are outliers in some columns and some features are highly skewed. There are also 8 categorical features that may need to be encoded using one-hot or label encoding. Additionally, there are highly correlated features that may need to be removed. These findings were obtained from exploring the dataset.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_agent = ConversationBot()\n",
    "chat_agent.run_text(\"Can you explore the dataset ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aace712-b5a4-4972-b807-fdde0a3cffa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer(\"Hello world\")[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b70ed09-bf75-4653-8b1f-0993a9f0ab78",
   "metadata": {},
   "source": [
    "# Demo 2: Can you preprocess the dataset ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "26a0e3a0-bd54-42c7-81b6-a6b8effb4309",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Preprocess\",\n",
      "    \"action_input\": \"\"\n",
      "}\u001b[0m\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from scipy.stats import boxcox\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "def preprocess(X_train, y_train, X_test, y_test):\n",
      "    # Winsorization to remove outliers\n",
      "    X_train = X_train.clip(lower=X_train.quantile(0.01), upper=X_train.quantile(0.99), axis=1)\n",
      "    X_test = X_test.clip(lower=X_test.quantile(0.01), upper=X_test.quantile(0.99), axis=1)\n",
      "    \n",
      "    # Box-Cox transformation to reduce skewness\n",
      "    numerical_cols = X_train.select_dtypes(include=np.number).columns\n",
      "    for col in numerical_cols:\n",
      "        X_train[col], _ = boxcox(X_train[col] + 1)\n",
      "        X_test[col], _ = boxcox(X_test[col] + 1)\n",
      "    \n",
      "    # One-hot encoding for categorical features\n",
      "    categorical_cols = X_train.select_dtypes(include='object').columns\n",
      "    encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
      "    encoder.fit(X_train[categorical_cols])\n",
      "    encoded_cols = list(encoder.get_feature_names(categorical_cols))\n",
      "    X_train_encoded\n",
      "start evaluate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:9: FutureWarning: Downcasting integer-dtype results in .where is deprecated and will change in a future version. To retain the old behavior, explicitly cast the results to the desired dtype.\n",
      "<string>:10: FutureWarning: Downcasting integer-dtype results in .where is deprecated and will change in a future version. To retain the old behavior, explicitly cast the results to the desired dtype.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2935595/1357834626.py\", line 250, in evaluate\n",
      "    X_train, y_train, X_test, y_test = prep_func(X_train, y_train, X_test, y_test)\n",
      "  File \"<string>\", line 15, in preprocess\n",
      "NameError: name 'boxcox' is not defined\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2935595/1357834626.py\", line 250, in evaluate\n",
      "    X_train, y_train, X_test, y_test = prep_func(X_train, y_train, X_test, y_test)\n",
      "  File \"<string>\", line 15, in preprocess\n",
      "NameError: name 'boxcox' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2935595/1357834626.py\", line 189, in inference\n",
      "    result = self.evaluate()\n",
      "  File \"/tmp/ipykernel_2935595/1357834626.py\", line 280, in evaluate\n",
      "    mse_llm, processed_data = evaluate(X_train, y_train, X_test, y_test, self.locals['preprocess'])\n",
      "  File \"/tmp/ipykernel_2935595/1357834626.py\", line 255, in evaluate\n",
      "    raise Exception(trace)\n",
      "Exception: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2935595/1357834626.py\", line 250, in evaluate\n",
      "    X_train, y_train, X_test, y_test = prep_func(X_train, y_train, X_test, y_test)\n",
      "  File \"<string>\", line 15, in preprocess\n",
      "NameError: name 'boxcox' is not defined\n",
      "\n",
      "\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "def preprocess(X_train, y_train, X_test, y_test):\n",
      "    # Remove outliers\n",
      "    numerical_cols = X_train.select_dtypes(include=np.number).columns\n",
      "    for col in numerical_cols:\n",
      "        q1 = X_train[col].quantile(0.25)\n",
      "        q3 = X_train[col].quantile(0.75)\n",
      "        iqr = q3 - q1\n",
      "        lower_bound = q1 - 1.5 * iqr\n",
      "        upper_bound =\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2935595/1357834626.py\", line 188, in inference\n",
      "    exec(preprocess_code, self.globals, self.locals)\n",
      "  File \"<string>\", line 15\n",
      "    upper_bound =\n",
      "                ^\n",
      "SyntaxError: invalid syntax\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from scipy.stats import boxcox\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "def preprocess(X_train, y_train, X_test, y_test):\n",
      "    # Combine train and test sets\n",
      "    X = pd.concat([X_train, X_test], axis=0)\n",
      "    y = pd.concat([y_train, y_test], axis=0)\n",
      "    \n",
      "    # Impute missing values with median\n",
      "    X = X.fillna(X.median())\n",
      "    \n",
      "    # Remove outliers using IQR method\n",
      "    numerical_cols = X.select_dtypes(include=np.number).columns\n",
      "    for col in numerical_cols:\n",
      "        q1 = X[col].quantile(0.25)\n",
      "        q3 = X[col].quantile(0.75)\n",
      "        iqr = q3 - q1\n",
      "        lower_bound = q1 - 1.5 * iqr\n",
      "        upper_bound = q3 + 1.5 * iqr\n",
      "        X\n",
      "start evaluate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:13: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2935595/1357834626.py\", line 250, in evaluate\n",
      "    X_train, y_train, X_test, y_test = prep_func(X_train, y_train, X_test, y_test)\n",
      "TypeError: cannot unpack non-iterable NoneType object\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2935595/1357834626.py\", line 250, in evaluate\n",
      "    X_train, y_train, X_test, y_test = prep_func(X_train, y_train, X_test, y_test)\n",
      "TypeError: cannot unpack non-iterable NoneType object\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2935595/1357834626.py\", line 189, in inference\n",
      "    result = self.evaluate()\n",
      "  File \"/tmp/ipykernel_2935595/1357834626.py\", line 280, in evaluate\n",
      "    mse_llm, processed_data = evaluate(X_train, y_train, X_test, y_test, self.locals['preprocess'])\n",
      "  File \"/tmp/ipykernel_2935595/1357834626.py\", line 255, in evaluate\n",
      "    raise Exception(trace)\n",
      "Exception: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2935595/1357834626.py\", line 250, in evaluate\n",
      "    X_train, y_train, X_test, y_test = prep_func(X_train, y_train, X_test, y_test)\n",
      "TypeError: cannot unpack non-iterable NoneType object\n",
      "\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "def preprocess(X_train, y_train, X_test, y_test):\n",
      "    # Drop columns with more than 50% missing values\n",
      "    X_train = X_train.dropna(thresh=X_train.shape[0]*0.5, axis=1)\n",
      "    X_test = X_tes\n",
      "start evaluate\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2935595/1357834626.py\", line 250, in evaluate\n",
      "    X_train, y_train, X_test, y_test = prep_func(X_train, y_train, X_test, y_test)\n",
      "  File \"<string>\", line 8, in preprocess\n",
      "NameError: name 'X_tes' is not defined\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2935595/1357834626.py\", line 250, in evaluate\n",
      "    X_train, y_train, X_test, y_test = prep_func(X_train, y_train, X_test, y_test)\n",
      "  File \"<string>\", line 8, in preprocess\n",
      "NameError: name 'X_tes' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2935595/1357834626.py\", line 189, in inference\n",
      "    result = self.evaluate()\n",
      "  File \"/tmp/ipykernel_2935595/1357834626.py\", line 280, in evaluate\n",
      "    mse_llm, processed_data = evaluate(X_train, y_train, X_test, y_test, self.locals['preprocess'])\n",
      "  File \"/tmp/ipykernel_2935595/1357834626.py\", line 255, in evaluate\n",
      "    raise Exception(trace)\n",
      "Exception: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2935595/1357834626.py\", line 250, in evaluate\n",
      "    X_train, y_train, X_test, y_test = prep_func(X_train, y_train, X_test, y_test)\n",
      "  File \"<string>\", line 8, in preprocess\n",
      "NameError: name 'X_tes' is not defined\n",
      "\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from scipy.stats import boxcox\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "def preprocess(X_train, y_train, X_test, y_test):\n",
      "    # Remove outliers\n",
      "    numerical_cols = X_train.select_dtypes(include=np.number).columns\n",
      "    for col in numerical_cols:\n",
      "        q1 = X_train[col].quantile(0.25)\n",
      "        q3 = X_train[col].quantile(0.75)\n",
      "        iqr = q3 - q1\n",
      "        lower_bound = q1 - 1.5 * iqr\n",
      "        upper_bound =\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2935595/1357834626.py\", line 188, in inference\n",
      "    exec(preprocess_code, self.globals, self.locals)\n",
      "  File \"<string>\", line 15\n",
      "    upper_bound =\n",
      "                ^\n",
      "SyntaxError: invalid syntax\n",
      "\n",
      "{'score': 100, 'code': \"\\nimport numpy as np\\nimport pandas as pd\\nfrom scipy.stats import boxcox\\nfrom sklearn.preprocessing import OneHotEncoder\\n\\ndef preprocess(X_train, y_train, X_test, y_test):\\n    # Winsorization to remove outliers\\n    X_train = X_train.clip(lower=X_train.quantile(0.01), upper=X_train.quantile(0.99), axis=1)\\n    X_test = X_test.clip(lower=X_test.quantile(0.01), upper=X_test.quantile(0.99), axis=1)\\n    \\n    # Box-Cox transformation to reduce skewness\\n    numerical_cols = X_train.select_dtypes(include=np.number).columns\\n    for col in numerical_cols:\\n        X_train[col], _ = boxcox(X_train[col] + 1)\\n        X_test[col], _ = boxcox(X_test[col] + 1)\\n    \\n    # One-hot encoding for categorical features\\n    categorical_cols = X_train.select_dtypes(include='object').columns\\n    encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\\n    encoder.fit(X_train[categorical_cols])\\n    encoded_cols = list(encoder.get_feature_names(categorical_cols))\\n    X_train_encoded\"}\n",
      "\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mConclusion:\n",
      "- The dataset contains no missing values.\n",
      "- There are outliers in many columns, which may need to be removed or transformed.\n",
      "- Some features are highly skewed, which may require a transformation or normalization.\n",
      "- There are 8 categorical features that may need to be encoded using one-hot or label encoding.\n",
      "- There are many highly correlated features that may need to be removed.\n",
      "\n",
      "Preprocessing Techniques:\n",
      "- Outliers: Since there are many columns with outliers, we will use the Winsorization technique to cap the extreme values at a certain percentile (e.g., 1st and 99th percentile).\n",
      "- Skewness: We will use the Box-Cox transformation to reduce the skewness of the features.\n",
      "- Categorical Features: We will use one-hot encoding to convert the categorical features into numerical features.\n",
      "- Highly Correlated Features: We will remove one of the highly correlated features to avoid multicollinearity.\n",
      "\n",
      "Python Function:\n",
      "```python\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from scipy.stats import boxcox\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "def preprocess(X_train, y_train, X_test, y_test):\n",
      "    # Winsorization to remove outliers\n",
      "    X_train = X_train.clip(lower=X_train.quantile(0.01), upper=X_train.quantile(0.99), axis=1)\n",
      "    X_test = X_test.clip(lower=X_test.quantile(0.01), upper=X_test.quantile(0.99), axis=1)\n",
      "    \n",
      "    # Box-Cox transformation to reduce skewness\n",
      "    numerical_cols = X_train.select_dtypes(include=np.number).columns\n",
      "    for col in numerical_cols:\n",
      "        X_train[col], _ = boxcox(X_train[col] + 1)\n",
      "        X_test[col], _ = boxcox(X_test[col] + 1)\n",
      "    \n",
      "    # One-hot encoding for categorical features\n",
      "    categorical_cols = X_train.select_dtypes(include='object').columns\n",
      "    encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
      "    encoder.fit(X_train[categorical_cols])\n",
      "    encoded_cols = list(encoder.get_feature_names(categorical_cols))\n",
      "    X_train_encoded\n",
      "\n",
      "Processing....\n",
      "\n",
      "Process Completed !\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The dataset contains no missing values. There are outliers in many columns, which may need to be removed or transformed. Some features are highly skewed, which may require a transformation or normalization. There are 8 categorical features that may need to be encoded using one-hot or label encoding. There are many highly correlated features that may need to be removed. The recommended preprocessing techniques are Winsorization to remove outliers, Box-Cox transformation to reduce skewness, one-hot encoding for categorical features, and removing one of the highly correlated features to avoid multicollinearity. A Python function has been provided for you to use.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The dataset contains no missing values. There are outliers in many columns, which may need to be removed or transformed. Some features are highly skewed, which may require a transformation or normalization. There are 8 categorical features that may need to be encoded using one-hot or label encoding. There are many highly correlated features that may need to be removed. The recommended preprocessing techniques are Winsorization to remove outliers, Box-Cox transformation to reduce skewness, one-hot encoding for categorical features, and removing one of the highly correlated features to avoid multicollinearity. A Python function has been provided for you to use.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_agent = ConversationBot()\n",
    "chat_agent.run_text(\"Can you process the dataset ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01619d9-e173-4ba1-a7ac-c1fce627a9f1",
   "metadata": {},
   "source": [
    "# Demo 3: Can you select the model ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88050eb-4494-4175-82fa-b0c11d9cd1b4",
   "metadata": {},
   "source": [
    "# Demo 4: Can you run hyper parameter tuning ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec0647d9-f21a-496d-a33f-44182a068a4d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 250 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n250 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/sklearn.py\", line 895, in fit\n    super().fit(X, y, sample_weight=sample_weight, init_score=init_score,\n  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/sklearn.py\", line 748, in fit\n    self._Booster = train(\n  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/engine.py\", line 271, in train\n    booster = Booster(params=params, train_set=train_set)\n  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/basic.py\", line 2605, in __init__\n    train_set.construct()\n  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/basic.py\", line 1815, in construct\n    self._lazy_init(self.data, label=self.label,\n  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/basic.py\", line 1474, in _lazy_init\n    data, feature_name, categorical_feature, self.pandas_categorical = _data_from_pandas(data,\n  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/basic.py\", line 594, in _data_from_pandas\n    raise ValueError(\"DataFrame.dtypes for data must be int, float or bool.\\n\"\nValueError: DataFrame.dtypes for data must be int, float or bool.\nDid not expect the data types in the following fields: X0, X1, X2, X3, X4, X5, X6, X8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.5\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m500\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m9\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_leaves\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m31\u001b[39m, \u001b[38;5;241m63\u001b[39m, \u001b[38;5;241m127\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_child_samples\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m50\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg_alpha\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1.0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg_lambda\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1.0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolsample_bytree\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m1.0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubsample\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m1.0\u001b[39m]}\n\u001b[1;32m      5\u001b[0m hyper_algo \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(clf, param_grid, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m search \u001b[38;5;241m=\u001b[39m \u001b[43mhyper_algo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    869\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    871\u001b[0m     )\n\u001b[1;32m    873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 875\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    879\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:1749\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1748\u001b[0m     \u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1749\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/sklearn/model_selection/_search.py:852\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[1;32m    846\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    847\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    848\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    849\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[1;32m    850\u001b[0m     )\n\u001b[0;32m--> 852\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[1;32m    361\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m     )\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    377\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 250 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n250 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/sklearn.py\", line 895, in fit\n    super().fit(X, y, sample_weight=sample_weight, init_score=init_score,\n  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/sklearn.py\", line 748, in fit\n    self._Booster = train(\n  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/engine.py\", line 271, in train\n    booster = Booster(params=params, train_set=train_set)\n  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/basic.py\", line 2605, in __init__\n    train_set.construct()\n  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/basic.py\", line 1815, in construct\n    self._lazy_init(self.data, label=self.label,\n  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/basic.py\", line 1474, in _lazy_init\n    data, feature_name, categorical_feature, self.pandas_categorical = _data_from_pandas(data,\n  File \"/home/jason-tsai/.cache/pypoetry/virtualenvs/insight-explorer-JqXJ1KnR-py3.8/lib/python3.8/site-packages/lightgbm/basic.py\", line 594, in _data_from_pandas\n    raise ValueError(\"DataFrame.dtypes for data must be int, float or bool.\\n\"\nValueError: DataFrame.dtypes for data must be int, float or bool.\nDid not expect the data types in the following fields: X0, X1, X2, X3, X4, X5, X6, X8\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "clf = LGBMRegressor()\n",
    "param_grid = {'learning_rate': [0.01, 0.05, 0.1, 0.5], 'n_estimators': [50, 100, 200, 500], 'max_depth': [3, 5, 7, 9], 'num_leaves': [15, 31, 63, 127], 'min_child_samples': [10, 20, 30, 50], 'reg_alpha': [0.0, 0.1, 0.5, 1.0], 'reg_lambda': [0.0, 0.1, 0.5, 1.0], 'colsample_bytree': [0.5, 0.7, 1.0], 'subsample': [0.5, 0.7, 1.0]}\n",
    "hyper_algo = RandomizedSearchCV(clf, param_grid, random_state=0, n_iter=50)\n",
    "search = hyper_algo.fit(status.X_train, status.y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "397b7a29-fb93-4583-a5ab-e1fdb1b00a97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"hyperopt\",\n",
      "    \"action_input\": \"\"\n",
      "}\u001b[0m\n",
      "Construct estimator with following:\n",
      "```\n",
      "import lightgbm as lgb\n",
      "\n",
      "clf = lgb.LGBMRegressor()\n",
      "```\n",
      "\n",
      "\n",
      "Construct params grid:\n",
      "{'learning_rate': [0.01, 0.05, 0.1, 0.5], 'n_estimators': [50, 100, 200, 500], 'max_depth': [3, 5, 7, 9], 'num_leaves': [15, 31, 63, 127], 'min_child_samples': [10, 20, 30, 50], 'reg_alpha': [0.0, 0.1, 0.5, 1.0], 'reg_lambda': [0.0, 0.1, 0.5, 1.0], 'colsample_bytree': [0.5, 0.7, 1.0], 'subsample': [0.5, 0.7, 1.0]}\n",
      "\n",
      "I have chosen to use RandomizedSearchCV for the hyper parameter search on the LGBMRegressor model. This is because RandomizedSearchCV is a good choice when the parameter space is large and it is not feasible to exhaustively search all possible combinations. Additionally, RandomizedSearchCV can be more efficient than GridSearchCV as it randomly samples a subset of the parameter space, which can lead to finding better hyperparameters in less time.\n",
      "\n",
      "I have chosen not to use the following algorithms:\n",
      "\n",
      "1. GridSearchCV: While GridSearchCV is a good choice for smaller parameter spaces, the given parameter grid for the LGBMRegressor model is quite large, which would make GridSearchCV computationally expensive and time-consuming.\n",
      "\n",
      "2. HalvingGridSearchCV: HalvingGridSearchCV is a good choice when the model is computationally expensive and the parameter space is large. However, since the LGBMRegressor model is not computationally expensive, HalvingGridSearchCV is not necessary.\n",
      "\n",
      "3. ParameterGrid: ParameterGrid is a good choice when the parameter space is small and discrete. However, the given parameter grid for the LGBMRegressor model is not discrete and is quite large, which would make ParameterGrid computationally expensive and time-consuming.\n",
      "\n",
      "4. ParameterSampler: ParameterSampler is a good choice when the parameter space is continuous and can be sampled from a distribution. However, the given parameter grid for the LGBMRegressor model is not continuous and is quite large, which would make ParameterSampler computationally expensive and time-consuming.\n",
      "\n",
      "5. HalvingRandomSearchCV: HalvingRandomSearchCV is a good choice when the model is computationally expensive and the parameter space is large. However, since the LGBMRegressor model is not computationally expensive, HalvingRandomSearchCV is not necessary.\n",
      "\n",
      "Deciding search algo:\n",
      "RandomizedSearchCV\n",
      "\n",
      "\n",
      "Constructing search algo:\n",
      "```\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "hyper_algo = RandomizedSearchCV(clf, param_grid, random_state=0, n_iter=15)\n",
      "```\n",
      "\n",
      "Best search score: 0.5421192936260335\n",
      "> \u001b[0;32m/tmp/ipykernel_2935595/2896127127.py\u001b[0m(65)\u001b[0;36mevaluate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     63 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     64 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 65 \u001b[0;31m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Testing score: {score}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     66 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     67 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>   score\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10289.612378331778\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  l\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m     60 \u001b[0m        \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     61 \u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     62 \u001b[0m        \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     63 \u001b[0m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     64 \u001b[0m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 65 \u001b[0;31m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Testing score: {score}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     66 \u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     67 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     68 \u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mconstruct_search_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_algo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     69 \u001b[0m        \u001b[0;31m# construct search instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     70 \u001b[0m        construct_search_prompt = f\"\"\"\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  y_pred\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([ 79.01196233,  94.57370525,  79.2718237 , ...,  91.96851864,\n",
      "       110.22102444,  91.88256186])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  y_test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NameError: name 'y_test' is not defined\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.y_test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "4204    0\n",
      "4205    0\n",
      "4206    0\n",
      "4207    0\n",
      "4208    0\n",
      "Name: y, Length: 4209, dtype: int64\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    }
   ],
   "source": [
    "chat_agent = ConversationBot()\n",
    "chat_agent.run_text(\"Can you run hyper parameter tuning ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f994f41e-e24d-40f8-ab3d-c06866458d26",
   "metadata": {},
   "source": [
    "# Demo 5: Can you run the ML process end to end ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6bd47c-5750-457c-99b9-de8b4adf724e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
